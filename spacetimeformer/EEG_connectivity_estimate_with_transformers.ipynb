{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxq7PL4Nm1XT",
        "outputId": "d05ac573-a4e7-48e8-a53d-9b25b195938f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
            "\u001b[K     |████████████████████████████████| 708 kB 30.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.12.1+cu113)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.1.1)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2022.8.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.9.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.49.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (3.2.1)\n",
            "Installing collected packages: torchmetrics, pyDeprecate, pytorch-lightning\n",
            "Successfully installed pyDeprecate-0.3.2 pytorch-lightning-1.7.7 torchmetrics-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "090sDCPClanY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e97e83-8bf2-490a-897d-55c2b3c3a67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt"
      ],
      "metadata": {
        "id": "bS27gv4angMD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b72d4011-ebe7-4dfd-95f9-d15a2fcedc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 1)) (0.29.32)\n",
            "Collecting cmdstanpy==0.9.68\n",
            "  Downloading cmdstanpy-0.9.68-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting pystan~=2.19.1.1\n",
            "  Downloading pystan-2.19.1.1-cp37-cp37m-manylinux1_x86_64.whl (67.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.3 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 5)) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 7)) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 8)) (2.8.2)\n",
            "Collecting performer-pytorch\n",
            "  Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 10)) (4.64.1)\n",
            "Collecting nystrom-attention\n",
            "  Downloading nystrom_attention-0.0.11-py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.7.7)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 13)) (1.6.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 14)) (1.0.2)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 16)) (0.11.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 17)) (4.6.0.66)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 60.3 MB/s \n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from cmdstanpy==0.9.68->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.4->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 5)) (2022.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 7)) (0.5.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.0->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 6)) (4.1.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 9)) (1.12.1+cu113)\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading local_attention-1.4.4-py3-none-any.whl (5.0 kB)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (2.9.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (21.3)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.3.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (2022.8.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (2.23.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.8.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.49.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netCDF4->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 13)) (1.6.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 14)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 14)) (1.2.0)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 67.2 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 67.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 18)) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 18)) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/requirements.txt (line 18)) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 59.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 75.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 35.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 48.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 3.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 50.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 63.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 65.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: axial-positional-embedding, antlr4-python3-runtime, pathtools\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2901 sha256=14c19bd413ead8af51530ad3e7b540c6c0bd1c40d4536ca9dd450e3eb2863ccc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/2c/c3/9a1cb267c0d0d9b6eeba7952addb32b17857d1f799690c27a8\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=b85191ed86389a1d17d4779dbdcbbdbf836587e4e4bb78eeca96682acc5ed63a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=caa7f5c48c0acbfe95de9ddaaffefd75d171182bd95e0619c002608f4ae78cb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built axial-positional-embedding antlr4-python3-runtime pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, local-attention, GitPython, einops, docker-pycreds, axial-positional-embedding, antlr4-python3-runtime, wandb, pystan, performer-pytorch, omegaconf, nystrom-attention, cmdstanpy\n",
            "  Attempting uninstall: pystan\n",
            "    Found existing installation: pystan 3.3.0\n",
            "    Uninstalling pystan-3.3.0:\n",
            "      Successfully uninstalled pystan-3.3.0\n",
            "  Attempting uninstall: cmdstanpy\n",
            "    Found existing installation: cmdstanpy 1.0.7\n",
            "    Uninstalling cmdstanpy-1.0.7:\n",
            "      Successfully uninstalled cmdstanpy-1.0.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "prophet 1.1.1 requires cmdstanpy>=1.0.4, but you have cmdstanpy 0.9.68 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 antlr4-python3-runtime-4.9.3 axial-positional-embedding-0.2.1 cmdstanpy-0.9.68 docker-pycreds-0.4.0 einops-0.5.0 gitdb-4.0.9 local-attention-1.4.4 nystrom-attention-0.0.11 omegaconf-2.2.3 pathtools-0.1.2 performer-pytorch-1.1.4 pystan-2.19.1.1 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX6rysLHvd_-",
        "outputId": "b398e2ae-b057-4579-d866-db25a3992f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/."
      ],
      "metadata": {
        "id": "sJqo1F3ZpV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ad8488-7fe8-4fb2-e99f-3316e199b190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/MyDrive/Documenti/Scuola/Universita%CC%80/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer\n",
            "Installing collected packages: spacetimeformer\n",
            "  Running setup.py develop for spacetimeformer\n",
            "Successfully installed spacetimeformer-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/extensions/cauchy/setup.py install"
      ],
      "metadata": {
        "id": "l-7Mswpopb00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo apt-get install python3.7-dev"
      ],
      "metadata": {
        "id": "EkIFtv6_0WWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/train.py spacetimeformer crypto --run_name spatiotemporal_crypto --d_model 100 --d_ff 400 --enc_layers 4 --dec_layers 4 --batch_size 32 --start_token_len 4 --n_heads 4 --grad_clip_norm 1 --trials 1"
      ],
      "metadata": {
        "id": "AMMSjDkJluOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!export STF_WANDB_ACCT=\"luca_maurici\"\n",
        "!export STF_WANDB_PROJ=\"EEG connectivity estimate with transformers\"\n",
        "# optionally: change wandb logging directory (defaults to ./data/STF_LOG_DIR)\n",
        "!export STF_LOG_DIR=\"./data/STF_LOG_DIR\"\n",
        "'''"
      ],
      "metadata": {
        "id": "jSo6A1_dsZfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer"
      ],
      "metadata": {
        "id": "iobZ7kHp7E71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34424774-1f6a-451a-f64b-16d0e9ea49a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_eeg.py spacetimeformer eeg_social_memory --d_model 200 --d_ff 700 --enc_layers 5 --dec_layers 6 --base_lr 1e-3 --l2_coeff 1e-3 --loss mae --d_qk 30 --d_v 30 --n_heads 10 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .8 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --local_self_attn none --local_cross_attn none --attn_plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAbBzj6euhG-",
        "outputId": "6f0a7b95-0d0d-4600-91f4-8edaf7551d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.run_name: generated, 9ch, single_layer, layers 1, heads 1, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 1, cp 15, tp 1, tf 3, loss mse, local_self_attn none, local_cross_attn none\n",
            "\n",
            "project EEG connectivity estimate with transformers, entity luca_maurici\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca_maurici\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./plots_checkpoints_logs/generated, 9ch, single_layer, layers 1, heads 1, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 1, cp 15, tp 1, tf 3, loss mse, local_self_attn none, local_cross_attn none/wandb_logs/421346176/wandb/run-20220923_175936-211awq27\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmooth-dawn-246\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/211awq27\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "(34, 200, 9)\n",
            "yc.shape: (34, 200, 9)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.0057233   0.00051244  0.01798523 -0.00721104 -0.00505955 -0.01040848\n",
            "  0.00038984 -0.00203722 -0.01552563]\n",
            "std_dev yc: [1.56419104 1.49498691 4.52605295 3.22616496 2.25415107 3.7351453\n",
            " 1.5170044  1.65938239 7.18269641]\n",
            "\n",
            "(158, 200, 9)\n",
            "(200, 1422)\n",
            "[1.60016668 1.46903922 4.92585588 ... 1.56560031 1.6938406  6.068825  ]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-1.74590896e-03 -1.88518797e-02  9.43610800e-02 ... -2.33820926e-02\n",
            "   -4.74136937e-02  7.08147149e-02]\n",
            "  [-6.43457474e-03  6.68456019e-02 -4.72081496e-04 ...  1.70489735e-02\n",
            "    9.30675012e-03  2.22419382e-02]\n",
            "  [-3.51931809e-02  6.13897973e-03  4.26827636e-02 ...  5.93781290e-02\n",
            "    1.19140037e-01 -4.23901588e-02]\n",
            "  ...\n",
            "  [-3.28196024e-02 -9.19168500e-02 -5.89941009e-02 ...  2.19247729e-02\n",
            "    6.56000895e-02  1.96556930e-01]\n",
            "  [-1.13014483e-01 -7.11985572e-02  5.39496324e-02 ...  1.37428499e-01\n",
            "    1.02172411e-01  5.86787255e-02]\n",
            "  [ 4.82439092e-02  3.61518989e-03 -2.69948044e-02 ... -4.67917148e-02\n",
            "   -2.07787428e-02 -2.93986172e-02]]\n",
            "\n",
            " [[-1.47641912e+00  9.84593627e-01 -1.10479156e+00 ... -2.29056295e+00\n",
            "    4.36906379e-01 -5.83118456e-01]\n",
            "  [-6.79289765e-01  1.32079500e+00  1.36299446e+00 ... -1.60908787e+00\n",
            "    3.41964943e-01 -2.43752050e-01]\n",
            "  [ 8.78929284e-01  1.46369608e+00  1.02580579e+00 ...  3.40240401e-01\n",
            "   -2.16888174e-01 -2.62340874e-01]\n",
            "  ...\n",
            "  [ 1.46783920e+00 -1.62761642e-01  6.33578702e-01 ... -7.43526439e-01\n",
            "    3.79024937e-01  6.53923532e-01]\n",
            "  [-8.57143436e-01  3.00296995e-01 -8.97490708e-01 ... -1.59065898e+00\n",
            "    2.82410804e-01  3.38005691e-01]\n",
            "  [-1.22844496e+00  1.37176723e+00 -2.85435628e+00 ... -9.57256052e-02\n",
            "    9.11764707e-01  3.37677342e+00]]\n",
            "\n",
            " [[-1.28641335e+00 -9.33519180e-01  8.67473205e-01 ...  9.95202677e-01\n",
            "    5.19761784e-01 -1.91099590e+00]\n",
            "  [-1.41874095e+00 -1.17301261e+00  6.16018287e-01 ...  1.03958144e+00\n",
            "    6.18601107e-01 -1.28492356e+00]\n",
            "  [-5.59735890e-01 -8.13778303e-02  4.64912385e-01 ...  3.13009216e-01\n",
            "   -3.60375802e-02 -1.04249365e-01]\n",
            "  ...\n",
            "  [ 5.57401139e-01 -7.83498335e-01 -7.24652544e-01 ...  1.88273218e+00\n",
            "   -4.44242847e-01 -2.22228452e+00]\n",
            "  [-2.25105238e-02  2.86794737e-01  1.74365743e+00 ...  1.94423522e+00\n",
            "   -2.05871861e-01 -2.76246021e-01]\n",
            "  [-8.40031090e-01  5.28608000e-01  2.04622991e+00 ...  8.99431369e-01\n",
            "    1.53118105e+00  3.00934023e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.35086093e-02  5.94580900e-02  1.84556458e-02 ...  9.39875459e-02\n",
            "   -1.45550430e-01 -1.04118188e-01]\n",
            "  [-2.00959471e-02 -1.10872712e-01  3.14679084e-02 ...  9.21156344e-02\n",
            "    9.88593252e-02  1.31895880e-02]\n",
            "  [-2.76036364e-02 -5.51987828e-03  1.19199398e-01 ...  4.61042037e-02\n",
            "    1.23105886e-01  3.47084241e-02]\n",
            "  ...\n",
            "  [ 2.46969390e-02  6.94979982e-02 -4.33916413e-02 ...  6.69660994e-02\n",
            "   -6.76709397e-02 -1.14224794e-01]\n",
            "  [ 5.68924105e-02  9.67864269e-02  1.72012249e-02 ...  9.37530391e-02\n",
            "    6.45752099e-02  5.39747241e-04]\n",
            "  [ 7.73287774e-02  9.24363374e-02 -5.35573661e-02 ... -4.07021495e-02\n",
            "   -7.55061619e-03  5.18275926e-02]]\n",
            "\n",
            " [[ 1.24410491e+00  4.56522226e-01 -6.12669170e-02 ... -2.23739618e+00\n",
            "    8.20437947e-01 -2.29270429e+00]\n",
            "  [ 9.83349339e-01  8.83894094e-01 -6.89676837e-01 ... -1.03499924e+00\n",
            "    1.09623442e-01 -8.29550633e-01]\n",
            "  [-6.56350092e-01  1.93330809e-01  1.81778629e-01 ...  5.46996709e-01\n",
            "   -1.96082099e-02  4.71854152e-01]\n",
            "  ...\n",
            "  [-2.56258370e-02 -3.99913690e-01 -4.56947222e-01 ...  4.98276825e-01\n",
            "    1.72762270e-01 -1.94745441e-01]\n",
            "  [ 2.49188839e-01  1.37192834e+00  4.68101329e-01 ...  2.38512849e+00\n",
            "    3.62068014e-02  9.34636944e-02]\n",
            "  [-1.18555365e+00  2.20315614e+00 -1.94300417e+00 ...  1.83571389e+00\n",
            "   -3.97192389e-01  2.35615669e-01]]\n",
            "\n",
            " [[-3.96733905e-01 -3.26997835e+00  8.37493832e-01 ...  2.08839696e+00\n",
            "    9.95375733e-01  1.84218204e+00]\n",
            "  [-2.92043054e-01 -9.09160009e-01  2.97296047e-01 ...  1.24471282e+00\n",
            "    3.57086791e-02  1.07406679e+00]\n",
            "  [ 1.47186340e+00  2.12110038e-01 -1.37008669e-02 ... -1.13016787e+00\n",
            "   -1.09103318e-01 -3.03093111e-01]\n",
            "  ...\n",
            "  [ 6.00888928e-01  1.49592435e+00 -4.97307978e-02 ... -1.49553547e+00\n",
            "   -5.43291500e-01 -7.82928337e-01]\n",
            "  [-1.60621115e+00  4.66136899e-01 -9.85861299e-02 ...  6.90617801e-01\n",
            "   -8.62017911e-02 -8.34847419e-01]\n",
            "  [-3.07428712e+00 -2.07100986e+00 -9.25016908e-01 ...  4.76766248e+00\n",
            "    1.02810456e+00 -1.04855846e+00]]]---------------------\n",
            "\n",
            "\n",
            "(33, 200, 9)\n",
            "(200, 297)\n",
            "[1.45436184 1.63120697 3.83023434 3.01286795 1.99230428 3.7413041\n",
            " 1.79118556 1.53300367 5.67498735 1.66316938 1.35546227 3.83093159\n",
            " 2.56427069 2.17028618 3.20443321 1.62308037 1.50930279 6.59952531\n",
            " 1.77901456 1.50974559 3.75680915 2.92665365 2.23997756 3.48510991\n",
            " 1.57181463 1.4877157  6.31434174 1.78036484 1.58738289 3.60933578\n",
            " 3.08795193 2.55559413 3.49029998 1.55554343 1.37484266 7.02121018\n",
            " 1.5727572  1.49612812 4.04688512 2.70725866 2.20638418 3.42321331\n",
            " 1.55386946 1.45340168 5.6505735  1.60690806 1.37417784 3.8431932\n",
            " 3.03915179 1.90919434 3.39643938 1.52057443 1.48932109 6.43619363\n",
            " 1.66093864 1.35562455 3.58518215 3.33053168 2.01724227 3.48247825\n",
            " 1.46163886 1.73668447 6.39154835 1.35797618 1.55559597 3.8819815\n",
            " 2.6364913  1.97021882 3.44027225 1.47760479 1.34249095 6.41656941\n",
            " 1.53024462 1.5661523  4.04873506 2.69124477 2.27888273 3.30256685\n",
            " 1.56465831 1.42267734 6.68367739 1.61806572 1.42541048 3.73886585\n",
            " 2.52450405 2.00007264 3.3638932  1.59096018 1.36005369 6.47745974\n",
            " 1.49896856 1.56409318 3.79028711 2.72835872 1.9806708  3.28457249\n",
            " 1.63238719 1.38064352 6.23092011 1.47814294 1.47105072 3.90432461\n",
            " 2.82965153 2.20869802 3.09238904 1.73288052 1.42095477 5.85343848\n",
            " 1.47548654 1.48289123 4.20528382 2.61815547 2.0740417  3.23106873\n",
            " 1.54080436 1.52425397 6.9009451  1.77189008 1.43686545 3.76103838\n",
            " 3.31620268 2.11339194 3.74061566 1.65357926 1.40130951 6.59104009\n",
            " 1.61137154 1.35207016 3.93362071 2.86562734 1.9414479  3.400495\n",
            " 1.46222742 1.65600285 6.04453013 1.73923683 1.40587381 3.75131623\n",
            " 2.79432518 2.2616353  3.8174633  1.43010239 1.52371336 6.00860994\n",
            " 1.79355623 1.64697871 3.699941   3.18923568 2.18176622 3.26143204\n",
            " 1.59865316 1.39265248 5.20511686 1.57080951 1.48841742 3.57474572\n",
            " 2.69297527 2.15537061 3.12654365 1.50570575 1.43748513 5.39585564\n",
            " 1.48602818 1.49208384 4.10287439 3.66219679 2.46969643 4.53521361\n",
            " 1.545061   1.4731363  7.33639164 1.66803767 1.57521174 4.08476391\n",
            " 2.57159435 1.93612539 3.62207195 1.47484219 1.4512477  5.49024816\n",
            " 1.58012012 1.37786689 3.50566505 2.96388327 2.08159901 3.25453513\n",
            " 1.52952838 1.5201101  5.92923224 1.75225302 1.40204747 3.97864286\n",
            " 2.67130323 2.21398133 3.3018906  1.42628332 1.2610213  6.20686731\n",
            " 1.57927613 1.52235119 3.6103295  3.06774652 2.23325553 3.71255684\n",
            " 1.5216186  1.42102962 6.22829448 1.66678466 1.47513163 3.81407298\n",
            " 3.02979226 2.16364455 3.69452131 1.40041721 1.58457986 6.14509882\n",
            " 1.67111306 1.37481533 4.17583133 2.79233635 2.327031   3.7324306\n",
            " 1.57643911 1.50301946 6.8617915  1.65637368 1.4319477  4.39402039\n",
            " 3.12821701 2.64438379 4.23103667 1.5669503  2.1391294  7.34179298\n",
            " 1.71651109 1.54396954 3.63983254 3.38768753 2.00901688 3.72836152\n",
            " 1.54707647 1.49001439 6.56168228 1.68970036 1.50580047 4.16861959\n",
            " 2.99615748 2.22246487 3.26952556 1.37224082 1.59128177 5.53883788\n",
            " 1.5675302  1.42774974 3.33534831 2.88004048 1.92594192 3.09488131\n",
            " 1.4835554  1.58249979 5.8227237  1.50158894 1.51581955 3.6882506\n",
            " 2.83031525 1.95242688 3.69123493 1.4199601  1.36763433 5.98480351\n",
            " 1.53357633 1.58807266 3.74629176 2.94453916 1.69916805 3.69710414\n",
            " 1.44199541 1.38396286 6.02209948 1.45217624 1.52732273 3.42452531\n",
            " 2.70091706 1.91614837 3.26312665 1.53106401 1.41044334 7.3524125\n",
            " 1.44572985 1.53994545 3.74232482 2.53080845 1.88063909 3.25430617\n",
            " 1.55866192 1.4684448  6.32916926]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[-0.31069704 -1.6201409   0.99580893 ... -0.57675983  0.23812149\n",
            "    3.26560284]\n",
            "  [ 0.16549339 -0.08283742 -0.53108821 ...  0.35810269  0.17131829\n",
            "   -0.35511918]\n",
            "  [ 0.0472111  -0.08676393 -0.05236945 ...  0.28142576  0.23010811\n",
            "   -0.81585421]\n",
            "  ...\n",
            "  [ 1.41916204 -0.47817519 -2.23607057 ... -0.23947315  0.98156624\n",
            "    0.12690035]\n",
            "  [ 0.15668484  0.28908942 -0.58384231 ...  0.69615229 -1.54390645\n",
            "    1.55689038]\n",
            "  [-1.69354464  1.94231688  2.21163824 ...  0.56599599 -2.97562445\n",
            "    1.54380229]]\n",
            "\n",
            " [[ 1.48053993  4.02299867  0.35004388 ...  1.19997172 -1.8726163\n",
            "   -2.76662613]\n",
            "  [ 0.26473817  0.92853184  0.82702917 ... -0.49720739 -0.3045028\n",
            "   -0.53959747]\n",
            "  [-1.5316349  -2.14844059 -0.27036469 ... -0.04807381  1.42960932\n",
            "    0.62313611]\n",
            "  ...\n",
            "  [-0.72852604 -0.96939611  0.0363341  ... -0.96145595 -0.87824603\n",
            "    0.26985526]\n",
            "  [-0.73193052  1.47579489  1.74870256 ... -0.7731368   0.2999227\n",
            "    2.69671122]\n",
            "  [-0.30818197  3.71879721  2.11709397 ...  0.0874547   2.3205901\n",
            "    3.24284604]]\n",
            "\n",
            " [[-0.36520286 -1.78956003  3.2313185  ...  2.29875555 -2.01678297\n",
            "    0.01635173]\n",
            "  [ 0.3702284  -0.35788513  0.15769659 ...  0.34097791  0.33640677\n",
            "   -0.69642324]\n",
            "  [ 0.20159549 -0.36784794 -3.39957772 ... -0.10363389  0.66559868\n",
            "   -0.51897635]\n",
            "  ...\n",
            "  [ 0.41285352  0.77132413 -0.91673523 ...  0.48291715  0.82733205\n",
            "    0.27641991]\n",
            "  [-1.01578525 -0.84405585 -0.56891974 ... -0.79237952  0.51892183\n",
            "    2.90146782]\n",
            "  [-3.12391328 -3.53599363 -0.56997401 ... -1.31253595  1.32160957\n",
            "   -0.05614428]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.01431282  0.09106618  0.00861165 ...  0.03606313 -0.05802297\n",
            "   -0.03697124]\n",
            "  [-0.04090107  0.07880304 -0.01577691 ... -0.01532109 -0.03208404\n",
            "   -0.0320456 ]\n",
            "  [-0.09538398  0.0119169   0.10164676 ...  0.02621875  0.0122831\n",
            "   -0.0137345 ]\n",
            "  ...\n",
            "  [ 0.11260183 -0.00897744 -0.00921647 ...  0.06609556  0.07876594\n",
            "    0.01802568]\n",
            "  [-0.01431282  0.09106618  0.00861165 ...  0.03606313 -0.05802297\n",
            "   -0.03697124]\n",
            "  [-0.04090107  0.07880304 -0.01577691 ... -0.01532109 -0.03208404\n",
            "   -0.0320456 ]]\n",
            "\n",
            " [[ 2.98323913 -0.12888622  0.04077921 ... -2.01622121 -1.3015735\n",
            "    2.89823609]\n",
            "  [ 1.54886104  0.40404946 -1.56156247 ... -0.45677039 -0.70835317\n",
            "   -0.45537869]\n",
            "  [-1.05716535  0.93116254 -0.45926317 ...  0.09384125 -1.69340732\n",
            "   -0.43827883]\n",
            "  ...\n",
            "  [ 1.01588474 -0.89524509  1.15783913 ...  0.23709242  1.23980114\n",
            "   -0.53320119]\n",
            "  [ 1.79730467 -0.66921744 -1.3209034  ...  1.04350309  0.76799982\n",
            "    0.2040732 ]\n",
            "  [ 2.00181008  0.48988621  0.74023729 ...  0.03542979 -2.30455575\n",
            "   -1.1326698 ]]\n",
            "\n",
            " [[-0.5754114  -0.21478541  0.89147884 ... -1.08658405  0.30430471\n",
            "    0.39857388]\n",
            "  [ 0.47720676 -0.51979632  0.26975294 ...  0.61355597  0.18791722\n",
            "    0.15251604]\n",
            "  [ 0.26718337  0.20323862 -0.11472957 ...  0.69642759 -0.35174475\n",
            "   -0.09871861]\n",
            "  ...\n",
            "  [ 0.54157269 -0.09431096 -0.21708723 ...  0.33409682 -1.81829871\n",
            "   -0.96132781]\n",
            "  [ 0.19283518  0.79368062  0.41615796 ... -0.85611198 -1.20809544\n",
            "    0.17729072]\n",
            "  [-0.78241281  1.01454685  1.71838945 ... -2.16183931 -1.14492376\n",
            "    0.18641555]]]---------------------\n",
            "\n",
            "\n",
            "(34, 200, 9)\n",
            "(200, 306)\n",
            "[ 1.45087907  1.68676128  4.68674367  2.80196458  2.41487704  3.75616049\n",
            "  1.51706268  1.85134039  7.30933535  1.45619101  1.47758504  4.44476796\n",
            "  3.13954769  2.09266324  3.79818429  1.45573028  1.55140817  7.27237415\n",
            "  1.56362115  1.43067559  4.3120926   3.46395137  2.25670277  4.2761775\n",
            "  1.53549324  1.63962857  6.20949475  1.5550797   1.50695511  4.10303858\n",
            "  3.05832382  2.07643567  3.98319982  1.43625713  1.62544159  7.58669572\n",
            "  1.61751753  1.45955865  4.58258872  3.26543626  2.18628609  3.98912055\n",
            "  1.50435674  1.4958856   7.34798814  1.72552634  1.53892635  4.31699356\n",
            "  2.88147816  2.4636648   3.61927398  1.53470222  1.76134452  6.55687274\n",
            "  1.59752657  1.51446236  4.86454065  3.59993397  2.20230163  3.88863202\n",
            "  1.56718754  1.81931574  7.12651228  1.57600107  1.59542538  6.40834934\n",
            "  3.61647993  3.88421819  4.42486274  1.65269591  2.36512122 11.79575763\n",
            "  1.58140556  1.45659513  3.90147743  3.73918976  2.10457564  3.32216499\n",
            "  1.48720068  1.6258488   6.80636321  1.41712528  1.39293172  4.21957461\n",
            "  2.83103432  1.92965312  3.998492    1.34986578  1.85266919  6.41498688\n",
            "  1.51034781  1.51427366  4.72457977  3.24002108  2.08801414  3.71977452\n",
            "  1.57488291  1.45701178  6.84834985  1.57530959  1.42899381  4.77068038\n",
            "  3.40565991  2.56866014  3.97395982  1.47322643  1.95933338  7.58363563\n",
            "  1.58883355  1.56952842  4.46341932  2.90518476  1.86742899  3.4336589\n",
            "  1.53210011  1.70838387  7.43486548  1.55794096  1.69973804  4.29803827\n",
            "  3.03622651  2.06249478  3.56750091  1.65671319  1.65642313  7.28163661\n",
            "  1.66563556  1.37964105  4.2525102   3.25869731  2.13059236  3.32387423\n",
            "  1.39745507  1.79720113  6.97885279  1.67274024  1.45441172  4.59814214\n",
            "  2.68921154  2.02668995  3.66709088  1.69562274  1.47891916  7.01724106\n",
            "  1.45269476  1.41683113  4.31600652  3.4611187   2.50938378  3.35071912\n",
            "  1.55045188  1.60812389  6.54464668  1.52313287  1.37487354  4.60492032\n",
            "  3.16202197  2.04530989  3.88416812  1.49909952  1.53191502  7.0688702\n",
            "  1.52707127  1.4455195   4.80526688  4.04650656  2.04119976  3.74355236\n",
            "  1.57310087  1.79793326  7.20343014  1.62778755  1.42899959  6.51216915\n",
            "  3.96111221  2.29872714  3.64726018  1.58858057  1.5695799   7.25397225\n",
            "  1.56264317  1.4935262   3.73520851  3.00318977  2.10595404  3.51075625\n",
            "  1.59003499  1.49744099  6.22134109  1.49190644  1.49378921  4.55509948\n",
            "  2.8171685   1.92027697  3.73899534  1.39420242  1.4213626   6.13176125\n",
            "  1.6803015   1.49146107  4.16049194  3.23663439  2.35896805  3.66769398\n",
            "  1.50470882  1.79969374  7.16788711  1.58725084  1.3974125   4.15609769\n",
            "  2.77906285  2.14183359  3.92044308  1.38978749  1.44791597  6.78616219\n",
            "  1.55729003  1.41029045  4.10481213  3.07397109  2.28299838  3.40926193\n",
            "  1.43602763  1.4594371   6.83402771  1.568207    1.40412246  4.05677945\n",
            "  3.20937807  2.35098071  3.44872325  1.26977281  1.41132867  5.81751977\n",
            "  1.53972161  1.42574932  4.20885698  2.77019236  2.14433983  3.33676734\n",
            "  1.56240019  1.57553906  6.6367394   1.54796242  1.49916866  3.8515915\n",
            "  3.44898076  2.32663581  3.61793616  1.54316484  1.45343878  6.8566775\n",
            "  1.54787266  1.51966703  4.14373093  3.34370662  2.13505636  4.05609958\n",
            "  1.55218922  1.36041561  6.73234973  1.42995915  1.38013621  4.21816318\n",
            "  3.05710317  2.14281622  3.66866769  1.54271333  1.57016088  7.6530885\n",
            "  1.6505227   1.54063492  4.81616637  2.97449679  2.24119773  3.6971207\n",
            "  1.42414138  1.63791965  6.70356163  1.60238007  1.62339057  3.85019431\n",
            "  3.09657589  2.12226959  3.58805633  1.55809837  1.85352791  7.42585731\n",
            "  1.55626245  1.6439093   4.22298856  2.94676691  2.14386551  3.57222248\n",
            "  1.48607007  1.66090399  7.84087175  1.4680749   1.59874048  5.01755537\n",
            "  3.59757997  2.03233047  3.78279352  1.5067272   1.50078879  6.99598389]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 1.87059301e+00 -6.23799385e-01 -3.24138664e-01 ... -2.69043138e-01\n",
            "    6.84301882e-01 -1.56197864e-01]\n",
            "  [-7.78849031e-01 -5.36685691e-01  2.09726717e-01 ...  3.16175471e-01\n",
            "    3.41543783e-01 -3.54014410e-02]\n",
            "  [-6.36738813e-01  3.16195600e-01 -4.21355906e-01 ...  4.00328178e-01\n",
            "    2.92912582e-01  1.67432620e-02]\n",
            "  ...\n",
            "  [ 1.07075160e+00  5.61664614e-02  3.91706277e-01 ... -7.14240733e-01\n",
            "   -3.77059270e-01  5.35521674e-02]\n",
            "  [ 1.19301558e+00  7.61197503e-01  1.38893495e-01 ...  3.29018508e-01\n",
            "    2.67309898e-01  2.30546904e-01]\n",
            "  [ 8.66142812e-01  2.46530539e-01 -3.93361523e-01 ...  3.33346012e+00\n",
            "   -3.14185524e-01 -9.99825759e-01]]\n",
            "\n",
            " [[ 2.04445122e-02  1.33171480e-02 -1.48646209e-01 ...  1.76293131e-01\n",
            "    2.09868373e-03  1.28751939e-02]\n",
            "  [ 3.61653077e-02  5.77686317e-02 -1.23641616e-01 ...  9.07391275e-02\n",
            "   -1.07251181e-01  6.84631570e-03]\n",
            "  [ 4.61554175e-02  5.76391108e-02 -2.48369787e-02 ...  3.71791822e-02\n",
            "   -4.67201833e-02 -6.59871465e-02]\n",
            "  ...\n",
            "  [-7.03128252e-02 -2.64098189e-02 -1.88632454e-03 ... -1.02793993e-01\n",
            "    1.65572709e-01  5.15268142e-02]\n",
            "  [-7.91841487e-02  1.59264649e-03  3.18447398e-03 ... -1.15587715e-02\n",
            "    8.06805537e-02  1.19854605e-01]\n",
            "  [ 1.85204544e-02  3.96621746e-02 -8.12952129e-02 ...  1.26041478e-01\n",
            "   -6.16744284e-02 -5.40529830e-02]]\n",
            "\n",
            " [[-1.56826846e+00 -2.52309710e-01 -9.14020544e-01 ...  1.52255927e-01\n",
            "   -1.56527568e+00  1.12114997e+00]\n",
            "  [-3.37250058e-01 -4.38569548e-01 -8.64894731e-01 ...  1.09809559e-01\n",
            "   -1.60505479e+00  6.54330123e-02]\n",
            "  [ 5.86163156e-01 -1.40997996e-01 -1.62652872e-01 ... -1.38167226e-01\n",
            "    3.30245006e-01 -4.86945787e-01]\n",
            "  ...\n",
            "  [ 5.45517778e-01  9.75671969e-01  8.29007207e-01 ... -2.60046425e-01\n",
            "   -4.36561064e-02  8.31384778e-01]\n",
            "  [-4.81260120e-01  5.70203569e-02  3.10099161e-01 ... -6.82325202e-01\n",
            "    1.24836103e+00  5.73218460e-01]\n",
            "  [ 1.43751441e+00  9.80952856e-01 -1.56160162e+00 ... -3.32411420e-01\n",
            "    3.59265360e+00 -4.10525517e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-9.21940702e-01  1.42417877e+00  1.37345658e+00 ... -2.00936007e-01\n",
            "   -3.11910334e-01 -6.90340665e-01]\n",
            "  [-9.61911400e-01  4.16846640e-01  1.06839228e+00 ... -4.89896438e-02\n",
            "    2.91666008e-01 -9.56523184e-02]\n",
            "  [ 1.88694385e-01 -7.43628636e-01 -7.21794393e-01 ...  9.23504704e-01\n",
            "   -2.70790043e-01 -9.94576694e-01]\n",
            "  ...\n",
            "  [-2.08422003e+00 -1.44875617e+00 -5.88976969e+00 ... -6.44596042e-01\n",
            "   -1.81618487e+00  1.66760434e+00]\n",
            "  [ 4.15562749e-01 -5.20744320e-01  3.95102964e+00 ... -1.58356397e+00\n",
            "   -5.58434859e+00 -1.48673150e+00]\n",
            "  [ 1.78570609e+00  4.01260854e-01  6.46055785e+00 ... -1.64409116e+00\n",
            "   -2.15560763e+00 -4.22804791e+00]]\n",
            "\n",
            " [[-1.33348110e+00 -2.31509860e+00 -2.52148390e-01 ...  1.74654275e+00\n",
            "   -8.50844487e-01  7.02185891e-01]\n",
            "  [-1.82849010e-01 -6.53214626e-01  2.28430839e-01 ... -9.22429800e-01\n",
            "   -6.11016078e-02  3.79106264e-01]\n",
            "  [ 1.48293031e+00 -1.43975918e-02  4.15522543e-01 ... -5.42051449e-01\n",
            "    9.98701441e-01  1.12459810e-01]\n",
            "  ...\n",
            "  [ 2.95103541e-01 -1.46990909e+00 -2.15673342e-01 ... -1.01808662e+00\n",
            "   -4.61800502e-01 -7.12079814e-01]\n",
            "  [ 9.23285099e-01 -1.53906542e+00 -4.33497204e-01 ... -1.86417895e+00\n",
            "   -1.18346816e-02 -4.96782327e-01]\n",
            "  [ 1.89677202e+00 -1.66790183e+00 -3.02403824e-01 ... -1.93917355e+00\n",
            "    6.48796512e-01 -3.82070411e-01]]\n",
            "\n",
            " [[ 1.15047811e-02 -2.17210505e-02 -1.16271255e-02 ... -1.98179336e-02\n",
            "   -6.07719840e-02  7.92767671e-02]\n",
            "  [ 7.94078854e-02 -4.41023205e-02  1.56829601e-01 ...  4.56188437e-02\n",
            "   -3.49988068e-02  9.03860016e-02]\n",
            "  [ 5.16156115e-02  6.17925437e-02  3.57408337e-02 ...  4.98891639e-02\n",
            "    7.76726579e-02 -6.71590267e-02]\n",
            "  ...\n",
            "  [ 3.61653077e-02  5.77686317e-02 -1.23641616e-01 ...  9.07391275e-02\n",
            "   -1.07251181e-01  6.84631570e-03]\n",
            "  [ 4.61554175e-02  5.76391108e-02 -2.48369787e-02 ...  3.71791822e-02\n",
            "   -4.67201833e-02 -6.59871465e-02]\n",
            "  [ 1.91049002e-02 -2.92845225e-02  3.56196863e-02 ... -1.93658704e-02\n",
            "   -6.85788323e-02 -3.30032020e-02]]]---------------------\n",
            "\n",
            "\n",
            "(158, 200, 9)\n",
            "(33, 200, 9)\n",
            "(34, 200, 9)\n",
            "yc.shape: (34, 200, 9)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-1.03828302e-18  9.12413527e-18 -1.33675750e-18 -2.86943671e-18\n",
            "  2.46738904e-18  8.76953738e-18  3.23015715e-18 -1.10940668e-17\n",
            " -6.77358496e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 15\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (key_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (value_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (out_projection): Linear(in_features=300, out_features=200, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (key_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (value_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (out_projection): Linear(in_features=300, out_features=200, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: None\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 200\n",
            "\t\tFF Dim: 700\n",
            "\t\tEnc Layers: 5\n",
            "\t\tDec Layers: 6\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [164, 89, 6, 4, 65, 62, 36, 76, 157, 97, 28, 120, 68, 172, 29, 98, 141, 178, 59, 22, 41, 183, 108, 43, 31, 115, 129, 69, 177, 128, 48, 30, 7, 94, 54, 75, 95, 154, 56, 125, 111, 169, 52, 101, 74, 181, 116, 144, 45, 146, 113, 21, 87, 132, 147, 104, 18, 37, 159, 13, 142, 171, 182, 47, 55, 58, 63, 72, 46, 110, 161, 26, 166, 5, 153, 84, 19, 82, 44, 138, 17, 32, 14, 150, 93, 2, 118, 80, 50, 60, 100, 23, 88, 35, 38, 25, 130, 77, 15, 121, 127, 78, 67, 134, 109, 148, 165, 136, 27, 133, 137, 122, 86, 51, 1, 173, 92, 180, 10, 81, 70, 24, 53, 131, 112, 105, 103, 12, 119, 160, 83, 126, 9, 167, 124, 135, 140, 155, 66, 16, 149, 42, 174, 107, 102, 8, 163, 145, 96, 114, 57, 170, 40, 175, 49, 152, 0, 179, 106, 184, 79, 33, 158, 162, 151, 91, 73, 156, 90, 123, 61, 34, 71, 139, 11, 85, 20, 117, 3, 176, 143, 39, 64, 168, 99]\n",
            "self.series.num_trials(split): 34\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 7.2 M \n",
            "----------------------------------------------------\n",
            "7.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 M     Total params\n",
            "28.924    Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [13, 130, 142, 136, 79, 17, 74, 113, 18, 35, 27, 34, 104, 121, 114, 184, 106, 162, 80, 129, 140, 14, 78, 178, 94, 41, 15, 28, 146, 25, 132, 12, 109, 40, 93, 161, 46, 123, 70, 62, 6, 3, 170, 24, 153, 135, 90, 59, 22, 112, 44, 43, 125, 45, 105, 168, 154, 31, 98, 100, 116, 85, 60, 69, 88, 36, 175, 150, 163, 180, 71, 89, 52, 141, 33, 63, 8, 119, 32, 72, 107, 156, 23, 0, 84, 143, 183, 151, 77, 49, 61, 171, 99, 68, 137, 158, 54, 95, 160, 76, 122, 66, 177, 86, 11, 73, 4, 118, 67, 48, 64, 47, 58, 83, 111, 19, 102, 20, 110, 42, 147, 157, 145, 1, 167, 5, 128, 57, 131, 169, 21, 182, 120, 75, 97, 39, 172, 155, 181, 9, 149, 81, 87, 2, 56, 138, 176, 148, 174, 82, 126, 133, 108, 103, 144, 166, 124, 152, 165, 38, 139, 92, 96, 117, 50, 26, 37, 159, 55, 10, 164, 53, 65, 101, 127, 30, 51, 173, 115, 134, 7, 179, 16, 29, 91]\n",
            "self.series.num_trials(split): 33\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:01<00:00,  1.02it/s]self.total_samples: 16\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [13, 130, 142, 136, 79, 17, 74, 113, 18, 35, 27, 34, 104, 121, 114, 184, 106, 162, 80, 129, 140, 14, 78, 178, 94, 41, 15, 28, 146, 25, 132, 12, 109, 40, 93, 161, 46, 123, 70, 62, 6, 3, 170, 24, 153, 135, 90, 59, 22, 112, 44, 43, 125, 45, 105, 168, 154, 31, 98, 100, 116, 85, 60, 69, 88, 36, 175, 150, 163, 180, 71, 89, 52, 141, 33, 63, 8, 119, 32, 72, 107, 156, 23, 0, 84, 143, 183, 151, 77, 49, 61, 171, 99, 68, 137, 158, 54, 95, 160, 76, 122, 66, 177, 86, 11, 73, 4, 118, 67, 48, 64, 47, 58, 83, 111, 19, 102, 20, 110, 42, 147, 157, 145, 1, 167, 5, 128, 57, 131, 169, 21, 182, 120, 75, 97, 39, 172, 155, 181, 9, 149, 81, 87, 2, 56, 138, 176, 148, 174, 82, 126, 133, 108, 103, 144, 166, 124, 152, 165, 38, 139, 92, 96, 117, 50, 26, 37, 159, 55, 10, 164, 53, 65, 101, 127, 30, 51, 173, 115, 134, 7, 179, 16, 29, 91]\n",
            "self.series.num_trials(split): 158\n",
            "Epoch 0:   0% 0/277 [00:00<?, ?it/s] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Traceback (most recent call last):\n",
            "  File \"train_eeg.py\", line 597, in <module>\n",
            "    main(args)\n",
            "  File \"train_eeg.py\", line 572, in main\n",
            "    trainer.fit(forecaster, datamodule=data_module)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 697, in fit\n",
            "    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n",
            "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\n",
            "    return self._run_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 271, in advance\n",
            "    self._outputs = self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 203, in advance\n",
            "    batch_output = self.batch_loop.run(kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 87, in advance\n",
            "    outputs = self.optimizer_loop.run(optimizers, kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
            "    self.advance(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 201, in advance\n",
            "    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 248, in _run_optimization\n",
            "    self._optimizer_step(optimizer, opt_idx, kwargs.get(\"batch_idx\", 0), closure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 367, in _optimizer_step\n",
            "    using_lbfgs=is_lbfgs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1550, in _call_lightning_module_hook\n",
            "    output = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/module.py\", line 1705, in optimizer_step\n",
            "    optimizer.step(closure=optimizer_closure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\", line 168, in step\n",
            "    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/strategies/strategy.py\", line 216, in optimizer_step\n",
            "    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 153, in optimizer_step\n",
            "    return optimizer.step(closure=closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 113, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\", line 119, in step\n",
            "    loss = closure()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 138, in _wrap_closure\n",
            "    closure_result = closure()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 146, in __call__\n",
            "    self._result = self.closure(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 132, in closure\n",
            "    step_output = self._step_fn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 407, in _training_step\n",
            "    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1704, in _call_strategy_hook\n",
            "    output = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/strategies/strategy.py\", line 358, in training_step\n",
            "    return self.model.training_step(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/forecaster.py\", line 259, in training_step\n",
            "    return self.step(batch, train=True)\n",
            "  File \"/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/spacetimeformer_model/spacetimeformer_model.py\", line 204, in step\n",
            "    forward_kwargs=kwargs,\n",
            "  File \"/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/spacetimeformer_model/spacetimeformer_model.py\", line 256, in compute_loss\n",
            "    x_c, y_c, x_t, y_t, **forward_kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/forecaster.py\", line 214, in forward\n",
            "    x_c, seasonal_yc, x_t, y_t, **forward_kwargs\n",
            "  File \"/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/spacetimeformer_model/spacetimeformer_model.py\", line 360, in forward_model_pass\n",
            "    plt.plot([*y_c[0,:,ch], *y_t[0,:,ch]])\n",
            "IndexError: index 9 is out of bounds for dimension 2 with size 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msmooth-dawn-246\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/211awq27\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 26 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./plots_checkpoints_logs/generated, 9ch, single_layer, layers 1, heads 1, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 1, cp 15, tp 1, tf 3, loss mse, local_self_attn none, local_cross_attn none/wandb_logs/421346176/wandb/run-20220923_175936-211awq27/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#single_layer\n",
        "!python train_eeg.py spacetimeformer eeg_social_memory --d_model 200 --d_ff 700 --enc_layers 1 --dec_layers 1 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 30 --d_v 30 --n_heads 1 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .8 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --local_self_attn none --local_cross_attn none --attn_plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORA7l4cf8MBf",
        "outputId": "f8ef358a-ef3c-4d34-8027-b97765d8740e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.run_name: generated, 9ch, enc_layers 1, dec_layers 1, heads 1, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6\n",
            "\n",
            "project EEG connectivity estimate with transformers, entity luca_maurici\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca_maurici\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./plots_checkpoints_logs/generated, 9ch, enc_layers 1, dec_layers 1, heads 1, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6/wandb_logs/339748902/wandb/run-20220924_130333-30q22t2h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhelpful-aardvark-259\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/30q22t2h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "(34, 200, 9)\n",
            "yc.shape: (34, 200, 9)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.0057233   0.00051244  0.01798523 -0.00721104 -0.00505955 -0.01040848\n",
            "  0.00038984 -0.00203722 -0.01552563]\n",
            "std_dev yc: [1.56419104 1.49498691 4.52605295 3.22616496 2.25415107 3.7351453\n",
            " 1.5170044  1.65938239 7.18269641]\n",
            "\n",
            "(158, 200, 9)\n",
            "(200, 1422)\n",
            "[1.60016668 1.46903922 4.92585588 ... 1.56560031 1.6938406  6.068825  ]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-1.74590896e-03 -1.88518797e-02  9.43610800e-02 ... -2.33820926e-02\n",
            "   -4.74136937e-02  7.08147149e-02]\n",
            "  [-6.43457474e-03  6.68456019e-02 -4.72081496e-04 ...  1.70489735e-02\n",
            "    9.30675012e-03  2.22419382e-02]\n",
            "  [-3.51931809e-02  6.13897973e-03  4.26827636e-02 ...  5.93781290e-02\n",
            "    1.19140037e-01 -4.23901588e-02]\n",
            "  ...\n",
            "  [-3.28196024e-02 -9.19168500e-02 -5.89941009e-02 ...  2.19247729e-02\n",
            "    6.56000895e-02  1.96556930e-01]\n",
            "  [-1.13014483e-01 -7.11985572e-02  5.39496324e-02 ...  1.37428499e-01\n",
            "    1.02172411e-01  5.86787255e-02]\n",
            "  [ 4.82439092e-02  3.61518989e-03 -2.69948044e-02 ... -4.67917148e-02\n",
            "   -2.07787428e-02 -2.93986172e-02]]\n",
            "\n",
            " [[-1.47641912e+00  9.84593627e-01 -1.10479156e+00 ... -2.29056295e+00\n",
            "    4.36906379e-01 -5.83118456e-01]\n",
            "  [-6.79289765e-01  1.32079500e+00  1.36299446e+00 ... -1.60908787e+00\n",
            "    3.41964943e-01 -2.43752050e-01]\n",
            "  [ 8.78929284e-01  1.46369608e+00  1.02580579e+00 ...  3.40240401e-01\n",
            "   -2.16888174e-01 -2.62340874e-01]\n",
            "  ...\n",
            "  [ 1.46783920e+00 -1.62761642e-01  6.33578702e-01 ... -7.43526439e-01\n",
            "    3.79024937e-01  6.53923532e-01]\n",
            "  [-8.57143436e-01  3.00296995e-01 -8.97490708e-01 ... -1.59065898e+00\n",
            "    2.82410804e-01  3.38005691e-01]\n",
            "  [-1.22844496e+00  1.37176723e+00 -2.85435628e+00 ... -9.57256052e-02\n",
            "    9.11764707e-01  3.37677342e+00]]\n",
            "\n",
            " [[-1.28641335e+00 -9.33519180e-01  8.67473205e-01 ...  9.95202677e-01\n",
            "    5.19761784e-01 -1.91099590e+00]\n",
            "  [-1.41874095e+00 -1.17301261e+00  6.16018287e-01 ...  1.03958144e+00\n",
            "    6.18601107e-01 -1.28492356e+00]\n",
            "  [-5.59735890e-01 -8.13778303e-02  4.64912385e-01 ...  3.13009216e-01\n",
            "   -3.60375802e-02 -1.04249365e-01]\n",
            "  ...\n",
            "  [ 5.57401139e-01 -7.83498335e-01 -7.24652544e-01 ...  1.88273218e+00\n",
            "   -4.44242847e-01 -2.22228452e+00]\n",
            "  [-2.25105238e-02  2.86794737e-01  1.74365743e+00 ...  1.94423522e+00\n",
            "   -2.05871861e-01 -2.76246021e-01]\n",
            "  [-8.40031090e-01  5.28608000e-01  2.04622991e+00 ...  8.99431369e-01\n",
            "    1.53118105e+00  3.00934023e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.35086093e-02  5.94580900e-02  1.84556458e-02 ...  9.39875459e-02\n",
            "   -1.45550430e-01 -1.04118188e-01]\n",
            "  [-2.00959471e-02 -1.10872712e-01  3.14679084e-02 ...  9.21156344e-02\n",
            "    9.88593252e-02  1.31895880e-02]\n",
            "  [-2.76036364e-02 -5.51987828e-03  1.19199398e-01 ...  4.61042037e-02\n",
            "    1.23105886e-01  3.47084241e-02]\n",
            "  ...\n",
            "  [ 2.46969390e-02  6.94979982e-02 -4.33916413e-02 ...  6.69660994e-02\n",
            "   -6.76709397e-02 -1.14224794e-01]\n",
            "  [ 5.68924105e-02  9.67864269e-02  1.72012249e-02 ...  9.37530391e-02\n",
            "    6.45752099e-02  5.39747241e-04]\n",
            "  [ 7.73287774e-02  9.24363374e-02 -5.35573661e-02 ... -4.07021495e-02\n",
            "   -7.55061619e-03  5.18275926e-02]]\n",
            "\n",
            " [[ 1.24410491e+00  4.56522226e-01 -6.12669170e-02 ... -2.23739618e+00\n",
            "    8.20437947e-01 -2.29270429e+00]\n",
            "  [ 9.83349339e-01  8.83894094e-01 -6.89676837e-01 ... -1.03499924e+00\n",
            "    1.09623442e-01 -8.29550633e-01]\n",
            "  [-6.56350092e-01  1.93330809e-01  1.81778629e-01 ...  5.46996709e-01\n",
            "   -1.96082099e-02  4.71854152e-01]\n",
            "  ...\n",
            "  [-2.56258370e-02 -3.99913690e-01 -4.56947222e-01 ...  4.98276825e-01\n",
            "    1.72762270e-01 -1.94745441e-01]\n",
            "  [ 2.49188839e-01  1.37192834e+00  4.68101329e-01 ...  2.38512849e+00\n",
            "    3.62068014e-02  9.34636944e-02]\n",
            "  [-1.18555365e+00  2.20315614e+00 -1.94300417e+00 ...  1.83571389e+00\n",
            "   -3.97192389e-01  2.35615669e-01]]\n",
            "\n",
            " [[-3.96733905e-01 -3.26997835e+00  8.37493832e-01 ...  2.08839696e+00\n",
            "    9.95375733e-01  1.84218204e+00]\n",
            "  [-2.92043054e-01 -9.09160009e-01  2.97296047e-01 ...  1.24471282e+00\n",
            "    3.57086791e-02  1.07406679e+00]\n",
            "  [ 1.47186340e+00  2.12110038e-01 -1.37008669e-02 ... -1.13016787e+00\n",
            "   -1.09103318e-01 -3.03093111e-01]\n",
            "  ...\n",
            "  [ 6.00888928e-01  1.49592435e+00 -4.97307978e-02 ... -1.49553547e+00\n",
            "   -5.43291500e-01 -7.82928337e-01]\n",
            "  [-1.60621115e+00  4.66136899e-01 -9.85861299e-02 ...  6.90617801e-01\n",
            "   -8.62017911e-02 -8.34847419e-01]\n",
            "  [-3.07428712e+00 -2.07100986e+00 -9.25016908e-01 ...  4.76766248e+00\n",
            "    1.02810456e+00 -1.04855846e+00]]]---------------------\n",
            "\n",
            "\n",
            "(33, 200, 9)\n",
            "(200, 297)\n",
            "[1.45436184 1.63120697 3.83023434 3.01286795 1.99230428 3.7413041\n",
            " 1.79118556 1.53300367 5.67498735 1.66316938 1.35546227 3.83093159\n",
            " 2.56427069 2.17028618 3.20443321 1.62308037 1.50930279 6.59952531\n",
            " 1.77901456 1.50974559 3.75680915 2.92665365 2.23997756 3.48510991\n",
            " 1.57181463 1.4877157  6.31434174 1.78036484 1.58738289 3.60933578\n",
            " 3.08795193 2.55559413 3.49029998 1.55554343 1.37484266 7.02121018\n",
            " 1.5727572  1.49612812 4.04688512 2.70725866 2.20638418 3.42321331\n",
            " 1.55386946 1.45340168 5.6505735  1.60690806 1.37417784 3.8431932\n",
            " 3.03915179 1.90919434 3.39643938 1.52057443 1.48932109 6.43619363\n",
            " 1.66093864 1.35562455 3.58518215 3.33053168 2.01724227 3.48247825\n",
            " 1.46163886 1.73668447 6.39154835 1.35797618 1.55559597 3.8819815\n",
            " 2.6364913  1.97021882 3.44027225 1.47760479 1.34249095 6.41656941\n",
            " 1.53024462 1.5661523  4.04873506 2.69124477 2.27888273 3.30256685\n",
            " 1.56465831 1.42267734 6.68367739 1.61806572 1.42541048 3.73886585\n",
            " 2.52450405 2.00007264 3.3638932  1.59096018 1.36005369 6.47745974\n",
            " 1.49896856 1.56409318 3.79028711 2.72835872 1.9806708  3.28457249\n",
            " 1.63238719 1.38064352 6.23092011 1.47814294 1.47105072 3.90432461\n",
            " 2.82965153 2.20869802 3.09238904 1.73288052 1.42095477 5.85343848\n",
            " 1.47548654 1.48289123 4.20528382 2.61815547 2.0740417  3.23106873\n",
            " 1.54080436 1.52425397 6.9009451  1.77189008 1.43686545 3.76103838\n",
            " 3.31620268 2.11339194 3.74061566 1.65357926 1.40130951 6.59104009\n",
            " 1.61137154 1.35207016 3.93362071 2.86562734 1.9414479  3.400495\n",
            " 1.46222742 1.65600285 6.04453013 1.73923683 1.40587381 3.75131623\n",
            " 2.79432518 2.2616353  3.8174633  1.43010239 1.52371336 6.00860994\n",
            " 1.79355623 1.64697871 3.699941   3.18923568 2.18176622 3.26143204\n",
            " 1.59865316 1.39265248 5.20511686 1.57080951 1.48841742 3.57474572\n",
            " 2.69297527 2.15537061 3.12654365 1.50570575 1.43748513 5.39585564\n",
            " 1.48602818 1.49208384 4.10287439 3.66219679 2.46969643 4.53521361\n",
            " 1.545061   1.4731363  7.33639164 1.66803767 1.57521174 4.08476391\n",
            " 2.57159435 1.93612539 3.62207195 1.47484219 1.4512477  5.49024816\n",
            " 1.58012012 1.37786689 3.50566505 2.96388327 2.08159901 3.25453513\n",
            " 1.52952838 1.5201101  5.92923224 1.75225302 1.40204747 3.97864286\n",
            " 2.67130323 2.21398133 3.3018906  1.42628332 1.2610213  6.20686731\n",
            " 1.57927613 1.52235119 3.6103295  3.06774652 2.23325553 3.71255684\n",
            " 1.5216186  1.42102962 6.22829448 1.66678466 1.47513163 3.81407298\n",
            " 3.02979226 2.16364455 3.69452131 1.40041721 1.58457986 6.14509882\n",
            " 1.67111306 1.37481533 4.17583133 2.79233635 2.327031   3.7324306\n",
            " 1.57643911 1.50301946 6.8617915  1.65637368 1.4319477  4.39402039\n",
            " 3.12821701 2.64438379 4.23103667 1.5669503  2.1391294  7.34179298\n",
            " 1.71651109 1.54396954 3.63983254 3.38768753 2.00901688 3.72836152\n",
            " 1.54707647 1.49001439 6.56168228 1.68970036 1.50580047 4.16861959\n",
            " 2.99615748 2.22246487 3.26952556 1.37224082 1.59128177 5.53883788\n",
            " 1.5675302  1.42774974 3.33534831 2.88004048 1.92594192 3.09488131\n",
            " 1.4835554  1.58249979 5.8227237  1.50158894 1.51581955 3.6882506\n",
            " 2.83031525 1.95242688 3.69123493 1.4199601  1.36763433 5.98480351\n",
            " 1.53357633 1.58807266 3.74629176 2.94453916 1.69916805 3.69710414\n",
            " 1.44199541 1.38396286 6.02209948 1.45217624 1.52732273 3.42452531\n",
            " 2.70091706 1.91614837 3.26312665 1.53106401 1.41044334 7.3524125\n",
            " 1.44572985 1.53994545 3.74232482 2.53080845 1.88063909 3.25430617\n",
            " 1.55866192 1.4684448  6.32916926]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[-0.31069704 -1.6201409   0.99580893 ... -0.57675983  0.23812149\n",
            "    3.26560284]\n",
            "  [ 0.16549339 -0.08283742 -0.53108821 ...  0.35810269  0.17131829\n",
            "   -0.35511918]\n",
            "  [ 0.0472111  -0.08676393 -0.05236945 ...  0.28142576  0.23010811\n",
            "   -0.81585421]\n",
            "  ...\n",
            "  [ 1.41916204 -0.47817519 -2.23607057 ... -0.23947315  0.98156624\n",
            "    0.12690035]\n",
            "  [ 0.15668484  0.28908942 -0.58384231 ...  0.69615229 -1.54390645\n",
            "    1.55689038]\n",
            "  [-1.69354464  1.94231688  2.21163824 ...  0.56599599 -2.97562445\n",
            "    1.54380229]]\n",
            "\n",
            " [[ 1.48053993  4.02299867  0.35004388 ...  1.19997172 -1.8726163\n",
            "   -2.76662613]\n",
            "  [ 0.26473817  0.92853184  0.82702917 ... -0.49720739 -0.3045028\n",
            "   -0.53959747]\n",
            "  [-1.5316349  -2.14844059 -0.27036469 ... -0.04807381  1.42960932\n",
            "    0.62313611]\n",
            "  ...\n",
            "  [-0.72852604 -0.96939611  0.0363341  ... -0.96145595 -0.87824603\n",
            "    0.26985526]\n",
            "  [-0.73193052  1.47579489  1.74870256 ... -0.7731368   0.2999227\n",
            "    2.69671122]\n",
            "  [-0.30818197  3.71879721  2.11709397 ...  0.0874547   2.3205901\n",
            "    3.24284604]]\n",
            "\n",
            " [[-0.36520286 -1.78956003  3.2313185  ...  2.29875555 -2.01678297\n",
            "    0.01635173]\n",
            "  [ 0.3702284  -0.35788513  0.15769659 ...  0.34097791  0.33640677\n",
            "   -0.69642324]\n",
            "  [ 0.20159549 -0.36784794 -3.39957772 ... -0.10363389  0.66559868\n",
            "   -0.51897635]\n",
            "  ...\n",
            "  [ 0.41285352  0.77132413 -0.91673523 ...  0.48291715  0.82733205\n",
            "    0.27641991]\n",
            "  [-1.01578525 -0.84405585 -0.56891974 ... -0.79237952  0.51892183\n",
            "    2.90146782]\n",
            "  [-3.12391328 -3.53599363 -0.56997401 ... -1.31253595  1.32160957\n",
            "   -0.05614428]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.01431282  0.09106618  0.00861165 ...  0.03606313 -0.05802297\n",
            "   -0.03697124]\n",
            "  [-0.04090107  0.07880304 -0.01577691 ... -0.01532109 -0.03208404\n",
            "   -0.0320456 ]\n",
            "  [-0.09538398  0.0119169   0.10164676 ...  0.02621875  0.0122831\n",
            "   -0.0137345 ]\n",
            "  ...\n",
            "  [ 0.11260183 -0.00897744 -0.00921647 ...  0.06609556  0.07876594\n",
            "    0.01802568]\n",
            "  [-0.01431282  0.09106618  0.00861165 ...  0.03606313 -0.05802297\n",
            "   -0.03697124]\n",
            "  [-0.04090107  0.07880304 -0.01577691 ... -0.01532109 -0.03208404\n",
            "   -0.0320456 ]]\n",
            "\n",
            " [[ 2.98323913 -0.12888622  0.04077921 ... -2.01622121 -1.3015735\n",
            "    2.89823609]\n",
            "  [ 1.54886104  0.40404946 -1.56156247 ... -0.45677039 -0.70835317\n",
            "   -0.45537869]\n",
            "  [-1.05716535  0.93116254 -0.45926317 ...  0.09384125 -1.69340732\n",
            "   -0.43827883]\n",
            "  ...\n",
            "  [ 1.01588474 -0.89524509  1.15783913 ...  0.23709242  1.23980114\n",
            "   -0.53320119]\n",
            "  [ 1.79730467 -0.66921744 -1.3209034  ...  1.04350309  0.76799982\n",
            "    0.2040732 ]\n",
            "  [ 2.00181008  0.48988621  0.74023729 ...  0.03542979 -2.30455575\n",
            "   -1.1326698 ]]\n",
            "\n",
            " [[-0.5754114  -0.21478541  0.89147884 ... -1.08658405  0.30430471\n",
            "    0.39857388]\n",
            "  [ 0.47720676 -0.51979632  0.26975294 ...  0.61355597  0.18791722\n",
            "    0.15251604]\n",
            "  [ 0.26718337  0.20323862 -0.11472957 ...  0.69642759 -0.35174475\n",
            "   -0.09871861]\n",
            "  ...\n",
            "  [ 0.54157269 -0.09431096 -0.21708723 ...  0.33409682 -1.81829871\n",
            "   -0.96132781]\n",
            "  [ 0.19283518  0.79368062  0.41615796 ... -0.85611198 -1.20809544\n",
            "    0.17729072]\n",
            "  [-0.78241281  1.01454685  1.71838945 ... -2.16183931 -1.14492376\n",
            "    0.18641555]]]---------------------\n",
            "\n",
            "\n",
            "(34, 200, 9)\n",
            "(200, 306)\n",
            "[ 1.45087907  1.68676128  4.68674367  2.80196458  2.41487704  3.75616049\n",
            "  1.51706268  1.85134039  7.30933535  1.45619101  1.47758504  4.44476796\n",
            "  3.13954769  2.09266324  3.79818429  1.45573028  1.55140817  7.27237415\n",
            "  1.56362115  1.43067559  4.3120926   3.46395137  2.25670277  4.2761775\n",
            "  1.53549324  1.63962857  6.20949475  1.5550797   1.50695511  4.10303858\n",
            "  3.05832382  2.07643567  3.98319982  1.43625713  1.62544159  7.58669572\n",
            "  1.61751753  1.45955865  4.58258872  3.26543626  2.18628609  3.98912055\n",
            "  1.50435674  1.4958856   7.34798814  1.72552634  1.53892635  4.31699356\n",
            "  2.88147816  2.4636648   3.61927398  1.53470222  1.76134452  6.55687274\n",
            "  1.59752657  1.51446236  4.86454065  3.59993397  2.20230163  3.88863202\n",
            "  1.56718754  1.81931574  7.12651228  1.57600107  1.59542538  6.40834934\n",
            "  3.61647993  3.88421819  4.42486274  1.65269591  2.36512122 11.79575763\n",
            "  1.58140556  1.45659513  3.90147743  3.73918976  2.10457564  3.32216499\n",
            "  1.48720068  1.6258488   6.80636321  1.41712528  1.39293172  4.21957461\n",
            "  2.83103432  1.92965312  3.998492    1.34986578  1.85266919  6.41498688\n",
            "  1.51034781  1.51427366  4.72457977  3.24002108  2.08801414  3.71977452\n",
            "  1.57488291  1.45701178  6.84834985  1.57530959  1.42899381  4.77068038\n",
            "  3.40565991  2.56866014  3.97395982  1.47322643  1.95933338  7.58363563\n",
            "  1.58883355  1.56952842  4.46341932  2.90518476  1.86742899  3.4336589\n",
            "  1.53210011  1.70838387  7.43486548  1.55794096  1.69973804  4.29803827\n",
            "  3.03622651  2.06249478  3.56750091  1.65671319  1.65642313  7.28163661\n",
            "  1.66563556  1.37964105  4.2525102   3.25869731  2.13059236  3.32387423\n",
            "  1.39745507  1.79720113  6.97885279  1.67274024  1.45441172  4.59814214\n",
            "  2.68921154  2.02668995  3.66709088  1.69562274  1.47891916  7.01724106\n",
            "  1.45269476  1.41683113  4.31600652  3.4611187   2.50938378  3.35071912\n",
            "  1.55045188  1.60812389  6.54464668  1.52313287  1.37487354  4.60492032\n",
            "  3.16202197  2.04530989  3.88416812  1.49909952  1.53191502  7.0688702\n",
            "  1.52707127  1.4455195   4.80526688  4.04650656  2.04119976  3.74355236\n",
            "  1.57310087  1.79793326  7.20343014  1.62778755  1.42899959  6.51216915\n",
            "  3.96111221  2.29872714  3.64726018  1.58858057  1.5695799   7.25397225\n",
            "  1.56264317  1.4935262   3.73520851  3.00318977  2.10595404  3.51075625\n",
            "  1.59003499  1.49744099  6.22134109  1.49190644  1.49378921  4.55509948\n",
            "  2.8171685   1.92027697  3.73899534  1.39420242  1.4213626   6.13176125\n",
            "  1.6803015   1.49146107  4.16049194  3.23663439  2.35896805  3.66769398\n",
            "  1.50470882  1.79969374  7.16788711  1.58725084  1.3974125   4.15609769\n",
            "  2.77906285  2.14183359  3.92044308  1.38978749  1.44791597  6.78616219\n",
            "  1.55729003  1.41029045  4.10481213  3.07397109  2.28299838  3.40926193\n",
            "  1.43602763  1.4594371   6.83402771  1.568207    1.40412246  4.05677945\n",
            "  3.20937807  2.35098071  3.44872325  1.26977281  1.41132867  5.81751977\n",
            "  1.53972161  1.42574932  4.20885698  2.77019236  2.14433983  3.33676734\n",
            "  1.56240019  1.57553906  6.6367394   1.54796242  1.49916866  3.8515915\n",
            "  3.44898076  2.32663581  3.61793616  1.54316484  1.45343878  6.8566775\n",
            "  1.54787266  1.51966703  4.14373093  3.34370662  2.13505636  4.05609958\n",
            "  1.55218922  1.36041561  6.73234973  1.42995915  1.38013621  4.21816318\n",
            "  3.05710317  2.14281622  3.66866769  1.54271333  1.57016088  7.6530885\n",
            "  1.6505227   1.54063492  4.81616637  2.97449679  2.24119773  3.6971207\n",
            "  1.42414138  1.63791965  6.70356163  1.60238007  1.62339057  3.85019431\n",
            "  3.09657589  2.12226959  3.58805633  1.55809837  1.85352791  7.42585731\n",
            "  1.55626245  1.6439093   4.22298856  2.94676691  2.14386551  3.57222248\n",
            "  1.48607007  1.66090399  7.84087175  1.4680749   1.59874048  5.01755537\n",
            "  3.59757997  2.03233047  3.78279352  1.5067272   1.50078879  6.99598389]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 1.87059301e+00 -6.23799385e-01 -3.24138664e-01 ... -2.69043138e-01\n",
            "    6.84301882e-01 -1.56197864e-01]\n",
            "  [-7.78849031e-01 -5.36685691e-01  2.09726717e-01 ...  3.16175471e-01\n",
            "    3.41543783e-01 -3.54014410e-02]\n",
            "  [-6.36738813e-01  3.16195600e-01 -4.21355906e-01 ...  4.00328178e-01\n",
            "    2.92912582e-01  1.67432620e-02]\n",
            "  ...\n",
            "  [ 1.07075160e+00  5.61664614e-02  3.91706277e-01 ... -7.14240733e-01\n",
            "   -3.77059270e-01  5.35521674e-02]\n",
            "  [ 1.19301558e+00  7.61197503e-01  1.38893495e-01 ...  3.29018508e-01\n",
            "    2.67309898e-01  2.30546904e-01]\n",
            "  [ 8.66142812e-01  2.46530539e-01 -3.93361523e-01 ...  3.33346012e+00\n",
            "   -3.14185524e-01 -9.99825759e-01]]\n",
            "\n",
            " [[ 2.04445122e-02  1.33171480e-02 -1.48646209e-01 ...  1.76293131e-01\n",
            "    2.09868373e-03  1.28751939e-02]\n",
            "  [ 3.61653077e-02  5.77686317e-02 -1.23641616e-01 ...  9.07391275e-02\n",
            "   -1.07251181e-01  6.84631570e-03]\n",
            "  [ 4.61554175e-02  5.76391108e-02 -2.48369787e-02 ...  3.71791822e-02\n",
            "   -4.67201833e-02 -6.59871465e-02]\n",
            "  ...\n",
            "  [-7.03128252e-02 -2.64098189e-02 -1.88632454e-03 ... -1.02793993e-01\n",
            "    1.65572709e-01  5.15268142e-02]\n",
            "  [-7.91841487e-02  1.59264649e-03  3.18447398e-03 ... -1.15587715e-02\n",
            "    8.06805537e-02  1.19854605e-01]\n",
            "  [ 1.85204544e-02  3.96621746e-02 -8.12952129e-02 ...  1.26041478e-01\n",
            "   -6.16744284e-02 -5.40529830e-02]]\n",
            "\n",
            " [[-1.56826846e+00 -2.52309710e-01 -9.14020544e-01 ...  1.52255927e-01\n",
            "   -1.56527568e+00  1.12114997e+00]\n",
            "  [-3.37250058e-01 -4.38569548e-01 -8.64894731e-01 ...  1.09809559e-01\n",
            "   -1.60505479e+00  6.54330123e-02]\n",
            "  [ 5.86163156e-01 -1.40997996e-01 -1.62652872e-01 ... -1.38167226e-01\n",
            "    3.30245006e-01 -4.86945787e-01]\n",
            "  ...\n",
            "  [ 5.45517778e-01  9.75671969e-01  8.29007207e-01 ... -2.60046425e-01\n",
            "   -4.36561064e-02  8.31384778e-01]\n",
            "  [-4.81260120e-01  5.70203569e-02  3.10099161e-01 ... -6.82325202e-01\n",
            "    1.24836103e+00  5.73218460e-01]\n",
            "  [ 1.43751441e+00  9.80952856e-01 -1.56160162e+00 ... -3.32411420e-01\n",
            "    3.59265360e+00 -4.10525517e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-9.21940702e-01  1.42417877e+00  1.37345658e+00 ... -2.00936007e-01\n",
            "   -3.11910334e-01 -6.90340665e-01]\n",
            "  [-9.61911400e-01  4.16846640e-01  1.06839228e+00 ... -4.89896438e-02\n",
            "    2.91666008e-01 -9.56523184e-02]\n",
            "  [ 1.88694385e-01 -7.43628636e-01 -7.21794393e-01 ...  9.23504704e-01\n",
            "   -2.70790043e-01 -9.94576694e-01]\n",
            "  ...\n",
            "  [-2.08422003e+00 -1.44875617e+00 -5.88976969e+00 ... -6.44596042e-01\n",
            "   -1.81618487e+00  1.66760434e+00]\n",
            "  [ 4.15562749e-01 -5.20744320e-01  3.95102964e+00 ... -1.58356397e+00\n",
            "   -5.58434859e+00 -1.48673150e+00]\n",
            "  [ 1.78570609e+00  4.01260854e-01  6.46055785e+00 ... -1.64409116e+00\n",
            "   -2.15560763e+00 -4.22804791e+00]]\n",
            "\n",
            " [[-1.33348110e+00 -2.31509860e+00 -2.52148390e-01 ...  1.74654275e+00\n",
            "   -8.50844487e-01  7.02185891e-01]\n",
            "  [-1.82849010e-01 -6.53214626e-01  2.28430839e-01 ... -9.22429800e-01\n",
            "   -6.11016078e-02  3.79106264e-01]\n",
            "  [ 1.48293031e+00 -1.43975918e-02  4.15522543e-01 ... -5.42051449e-01\n",
            "    9.98701441e-01  1.12459810e-01]\n",
            "  ...\n",
            "  [ 2.95103541e-01 -1.46990909e+00 -2.15673342e-01 ... -1.01808662e+00\n",
            "   -4.61800502e-01 -7.12079814e-01]\n",
            "  [ 9.23285099e-01 -1.53906542e+00 -4.33497204e-01 ... -1.86417895e+00\n",
            "   -1.18346816e-02 -4.96782327e-01]\n",
            "  [ 1.89677202e+00 -1.66790183e+00 -3.02403824e-01 ... -1.93917355e+00\n",
            "    6.48796512e-01 -3.82070411e-01]]\n",
            "\n",
            " [[ 1.15047811e-02 -2.17210505e-02 -1.16271255e-02 ... -1.98179336e-02\n",
            "   -6.07719840e-02  7.92767671e-02]\n",
            "  [ 7.94078854e-02 -4.41023205e-02  1.56829601e-01 ...  4.56188437e-02\n",
            "   -3.49988068e-02  9.03860016e-02]\n",
            "  [ 5.16156115e-02  6.17925437e-02  3.57408337e-02 ...  4.98891639e-02\n",
            "    7.76726579e-02 -6.71590267e-02]\n",
            "  ...\n",
            "  [ 3.61653077e-02  5.77686317e-02 -1.23641616e-01 ...  9.07391275e-02\n",
            "   -1.07251181e-01  6.84631570e-03]\n",
            "  [ 4.61554175e-02  5.76391108e-02 -2.48369787e-02 ...  3.71791822e-02\n",
            "   -4.67201833e-02 -6.59871465e-02]\n",
            "  [ 1.91049002e-02 -2.92845225e-02  3.56196863e-02 ... -1.93658704e-02\n",
            "   -6.85788323e-02 -3.30032020e-02]]]---------------------\n",
            "\n",
            "\n",
            "(158, 200, 9)\n",
            "(33, 200, 9)\n",
            "(34, 200, 9)\n",
            "yc.shape: (34, 200, 9)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-1.03828302e-18  9.12413527e-18 -1.33675750e-18 -2.86943671e-18\n",
            "  2.46738904e-18  8.76953738e-18  3.23015715e-18 -1.10940668e-17\n",
            " -6.77358496e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 15\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=200, out_features=30, bias=True)\n",
            "  (key_projection): Linear(in_features=200, out_features=30, bias=True)\n",
            "  (value_projection): Linear(in_features=200, out_features=30, bias=True)\n",
            "  (out_projection): Linear(in_features=30, out_features=200, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=200, out_features=30, bias=True)\n",
            "  (key_projection): Linear(in_features=200, out_features=30, bias=True)\n",
            "  (value_projection): Linear(in_features=200, out_features=30, bias=True)\n",
            "  (out_projection): Linear(in_features=30, out_features=200, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: None\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 200\n",
            "\t\tFF Dim: 700\n",
            "\t\tEnc Layers: 1\n",
            "\t\tDec Layers: 1\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [164, 89, 6, 4, 65, 62, 36, 76, 157, 97, 28, 120, 68, 172, 29, 98, 141, 178, 59, 22, 41, 183, 108, 43, 31, 115, 129, 69, 177, 128, 48, 30, 7, 94, 54, 75, 95, 154, 56, 125, 111, 169, 52, 101, 74, 181, 116, 144, 45, 146, 113, 21, 87, 132, 147, 104, 18, 37, 159, 13, 142, 171, 182, 47, 55, 58, 63, 72, 46, 110, 161, 26, 166, 5, 153, 84, 19, 82, 44, 138, 17, 32, 14, 150, 93, 2, 118, 80, 50, 60, 100, 23, 88, 35, 38, 25, 130, 77, 15, 121, 127, 78, 67, 134, 109, 148, 165, 136, 27, 133, 137, 122, 86, 51, 1, 173, 92, 180, 10, 81, 70, 24, 53, 131, 112, 105, 103, 12, 119, 160, 83, 126, 9, 167, 124, 135, 140, 155, 66, 16, 149, 42, 174, 107, 102, 8, 163, 145, 96, 114, 57, 170, 40, 175, 49, 152, 0, 179, 106, 184, 79, 33, 158, 162, 151, 91, 73, 156, 90, 123, 61, 34, 71, 139, 11, 85, 20, 117, 3, 176, 143, 39, 64, 168, 99]\n",
            "self.series.num_trials(split): 34\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 662 K \n",
            "----------------------------------------------------\n",
            "662 K     Trainable params\n",
            "0         Non-trainable params\n",
            "662 K     Total params\n",
            "2.649     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [13, 130, 142, 136, 79, 17, 74, 113, 18, 35, 27, 34, 104, 121, 114, 184, 106, 162, 80, 129, 140, 14, 78, 178, 94, 41, 15, 28, 146, 25, 132, 12, 109, 40, 93, 161, 46, 123, 70, 62, 6, 3, 170, 24, 153, 135, 90, 59, 22, 112, 44, 43, 125, 45, 105, 168, 154, 31, 98, 100, 116, 85, 60, 69, 88, 36, 175, 150, 163, 180, 71, 89, 52, 141, 33, 63, 8, 119, 32, 72, 107, 156, 23, 0, 84, 143, 183, 151, 77, 49, 61, 171, 99, 68, 137, 158, 54, 95, 160, 76, 122, 66, 177, 86, 11, 73, 4, 118, 67, 48, 64, 47, 58, 83, 111, 19, 102, 20, 110, 42, 147, 157, 145, 1, 167, 5, 128, 57, 131, 169, 21, 182, 120, 75, 97, 39, 172, 155, 181, 9, 149, 81, 87, 2, 56, 138, 176, 148, 174, 82, 126, 133, 108, 103, 144, 166, 124, 152, 165, 38, 139, 92, 96, 117, 50, 26, 37, 159, 55, 10, 164, 53, 65, 101, 127, 30, 51, 173, 115, 134, 7, 179, 16, 29, 91]\n",
            "self.series.num_trials(split): 33\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:06<00:00,  3.12s/it]self.total_samples: 16\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [13, 130, 142, 136, 79, 17, 74, 113, 18, 35, 27, 34, 104, 121, 114, 184, 106, 162, 80, 129, 140, 14, 78, 178, 94, 41, 15, 28, 146, 25, 132, 12, 109, 40, 93, 161, 46, 123, 70, 62, 6, 3, 170, 24, 153, 135, 90, 59, 22, 112, 44, 43, 125, 45, 105, 168, 154, 31, 98, 100, 116, 85, 60, 69, 88, 36, 175, 150, 163, 180, 71, 89, 52, 141, 33, 63, 8, 119, 32, 72, 107, 156, 23, 0, 84, 143, 183, 151, 77, 49, 61, 171, 99, 68, 137, 158, 54, 95, 160, 76, 122, 66, 177, 86, 11, 73, 4, 118, 67, 48, 64, 47, 58, 83, 111, 19, 102, 20, 110, 42, 147, 157, 145, 1, 167, 5, 128, 57, 131, 169, 21, 182, 120, 75, 97, 39, 172, 155, 181, 9, 149, 81, 87, 2, 56, 138, 176, 148, 174, 82, 126, 133, 108, 103, 144, 166, 124, 152, 165, 38, 139, 92, 96, 117, 50, 26, 37, 159, 55, 10, 164, 53, 65, 101, 127, 30, 51, 173, 115, 134, 7, 179, 16, 29, 91]\n",
            "self.series.num_trials(split): 158\n",
            "Epoch 0:   0% 0/277 [00:00<?, ?it/s] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 0:   7% 20/277 [00:03<00:43,  5.95it/s, loss=1.28, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 0:  79% 220/277 [00:14<00:03, 14.94it/s, loss=0.973, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 240/277 [00:15<00:02, 15.38it/s, loss=0.973, v_num=2t2h]\n",
            "Epoch 0:  94% 260/277 [00:16<00:01, 16.16it/s, loss=0.973, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 41.43it/s]\u001b[A\n",
            "Epoch 0: 100% 277/277 [00:16<00:00, 16.78it/s, loss=0.973, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 0: 100% 277/277 [00:17<00:00, 15.50it/s, loss=0.969, v_num=2t2h]\n",
            "Epoch 1:  65% 180/277 [00:09<00:05, 18.75it/s, loss=0.796, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 1:  79% 220/277 [00:12<00:03, 17.14it/s, loss=0.757, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 240/277 [00:13<00:02, 17.45it/s, loss=0.757, v_num=2t2h]\n",
            "Epoch 1:  94% 260/277 [00:14<00:00, 18.27it/s, loss=0.757, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.01it/s]\u001b[A\n",
            "Epoch 1: 100% 277/277 [00:14<00:00, 18.93it/s, loss=0.757, v_num=2t2h]self.total_samples: 16\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "Epoch 1: 100% 277/277 [00:16<00:00, 17.27it/s, loss=0.749, v_num=2t2h]\n",
            "Epoch 2:  51% 140/277 [00:08<00:08, 17.03it/s, loss=0.67, v_num=2t2h] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 2:  79% 220/277 [00:13<00:03, 16.35it/s, loss=0.636, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  87% 240/277 [00:14<00:02, 16.68it/s, loss=0.636, v_num=2t2h]\n",
            "Epoch 2:  94% 260/277 [00:14<00:00, 17.48it/s, loss=0.636, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.39it/s]\u001b[A\n",
            "Epoch 2: 100% 277/277 [00:15<00:00, 18.13it/s, loss=0.636, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 2: 100% 277/277 [00:16<00:00, 16.74it/s, loss=0.618, v_num=2t2h]\n",
            "Epoch 3:  65% 180/277 [00:09<00:05, 18.16it/s, loss=0.62, v_num=2t2h] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 3:  79% 220/277 [00:13<00:03, 16.62it/s, loss=0.573, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/48 [00:00<?, ?it/s]\u001b[APLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 3:  87% 240/277 [00:15<00:02, 15.51it/s, loss=0.573, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 3:  94% 260/277 [00:17<00:01, 15.01it/s, loss=0.573, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:03<00:00, 10.77it/s]\u001b[A\n",
            "Epoch 3: 100% 277/277 [00:17<00:00, 15.60it/s, loss=0.573, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 3: 100% 277/277 [00:19<00:00, 14.55it/s, loss=0.621, v_num=2t2h]\n",
            "Epoch 4:  14% 40/277 [00:02<00:14, 16.30it/s, loss=0.677, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 4:  72% 200/277 [00:11<00:04, 16.75it/s, loss=0.601, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 4:  79% 220/277 [00:14<00:03, 15.67it/s, loss=0.618, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  87% 240/277 [00:14<00:02, 16.05it/s, loss=0.618, v_num=2t2h]\n",
            "Epoch 4:  94% 260/277 [00:15<00:01, 16.81it/s, loss=0.618, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.77it/s]\u001b[A\n",
            "Epoch 4: 100% 277/277 [00:15<00:00, 17.44it/s, loss=0.618, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 4: 100% 277/277 [00:17<00:00, 16.12it/s, loss=0.614, v_num=2t2h]\n",
            "Epoch 5:  36% 100/277 [00:05<00:10, 16.90it/s, loss=0.611, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 5:  79% 220/277 [00:13<00:03, 16.32it/s, loss=0.555, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  87% 240/277 [00:14<00:02, 16.68it/s, loss=0.555, v_num=2t2h]\n",
            "Epoch 5:  94% 260/277 [00:14<00:00, 17.49it/s, loss=0.555, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.01it/s]\u001b[A\n",
            "Epoch 5: 100% 277/277 [00:15<00:00, 18.12it/s, loss=0.555, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 5: 100% 277/277 [00:16<00:00, 16.69it/s, loss=0.535, v_num=2t2h]\n",
            "Epoch 6:   0% 0/277 [00:00<?, ?it/s, loss=0.535, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 6:  79% 220/277 [00:13<00:03, 16.84it/s, loss=0.506, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  87% 240/277 [00:13<00:02, 17.17it/s, loss=0.506, v_num=2t2h]\n",
            "Epoch 6:  94% 260/277 [00:14<00:00, 17.97it/s, loss=0.506, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.02it/s]\u001b[A\n",
            "Epoch 6: 100% 277/277 [00:14<00:00, 18.61it/s, loss=0.506, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 6: 100% 277/277 [00:16<00:00, 17.11it/s, loss=0.502, v_num=2t2h]\n",
            "Epoch 7:  65% 180/277 [00:10<00:05, 17.90it/s, loss=0.504, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 7:  79% 220/277 [00:14<00:03, 15.26it/s, loss=0.532, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  87% 240/277 [00:15<00:02, 15.34it/s, loss=0.532, v_num=2t2h]\n",
            "Epoch 7:  94% 260/277 [00:16<00:01, 16.06it/s, loss=0.532, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 28.87it/s]\u001b[A\n",
            "Epoch 7: 100% 277/277 [00:16<00:00, 16.69it/s, loss=0.532, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 7: 100% 277/277 [00:17<00:00, 15.40it/s, loss=0.543, v_num=2t2h]\n",
            "Epoch 8:   7% 20/277 [00:01<00:18, 14.27it/s, loss=0.548, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 8:  36% 100/277 [00:06<00:11, 14.94it/s, loss=0.569, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 8:  58% 160/277 [00:10<00:08, 14.55it/s, loss=0.512, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 8:  79% 220/277 [00:15<00:03, 14.36it/s, loss=0.515, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  87% 240/277 [00:16<00:02, 14.78it/s, loss=0.515, v_num=2t2h]\n",
            "Epoch 8:  94% 260/277 [00:16<00:01, 15.51it/s, loss=0.515, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.60it/s]\u001b[A\n",
            "Epoch 8: 100% 277/277 [00:17<00:00, 16.13it/s, loss=0.515, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 8: 100% 277/277 [00:18<00:00, 14.95it/s, loss=0.546, v_num=2t2h]\n",
            "Epoch 9:  43% 120/277 [00:06<00:08, 17.86it/s, loss=0.49, v_num=2t2h] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 9:  79% 220/277 [00:13<00:03, 16.46it/s, loss=0.526, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  87% 240/277 [00:14<00:02, 16.50it/s, loss=0.526, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 9:  94% 260/277 [00:16<00:01, 15.95it/s, loss=0.526, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:02<00:00, 15.86it/s]\u001b[A\n",
            "Epoch 9: 100% 277/277 [00:16<00:00, 16.58it/s, loss=0.526, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 9: 100% 277/277 [00:18<00:00, 15.36it/s, loss=0.516, v_num=2t2h]\n",
            "Epoch 10:  72% 200/277 [00:10<00:04, 18.30it/s, loss=0.477, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 10:  79% 220/277 [00:13<00:03, 16.81it/s, loss=0.515, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  87% 240/277 [00:14<00:02, 17.11it/s, loss=0.515, v_num=2t2h]\n",
            "Epoch 10:  94% 260/277 [00:14<00:00, 17.89it/s, loss=0.515, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.85it/s]\u001b[A\n",
            "Epoch 10: 100% 277/277 [00:14<00:00, 18.54it/s, loss=0.515, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 10: 100% 277/277 [00:16<00:00, 17.07it/s, loss=0.529, v_num=2t2h]\n",
            "Epoch 11:  79% 220/277 [00:11<00:03, 18.50it/s, loss=0.537, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  87% 240/277 [00:12<00:01, 18.77it/s, loss=0.537, v_num=2t2h]\n",
            "Epoch 11:  94% 260/277 [00:13<00:00, 19.60it/s, loss=0.537, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 41.23it/s]\u001b[A\n",
            "Epoch 11: 100% 277/277 [00:13<00:00, 20.23it/s, loss=0.537, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 11: 100% 277/277 [00:14<00:00, 18.50it/s, loss=0.534, v_num=2t2h]\n",
            "Epoch 12:  14% 40/277 [00:02<00:14, 16.07it/s, loss=0.507, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 12:  72% 200/277 [00:12<00:04, 16.11it/s, loss=0.609, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 12:  79% 220/277 [00:14<00:03, 15.14it/s, loss=0.496, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  87% 240/277 [00:15<00:02, 15.51it/s, loss=0.496, v_num=2t2h]\n",
            "Epoch 12:  94% 260/277 [00:15<00:01, 16.30it/s, loss=0.496, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 41.37it/s]\u001b[A\n",
            "Epoch 12: 100% 277/277 [00:16<00:00, 16.94it/s, loss=0.496, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 12: 100% 277/277 [00:17<00:00, 15.70it/s, loss=0.494, v_num=2t2h]\n",
            "Epoch 13:  79% 220/277 [00:11<00:03, 18.55it/s, loss=0.482, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  87% 240/277 [00:12<00:01, 18.79it/s, loss=0.482, v_num=2t2h]\n",
            "Epoch 13:  94% 260/277 [00:13<00:00, 19.61it/s, loss=0.482, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 39.26it/s]\u001b[A\n",
            "Epoch 13: 100% 277/277 [00:13<00:00, 20.20it/s, loss=0.482, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 13: 100% 277/277 [00:15<00:00, 18.46it/s, loss=0.508, v_num=2t2h]\n",
            "Epoch 14:  79% 220/277 [00:12<00:03, 18.32it/s, loss=0.507, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  87% 240/277 [00:12<00:01, 18.55it/s, loss=0.507, v_num=2t2h]\n",
            "Epoch 14:  94% 260/277 [00:13<00:00, 19.33it/s, loss=0.507, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.92it/s]\u001b[A\n",
            "Epoch 14: 100% 277/277 [00:13<00:00, 20.00it/s, loss=0.507, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 14: 100% 277/277 [00:15<00:00, 18.26it/s, loss=0.52, v_num=2t2h] \n",
            "Epoch 15:  79% 220/277 [00:11<00:03, 18.49it/s, loss=0.551, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  87% 240/277 [00:12<00:01, 18.72it/s, loss=0.551, v_num=2t2h]\n",
            "Epoch 15:  94% 260/277 [00:13<00:00, 19.53it/s, loss=0.551, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.52it/s]\u001b[A\n",
            "Epoch 15: 100% 277/277 [00:13<00:00, 20.20it/s, loss=0.551, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 15: 100% 277/277 [00:14<00:00, 18.47it/s, loss=0.511, v_num=2t2h]\n",
            "Epoch 16:  79% 220/277 [00:12<00:03, 17.75it/s, loss=0.493, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  87% 240/277 [00:13<00:02, 18.02it/s, loss=0.493, v_num=2t2h]\n",
            "Validation DataLoader 0:  42% 20/48 [00:00<00:01, 23.98it/s]\u001b[APLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 16:  94% 260/277 [00:15<00:01, 16.80it/s, loss=0.493, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:02<00:00, 14.83it/s]\u001b[A\n",
            "Epoch 16: 100% 277/277 [00:15<00:00, 17.40it/s, loss=0.493, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 16: 100% 277/277 [00:17<00:00, 16.10it/s, loss=0.488, v_num=2t2h]\n",
            "Epoch 17:   0% 0/277 [00:00<?, ?it/s, loss=0.488, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 17:  58% 160/277 [00:09<00:07, 16.23it/s, loss=0.538, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 17:  79% 220/277 [00:14<00:03, 15.13it/s, loss=0.488, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  87% 240/277 [00:15<00:02, 15.51it/s, loss=0.488, v_num=2t2h]\n",
            "Epoch 17:  94% 260/277 [00:15<00:01, 16.28it/s, loss=0.488, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.39it/s]\u001b[A\n",
            "Epoch 17: 100% 277/277 [00:16<00:00, 16.93it/s, loss=0.488, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 17: 100% 277/277 [00:17<00:00, 15.69it/s, loss=0.483, v_num=2t2h]\n",
            "Epoch 18:  29% 80/277 [00:04<00:11, 17.25it/s, loss=0.493, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 18:  79% 220/277 [00:14<00:03, 14.88it/s, loss=0.489, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  87% 240/277 [00:15<00:02, 15.26it/s, loss=0.489, v_num=2t2h]\n",
            "Epoch 18:  94% 260/277 [00:16<00:01, 16.03it/s, loss=0.489, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.50it/s]\u001b[A\n",
            "Epoch 18: 100% 277/277 [00:16<00:00, 16.66it/s, loss=0.489, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 18: 100% 277/277 [00:17<00:00, 15.42it/s, loss=0.521, v_num=2t2h]\n",
            "Epoch 19:  36% 100/277 [00:05<00:09, 17.86it/s, loss=0.5, v_num=2t2h]  PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 19:  79% 220/277 [00:13<00:03, 16.83it/s, loss=0.488, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  87% 240/277 [00:13<00:02, 17.18it/s, loss=0.488, v_num=2t2h]\n",
            "Epoch 19:  94% 260/277 [00:14<00:00, 17.93it/s, loss=0.488, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.45it/s]\u001b[A\n",
            "Epoch 19: 100% 277/277 [00:14<00:00, 18.57it/s, loss=0.488, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 19: 100% 277/277 [00:16<00:00, 17.03it/s, loss=0.494, v_num=2t2h]\n",
            "Epoch 20:  79% 220/277 [00:11<00:03, 18.49it/s, loss=0.485, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  87% 240/277 [00:12<00:01, 18.71it/s, loss=0.485, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 20:  94% 260/277 [00:14<00:00, 17.99it/s, loss=0.485, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:02<00:00, 18.38it/s]\u001b[A\n",
            "Epoch 20: 100% 277/277 [00:14<00:00, 18.60it/s, loss=0.485, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 20: 100% 277/277 [00:16<00:00, 16.99it/s, loss=0.493, v_num=2t2h]\n",
            "Epoch 21:  43% 120/277 [00:06<00:08, 17.75it/s, loss=0.447, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 21:  58% 160/277 [00:09<00:07, 16.13it/s, loss=0.559, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 21:  79% 220/277 [00:14<00:03, 15.46it/s, loss=0.459, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  87% 240/277 [00:15<00:02, 15.85it/s, loss=0.459, v_num=2t2h]\n",
            "Epoch 21:  94% 260/277 [00:15<00:01, 16.63it/s, loss=0.459, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.08it/s]\u001b[A\n",
            "Epoch 21: 100% 277/277 [00:16<00:00, 17.27it/s, loss=0.459, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 21: 100% 277/277 [00:17<00:00, 16.01it/s, loss=0.454, v_num=2t2h]\n",
            "Epoch 22:   0% 0/277 [00:00<?, ?it/s, loss=0.454, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22:   7% 20/277 [00:02<00:31,  8.15it/s, loss=0.501, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22:  22% 60/277 [00:05<00:20, 10.61it/s, loss=0.477, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22:  79% 220/277 [00:15<00:03, 14.38it/s, loss=0.525, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  87% 240/277 [00:16<00:02, 14.48it/s, loss=0.525, v_num=2t2h]\n",
            "Epoch 22:  94% 260/277 [00:17<00:01, 15.21it/s, loss=0.525, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 28.88it/s]\u001b[A\n",
            "Epoch 22: 100% 277/277 [00:17<00:00, 15.84it/s, loss=0.525, v_num=2t2h]self.total_samples: 16\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22: 100% 277/277 [00:20<00:00, 13.82it/s, loss=0.527, v_num=2t2h]\n",
            "Epoch 23:  14% 40/277 [00:02<00:14, 15.85it/s, loss=0.486, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  58% 160/277 [00:10<00:07, 15.99it/s, loss=0.46, v_num=2t2h] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  72% 200/277 [00:13<00:05, 15.12it/s, loss=0.555, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  79% 220/277 [00:15<00:03, 14.29it/s, loss=0.482, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/48 [00:00<?, ?it/s]\u001b[APLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  87% 240/277 [00:17<00:02, 13.76it/s, loss=0.482, v_num=2t2h]\n",
            "Epoch 23:  94% 260/277 [00:17<00:01, 14.51it/s, loss=0.482, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:02<00:00, 18.62it/s]\u001b[A\n",
            "Epoch 23: 100% 277/277 [00:18<00:00, 15.09it/s, loss=0.482, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 23: 100% 277/277 [00:19<00:00, 13.99it/s, loss=0.476, v_num=2t2h]\n",
            "Epoch 24:  79% 220/277 [00:12<00:03, 18.31it/s, loss=0.455, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  87% 240/277 [00:12<00:01, 18.51it/s, loss=0.455, v_num=2t2h]\n",
            "Epoch 24:  94% 260/277 [00:13<00:00, 19.33it/s, loss=0.455, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.53it/s]\u001b[A\n",
            "Epoch 24: 100% 277/277 [00:13<00:00, 19.98it/s, loss=0.455, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 24: 100% 277/277 [00:15<00:00, 18.28it/s, loss=0.454, v_num=2t2h]\n",
            "Epoch 25:   7% 20/277 [00:01<00:17, 14.43it/s, loss=0.455, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 25:  79% 220/277 [00:13<00:03, 16.78it/s, loss=0.483, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  87% 240/277 [00:14<00:02, 17.10it/s, loss=0.483, v_num=2t2h]\n",
            "Epoch 25:  94% 260/277 [00:14<00:00, 17.91it/s, loss=0.483, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Validation DataLoader 0:  83% 40/48 [00:02<00:00, 18.50it/s]\u001b[A\n",
            "Epoch 25: 100% 277/277 [00:16<00:00, 17.22it/s, loss=0.483, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 25: 100% 277/277 [00:17<00:00, 15.96it/s, loss=0.47, v_num=2t2h] \n",
            "Epoch 26:  79% 220/277 [00:12<00:03, 18.09it/s, loss=0.509, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  87% 240/277 [00:13<00:02, 18.38it/s, loss=0.509, v_num=2t2h]\n",
            "Epoch 26:  94% 260/277 [00:13<00:00, 19.09it/s, loss=0.509, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 37.41it/s]\u001b[A\n",
            "Epoch 26: 100% 277/277 [00:14<00:00, 19.73it/s, loss=0.509, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 26: 100% 277/277 [00:15<00:00, 17.78it/s, loss=0.525, v_num=2t2h]\n",
            "Epoch 27:  79% 220/277 [00:12<00:03, 18.33it/s, loss=0.469, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  87% 240/277 [00:12<00:01, 18.58it/s, loss=0.469, v_num=2t2h]\n",
            "Epoch 27:  94% 260/277 [00:13<00:00, 19.34it/s, loss=0.469, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.96it/s]\u001b[A\n",
            "Epoch 27: 100% 277/277 [00:13<00:00, 19.99it/s, loss=0.469, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 27: 100% 277/277 [00:15<00:00, 18.31it/s, loss=0.462, v_num=2t2h]\n",
            "Epoch 28:  29% 80/277 [00:04<00:11, 17.24it/s, loss=0.52, v_num=2t2h] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 28:  79% 220/277 [00:13<00:03, 16.78it/s, loss=0.467, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  87% 240/277 [00:14<00:02, 17.08it/s, loss=0.467, v_num=2t2h]\n",
            "Epoch 28:  94% 260/277 [00:14<00:00, 17.85it/s, loss=0.467, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.72it/s]\u001b[A\n",
            "Epoch 28: 100% 277/277 [00:14<00:00, 18.48it/s, loss=0.467, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 28: 100% 277/277 [00:16<00:00, 17.00it/s, loss=0.474, v_num=2t2h]\n",
            "Epoch 29:  79% 220/277 [00:11<00:03, 18.34it/s, loss=0.439, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  87% 240/277 [00:12<00:01, 18.56it/s, loss=0.439, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 29:  94% 260/277 [00:14<00:00, 17.86it/s, loss=0.439, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:02<00:00, 18.33it/s]\u001b[A\n",
            "Epoch 29: 100% 277/277 [00:14<00:00, 18.48it/s, loss=0.439, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 29: 100% 277/277 [00:16<00:00, 17.00it/s, loss=0.493, v_num=2t2h]\n",
            "Epoch 30:   7% 20/277 [00:01<00:16, 15.22it/s, loss=0.55, v_num=2t2h] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 30:  79% 220/277 [00:13<00:03, 16.31it/s, loss=0.524, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  87% 240/277 [00:14<00:02, 16.67it/s, loss=0.524, v_num=2t2h]\n",
            "Epoch 30:  94% 260/277 [00:14<00:00, 17.42it/s, loss=0.524, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.57it/s]\u001b[A\n",
            "Epoch 30: 100% 277/277 [00:15<00:00, 18.07it/s, loss=0.524, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 30: 100% 277/277 [00:16<00:00, 16.68it/s, loss=0.495, v_num=2t2h]\n",
            "Epoch 31:  79% 220/277 [00:11<00:03, 18.41it/s, loss=0.478, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  87% 240/277 [00:12<00:01, 18.63it/s, loss=0.478, v_num=2t2h]\n",
            "Epoch 31:  94% 260/277 [00:13<00:00, 19.44it/s, loss=0.478, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 39.73it/s]\u001b[A\n",
            "Epoch 31: 100% 277/277 [00:13<00:00, 20.01it/s, loss=0.478, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 31: 100% 277/277 [00:15<00:00, 18.32it/s, loss=0.476, v_num=2t2h]\n",
            "Epoch 32:  65% 180/277 [00:09<00:05, 18.18it/s, loss=0.474, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 32:  79% 220/277 [00:13<00:03, 16.81it/s, loss=0.454, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  87% 240/277 [00:14<00:02, 17.04it/s, loss=0.454, v_num=2t2h]\n",
            "Epoch 32:  94% 260/277 [00:14<00:00, 17.85it/s, loss=0.454, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.73it/s]\u001b[A\n",
            "Epoch 32: 100% 277/277 [00:14<00:00, 18.51it/s, loss=0.454, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 32: 100% 277/277 [00:16<00:00, 17.04it/s, loss=0.45, v_num=2t2h] \n",
            "Epoch 33:  79% 220/277 [00:12<00:03, 17.95it/s, loss=0.469, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  87% 240/277 [00:13<00:02, 18.24it/s, loss=0.469, v_num=2t2h]\n",
            "Epoch 33:  94% 260/277 [00:13<00:00, 19.01it/s, loss=0.469, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.96it/s]\u001b[A\n",
            "Epoch 33: 100% 277/277 [00:14<00:00, 19.67it/s, loss=0.469, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 33: 100% 277/277 [00:15<00:00, 17.79it/s, loss=0.518, v_num=2t2h]\n",
            "Epoch 34:  58% 160/277 [00:08<00:06, 18.17it/s, loss=0.454, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 34:  79% 220/277 [00:13<00:03, 16.73it/s, loss=0.515, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  87% 240/277 [00:14<00:02, 17.05it/s, loss=0.515, v_num=2t2h]\n",
            "Epoch 34:  94% 260/277 [00:14<00:00, 17.85it/s, loss=0.515, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.02it/s]\u001b[A\n",
            "Epoch 34: 100% 277/277 [00:14<00:00, 18.50it/s, loss=0.515, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 34: 100% 277/277 [00:16<00:00, 17.01it/s, loss=0.492, v_num=2t2h]\n",
            "Epoch 35:  14% 40/277 [00:02<00:15, 15.60it/s, loss=0.452, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 35:  79% 220/277 [00:13<00:03, 16.57it/s, loss=0.467, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  87% 240/277 [00:14<00:02, 16.92it/s, loss=0.467, v_num=2t2h]\n",
            "Epoch 35:  94% 260/277 [00:14<00:00, 17.70it/s, loss=0.467, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.51it/s]\u001b[A\n",
            "Epoch 35: 100% 277/277 [00:15<00:00, 18.36it/s, loss=0.467, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 35: 100% 277/277 [00:16<00:00, 16.82it/s, loss=0.452, v_num=2t2h]\n",
            "Epoch 36:   7% 20/277 [00:01<00:16, 15.42it/s, loss=0.469, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 36:  79% 220/277 [00:13<00:03, 16.61it/s, loss=0.493, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  87% 240/277 [00:14<00:02, 16.57it/s, loss=0.493, v_num=2t2h]\n",
            "Epoch 36:  94% 260/277 [00:14<00:00, 17.37it/s, loss=0.493, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:00<00:00, 40.50it/s]\u001b[A\n",
            "Epoch 36: 100% 277/277 [00:15<00:00, 18.02it/s, loss=0.493, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 36: 100% 277/277 [00:16<00:00, 16.42it/s, loss=0.481, v_num=2t2h]\n",
            "Epoch 37:  72% 200/277 [00:10<00:04, 18.24it/s, loss=0.458, v_num=2t2h]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 37:  79% 220/277 [00:13<00:03, 16.63it/s, loss=0.505, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  87% 240/277 [00:14<00:02, 16.93it/s, loss=0.505, v_num=2t2h]\n",
            "Epoch 37:  94% 260/277 [00:14<00:00, 17.70it/s, loss=0.505, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 38.85it/s]\u001b[A\n",
            "Epoch 37: 100% 277/277 [00:15<00:00, 18.34it/s, loss=0.505, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 37: 100% 277/277 [00:16<00:00, 16.90it/s, loss=0.539, v_num=2t2h]\n",
            "Epoch 38:  79% 220/277 [00:11<00:03, 18.45it/s, loss=0.488, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  87% 240/277 [00:12<00:01, 18.69it/s, loss=0.488, v_num=2t2h]\n",
            "Epoch 38:  94% 260/277 [00:13<00:00, 19.39it/s, loss=0.488, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 37.38it/s]\u001b[A\n",
            "Epoch 38: 100% 277/277 [00:13<00:00, 20.05it/s, loss=0.488, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 38: 100% 277/277 [00:15<00:00, 18.34it/s, loss=0.479, v_num=2t2h]\n",
            "Epoch 39:  79% 220/277 [00:12<00:03, 18.16it/s, loss=0.472, v_num=2t2h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  87% 240/277 [00:13<00:02, 18.42it/s, loss=0.472, v_num=2t2h]\n",
            "Epoch 39:  94% 260/277 [00:13<00:00, 19.22it/s, loss=0.472, v_num=2t2h]\n",
            "Validation DataLoader 0:  83% 40/48 [00:01<00:00, 39.39it/s]\u001b[A\n",
            "Epoch 39: 100% 277/277 [00:13<00:00, 19.87it/s, loss=0.472, v_num=2t2h]self.total_samples: 16\n",
            "Epoch 39: 100% 277/277 [00:15<00:00, 18.14it/s, loss=0.463, v_num=2t2h]\n",
            "Epoch 39: 100% 277/277 [00:15<00:00, 18.13it/s, loss=0.463, v_num=2t2h]\n",
            "torch.Size([128, 15, 9])\n",
            "torch.Size([15, 1152])\n",
            "[1.6708594  1.52380115 1.74496912 ... 1.80628531 0.95369818 2.49001359]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▃▄▆▇███████▇▇▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁███████████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▃▂▂▂▂▁▁▂▁▂▃▁▁▂▁▁▂▂▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▃▂▂▂▂▁▁▂▁▂▃▁▁▂▁▁▂▂▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇▄▄▄▄▃▃▂▃▃▂▃▂▁▂▂▂▂▃▂▂▂▁▁▂▂▂▂▁▁▁▁▂▂▂▂▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▃▃▂▃▄▃▇▃▃▃▃▂▂▃▂▂▂▂▅▂▁▆▂▂▃▃▂▂▂▁▃▄▃▂█▂▁▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▃▂▂▂▂▁▁▂▁▂▃▁▁▂▁▁▂▂▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇▄▄▄▄▃▃▂▃▃▂▃▂▁▂▂▂▂▃▂▂▂▁▁▂▂▂▂▁▁▁▁▂▂▂▂▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▃▂▂▂▂▁▁▂▁▂▃▁▁▂▁▁▂▂▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▅▅▄▃▃▃▃▄▄▃▂▃▃▂▃▂▃▃▂▁▁▂▂▁▃▂▃▂▃▁▂▃▃▂▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape █▅▃▃▃▂▂▂▂▂▂▃▂▂▂▁▂▂▁▂▁▁▂▁▂▂▁▁▁▂▁▁▂▂▂▂▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss ▇▄▃▄▂▃▂▂█▂▃▄▃▂▃▁▂▁▂▂▂▂▂▁▁▁▂▂▁▃▁▁▁▂▁▁▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss ▇▄▃▄▂▃▂▂█▂▃▄▃▂▃▁▂▁▂▂▂▂▂▁▁▁▂▂▁▃▁▁▁▂▁▁▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae ▆▄▃▄▂▄▂▂█▂▃▄▃▂▃▂▂▂▂▂▂▂▂▁▁▁▂▂▁▃▁▁▁▂▁▁▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▁▃▁▂▃▆▂▃█▂▃▄▄▅▃▁▃▂▂▃▂▂▁▁▁▃▂▂▂▃▁▁▁▃▁▂▂▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse ▇▄▃▄▂▃▂▂█▂▃▄▃▂▃▁▂▁▂▂▂▂▂▁▁▁▂▂▁▃▁▁▁▂▁▁▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae ▆▄▃▄▂▄▂▂█▂▃▄▃▂▃▂▂▂▂▂▂▂▂▁▁▁▂▂▁▃▁▁▁▂▁▁▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse ▇▄▃▄▂▃▂▂█▂▃▄▃▂▃▁▂▁▂▂▂▂▂▁▁▁▂▂▁▃▁▁▁▂▁▁▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse ▄▃▃▃▂▅▃▂█▂▄▅▄▃▃▂▃▂▂▂▂▂▂▁▁▂▂▂▂▃▁▁▁▂▁▁▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape █▄▃▄▃▃▂▂▄▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▂▁▁▁▂▂▁▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step 9160\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.00011\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.00035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.55478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.55478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.52438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 4.39333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.55478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.52438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.55478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.69002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.12737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 9159\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.00035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.49737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.49737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 6.21347\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.49737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.49737\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.74803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.08462\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mhelpful-aardvark-259\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/30q22t2h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 328 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./plots_checkpoints_logs/generated, 9ch, enc_layers 1, dec_layers 1, heads 1, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6/wandb_logs/339748902/wandb/run-20220924_130333-30q22t2h/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#single_layer small model\n",
        "!python train_eeg.py spacetimeformer eeg_social_memory --d_model 10 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 2 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .8 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn none --local_cross_attn none --time_emb_dim 6 --attn_plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvJMJ_VT5voM",
        "outputId": "51144554-6e92-4920-f5cf-3b28ccf79e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.run_name: generated, 9ch, enc_layers 5, dec_layers 6, heads 10, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6\n",
            "\n",
            "project EEG connectivity estimate with transformers, entity luca_maurici\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca_maurici\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./plots_checkpoints_logs/generated, 9ch, enc_layers 5, dec_layers 6, heads 10, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6/wandb_logs/917174274/wandb/run-20220924_135702-ussqsjxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-thunder-266\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/ussqsjxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "(34, 200, 9)\n",
            "yc.shape: (34, 200, 9)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.0057233   0.00051244  0.01798523 -0.00721104 -0.00505955 -0.01040848\n",
            "  0.00038984 -0.00203722 -0.01552563]\n",
            "std_dev yc: [1.56419104 1.49498691 4.52605295 3.22616496 2.25415107 3.7351453\n",
            " 1.5170044  1.65938239 7.18269641]\n",
            "\n",
            "(158, 200, 9)\n",
            "(200, 1422)\n",
            "[1.60016668 1.46903922 4.92585588 ... 1.56560031 1.6938406  6.068825  ]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-1.74590896e-03 -1.88518797e-02  9.43610800e-02 ... -2.33820926e-02\n",
            "   -4.74136937e-02  7.08147149e-02]\n",
            "  [-6.43457474e-03  6.68456019e-02 -4.72081496e-04 ...  1.70489735e-02\n",
            "    9.30675012e-03  2.22419382e-02]\n",
            "  [-3.51931809e-02  6.13897973e-03  4.26827636e-02 ...  5.93781290e-02\n",
            "    1.19140037e-01 -4.23901588e-02]\n",
            "  ...\n",
            "  [-3.28196024e-02 -9.19168500e-02 -5.89941009e-02 ...  2.19247729e-02\n",
            "    6.56000895e-02  1.96556930e-01]\n",
            "  [-1.13014483e-01 -7.11985572e-02  5.39496324e-02 ...  1.37428499e-01\n",
            "    1.02172411e-01  5.86787255e-02]\n",
            "  [ 4.82439092e-02  3.61518989e-03 -2.69948044e-02 ... -4.67917148e-02\n",
            "   -2.07787428e-02 -2.93986172e-02]]\n",
            "\n",
            " [[-1.47641912e+00  9.84593627e-01 -1.10479156e+00 ... -2.29056295e+00\n",
            "    4.36906379e-01 -5.83118456e-01]\n",
            "  [-6.79289765e-01  1.32079500e+00  1.36299446e+00 ... -1.60908787e+00\n",
            "    3.41964943e-01 -2.43752050e-01]\n",
            "  [ 8.78929284e-01  1.46369608e+00  1.02580579e+00 ...  3.40240401e-01\n",
            "   -2.16888174e-01 -2.62340874e-01]\n",
            "  ...\n",
            "  [ 1.46783920e+00 -1.62761642e-01  6.33578702e-01 ... -7.43526439e-01\n",
            "    3.79024937e-01  6.53923532e-01]\n",
            "  [-8.57143436e-01  3.00296995e-01 -8.97490708e-01 ... -1.59065898e+00\n",
            "    2.82410804e-01  3.38005691e-01]\n",
            "  [-1.22844496e+00  1.37176723e+00 -2.85435628e+00 ... -9.57256052e-02\n",
            "    9.11764707e-01  3.37677342e+00]]\n",
            "\n",
            " [[-1.28641335e+00 -9.33519180e-01  8.67473205e-01 ...  9.95202677e-01\n",
            "    5.19761784e-01 -1.91099590e+00]\n",
            "  [-1.41874095e+00 -1.17301261e+00  6.16018287e-01 ...  1.03958144e+00\n",
            "    6.18601107e-01 -1.28492356e+00]\n",
            "  [-5.59735890e-01 -8.13778303e-02  4.64912385e-01 ...  3.13009216e-01\n",
            "   -3.60375802e-02 -1.04249365e-01]\n",
            "  ...\n",
            "  [ 5.57401139e-01 -7.83498335e-01 -7.24652544e-01 ...  1.88273218e+00\n",
            "   -4.44242847e-01 -2.22228452e+00]\n",
            "  [-2.25105238e-02  2.86794737e-01  1.74365743e+00 ...  1.94423522e+00\n",
            "   -2.05871861e-01 -2.76246021e-01]\n",
            "  [-8.40031090e-01  5.28608000e-01  2.04622991e+00 ...  8.99431369e-01\n",
            "    1.53118105e+00  3.00934023e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.35086093e-02  5.94580900e-02  1.84556458e-02 ...  9.39875459e-02\n",
            "   -1.45550430e-01 -1.04118188e-01]\n",
            "  [-2.00959471e-02 -1.10872712e-01  3.14679084e-02 ...  9.21156344e-02\n",
            "    9.88593252e-02  1.31895880e-02]\n",
            "  [-2.76036364e-02 -5.51987828e-03  1.19199398e-01 ...  4.61042037e-02\n",
            "    1.23105886e-01  3.47084241e-02]\n",
            "  ...\n",
            "  [ 2.46969390e-02  6.94979982e-02 -4.33916413e-02 ...  6.69660994e-02\n",
            "   -6.76709397e-02 -1.14224794e-01]\n",
            "  [ 5.68924105e-02  9.67864269e-02  1.72012249e-02 ...  9.37530391e-02\n",
            "    6.45752099e-02  5.39747241e-04]\n",
            "  [ 7.73287774e-02  9.24363374e-02 -5.35573661e-02 ... -4.07021495e-02\n",
            "   -7.55061619e-03  5.18275926e-02]]\n",
            "\n",
            " [[ 1.24410491e+00  4.56522226e-01 -6.12669170e-02 ... -2.23739618e+00\n",
            "    8.20437947e-01 -2.29270429e+00]\n",
            "  [ 9.83349339e-01  8.83894094e-01 -6.89676837e-01 ... -1.03499924e+00\n",
            "    1.09623442e-01 -8.29550633e-01]\n",
            "  [-6.56350092e-01  1.93330809e-01  1.81778629e-01 ...  5.46996709e-01\n",
            "   -1.96082099e-02  4.71854152e-01]\n",
            "  ...\n",
            "  [-2.56258370e-02 -3.99913690e-01 -4.56947222e-01 ...  4.98276825e-01\n",
            "    1.72762270e-01 -1.94745441e-01]\n",
            "  [ 2.49188839e-01  1.37192834e+00  4.68101329e-01 ...  2.38512849e+00\n",
            "    3.62068014e-02  9.34636944e-02]\n",
            "  [-1.18555365e+00  2.20315614e+00 -1.94300417e+00 ...  1.83571389e+00\n",
            "   -3.97192389e-01  2.35615669e-01]]\n",
            "\n",
            " [[-3.96733905e-01 -3.26997835e+00  8.37493832e-01 ...  2.08839696e+00\n",
            "    9.95375733e-01  1.84218204e+00]\n",
            "  [-2.92043054e-01 -9.09160009e-01  2.97296047e-01 ...  1.24471282e+00\n",
            "    3.57086791e-02  1.07406679e+00]\n",
            "  [ 1.47186340e+00  2.12110038e-01 -1.37008669e-02 ... -1.13016787e+00\n",
            "   -1.09103318e-01 -3.03093111e-01]\n",
            "  ...\n",
            "  [ 6.00888928e-01  1.49592435e+00 -4.97307978e-02 ... -1.49553547e+00\n",
            "   -5.43291500e-01 -7.82928337e-01]\n",
            "  [-1.60621115e+00  4.66136899e-01 -9.85861299e-02 ...  6.90617801e-01\n",
            "   -8.62017911e-02 -8.34847419e-01]\n",
            "  [-3.07428712e+00 -2.07100986e+00 -9.25016908e-01 ...  4.76766248e+00\n",
            "    1.02810456e+00 -1.04855846e+00]]]---------------------\n",
            "\n",
            "\n",
            "(33, 200, 9)\n",
            "(200, 297)\n",
            "[1.45436184 1.63120697 3.83023434 3.01286795 1.99230428 3.7413041\n",
            " 1.79118556 1.53300367 5.67498735 1.66316938 1.35546227 3.83093159\n",
            " 2.56427069 2.17028618 3.20443321 1.62308037 1.50930279 6.59952531\n",
            " 1.77901456 1.50974559 3.75680915 2.92665365 2.23997756 3.48510991\n",
            " 1.57181463 1.4877157  6.31434174 1.78036484 1.58738289 3.60933578\n",
            " 3.08795193 2.55559413 3.49029998 1.55554343 1.37484266 7.02121018\n",
            " 1.5727572  1.49612812 4.04688512 2.70725866 2.20638418 3.42321331\n",
            " 1.55386946 1.45340168 5.6505735  1.60690806 1.37417784 3.8431932\n",
            " 3.03915179 1.90919434 3.39643938 1.52057443 1.48932109 6.43619363\n",
            " 1.66093864 1.35562455 3.58518215 3.33053168 2.01724227 3.48247825\n",
            " 1.46163886 1.73668447 6.39154835 1.35797618 1.55559597 3.8819815\n",
            " 2.6364913  1.97021882 3.44027225 1.47760479 1.34249095 6.41656941\n",
            " 1.53024462 1.5661523  4.04873506 2.69124477 2.27888273 3.30256685\n",
            " 1.56465831 1.42267734 6.68367739 1.61806572 1.42541048 3.73886585\n",
            " 2.52450405 2.00007264 3.3638932  1.59096018 1.36005369 6.47745974\n",
            " 1.49896856 1.56409318 3.79028711 2.72835872 1.9806708  3.28457249\n",
            " 1.63238719 1.38064352 6.23092011 1.47814294 1.47105072 3.90432461\n",
            " 2.82965153 2.20869802 3.09238904 1.73288052 1.42095477 5.85343848\n",
            " 1.47548654 1.48289123 4.20528382 2.61815547 2.0740417  3.23106873\n",
            " 1.54080436 1.52425397 6.9009451  1.77189008 1.43686545 3.76103838\n",
            " 3.31620268 2.11339194 3.74061566 1.65357926 1.40130951 6.59104009\n",
            " 1.61137154 1.35207016 3.93362071 2.86562734 1.9414479  3.400495\n",
            " 1.46222742 1.65600285 6.04453013 1.73923683 1.40587381 3.75131623\n",
            " 2.79432518 2.2616353  3.8174633  1.43010239 1.52371336 6.00860994\n",
            " 1.79355623 1.64697871 3.699941   3.18923568 2.18176622 3.26143204\n",
            " 1.59865316 1.39265248 5.20511686 1.57080951 1.48841742 3.57474572\n",
            " 2.69297527 2.15537061 3.12654365 1.50570575 1.43748513 5.39585564\n",
            " 1.48602818 1.49208384 4.10287439 3.66219679 2.46969643 4.53521361\n",
            " 1.545061   1.4731363  7.33639164 1.66803767 1.57521174 4.08476391\n",
            " 2.57159435 1.93612539 3.62207195 1.47484219 1.4512477  5.49024816\n",
            " 1.58012012 1.37786689 3.50566505 2.96388327 2.08159901 3.25453513\n",
            " 1.52952838 1.5201101  5.92923224 1.75225302 1.40204747 3.97864286\n",
            " 2.67130323 2.21398133 3.3018906  1.42628332 1.2610213  6.20686731\n",
            " 1.57927613 1.52235119 3.6103295  3.06774652 2.23325553 3.71255684\n",
            " 1.5216186  1.42102962 6.22829448 1.66678466 1.47513163 3.81407298\n",
            " 3.02979226 2.16364455 3.69452131 1.40041721 1.58457986 6.14509882\n",
            " 1.67111306 1.37481533 4.17583133 2.79233635 2.327031   3.7324306\n",
            " 1.57643911 1.50301946 6.8617915  1.65637368 1.4319477  4.39402039\n",
            " 3.12821701 2.64438379 4.23103667 1.5669503  2.1391294  7.34179298\n",
            " 1.71651109 1.54396954 3.63983254 3.38768753 2.00901688 3.72836152\n",
            " 1.54707647 1.49001439 6.56168228 1.68970036 1.50580047 4.16861959\n",
            " 2.99615748 2.22246487 3.26952556 1.37224082 1.59128177 5.53883788\n",
            " 1.5675302  1.42774974 3.33534831 2.88004048 1.92594192 3.09488131\n",
            " 1.4835554  1.58249979 5.8227237  1.50158894 1.51581955 3.6882506\n",
            " 2.83031525 1.95242688 3.69123493 1.4199601  1.36763433 5.98480351\n",
            " 1.53357633 1.58807266 3.74629176 2.94453916 1.69916805 3.69710414\n",
            " 1.44199541 1.38396286 6.02209948 1.45217624 1.52732273 3.42452531\n",
            " 2.70091706 1.91614837 3.26312665 1.53106401 1.41044334 7.3524125\n",
            " 1.44572985 1.53994545 3.74232482 2.53080845 1.88063909 3.25430617\n",
            " 1.55866192 1.4684448  6.32916926]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[-0.31069704 -1.6201409   0.99580893 ... -0.57675983  0.23812149\n",
            "    3.26560284]\n",
            "  [ 0.16549339 -0.08283742 -0.53108821 ...  0.35810269  0.17131829\n",
            "   -0.35511918]\n",
            "  [ 0.0472111  -0.08676393 -0.05236945 ...  0.28142576  0.23010811\n",
            "   -0.81585421]\n",
            "  ...\n",
            "  [ 1.41916204 -0.47817519 -2.23607057 ... -0.23947315  0.98156624\n",
            "    0.12690035]\n",
            "  [ 0.15668484  0.28908942 -0.58384231 ...  0.69615229 -1.54390645\n",
            "    1.55689038]\n",
            "  [-1.69354464  1.94231688  2.21163824 ...  0.56599599 -2.97562445\n",
            "    1.54380229]]\n",
            "\n",
            " [[ 1.48053993  4.02299867  0.35004388 ...  1.19997172 -1.8726163\n",
            "   -2.76662613]\n",
            "  [ 0.26473817  0.92853184  0.82702917 ... -0.49720739 -0.3045028\n",
            "   -0.53959747]\n",
            "  [-1.5316349  -2.14844059 -0.27036469 ... -0.04807381  1.42960932\n",
            "    0.62313611]\n",
            "  ...\n",
            "  [-0.72852604 -0.96939611  0.0363341  ... -0.96145595 -0.87824603\n",
            "    0.26985526]\n",
            "  [-0.73193052  1.47579489  1.74870256 ... -0.7731368   0.2999227\n",
            "    2.69671122]\n",
            "  [-0.30818197  3.71879721  2.11709397 ...  0.0874547   2.3205901\n",
            "    3.24284604]]\n",
            "\n",
            " [[-0.36520286 -1.78956003  3.2313185  ...  2.29875555 -2.01678297\n",
            "    0.01635173]\n",
            "  [ 0.3702284  -0.35788513  0.15769659 ...  0.34097791  0.33640677\n",
            "   -0.69642324]\n",
            "  [ 0.20159549 -0.36784794 -3.39957772 ... -0.10363389  0.66559868\n",
            "   -0.51897635]\n",
            "  ...\n",
            "  [ 0.41285352  0.77132413 -0.91673523 ...  0.48291715  0.82733205\n",
            "    0.27641991]\n",
            "  [-1.01578525 -0.84405585 -0.56891974 ... -0.79237952  0.51892183\n",
            "    2.90146782]\n",
            "  [-3.12391328 -3.53599363 -0.56997401 ... -1.31253595  1.32160957\n",
            "   -0.05614428]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.01431282  0.09106618  0.00861165 ...  0.03606313 -0.05802297\n",
            "   -0.03697124]\n",
            "  [-0.04090107  0.07880304 -0.01577691 ... -0.01532109 -0.03208404\n",
            "   -0.0320456 ]\n",
            "  [-0.09538398  0.0119169   0.10164676 ...  0.02621875  0.0122831\n",
            "   -0.0137345 ]\n",
            "  ...\n",
            "  [ 0.11260183 -0.00897744 -0.00921647 ...  0.06609556  0.07876594\n",
            "    0.01802568]\n",
            "  [-0.01431282  0.09106618  0.00861165 ...  0.03606313 -0.05802297\n",
            "   -0.03697124]\n",
            "  [-0.04090107  0.07880304 -0.01577691 ... -0.01532109 -0.03208404\n",
            "   -0.0320456 ]]\n",
            "\n",
            " [[ 2.98323913 -0.12888622  0.04077921 ... -2.01622121 -1.3015735\n",
            "    2.89823609]\n",
            "  [ 1.54886104  0.40404946 -1.56156247 ... -0.45677039 -0.70835317\n",
            "   -0.45537869]\n",
            "  [-1.05716535  0.93116254 -0.45926317 ...  0.09384125 -1.69340732\n",
            "   -0.43827883]\n",
            "  ...\n",
            "  [ 1.01588474 -0.89524509  1.15783913 ...  0.23709242  1.23980114\n",
            "   -0.53320119]\n",
            "  [ 1.79730467 -0.66921744 -1.3209034  ...  1.04350309  0.76799982\n",
            "    0.2040732 ]\n",
            "  [ 2.00181008  0.48988621  0.74023729 ...  0.03542979 -2.30455575\n",
            "   -1.1326698 ]]\n",
            "\n",
            " [[-0.5754114  -0.21478541  0.89147884 ... -1.08658405  0.30430471\n",
            "    0.39857388]\n",
            "  [ 0.47720676 -0.51979632  0.26975294 ...  0.61355597  0.18791722\n",
            "    0.15251604]\n",
            "  [ 0.26718337  0.20323862 -0.11472957 ...  0.69642759 -0.35174475\n",
            "   -0.09871861]\n",
            "  ...\n",
            "  [ 0.54157269 -0.09431096 -0.21708723 ...  0.33409682 -1.81829871\n",
            "   -0.96132781]\n",
            "  [ 0.19283518  0.79368062  0.41615796 ... -0.85611198 -1.20809544\n",
            "    0.17729072]\n",
            "  [-0.78241281  1.01454685  1.71838945 ... -2.16183931 -1.14492376\n",
            "    0.18641555]]]---------------------\n",
            "\n",
            "\n",
            "(34, 200, 9)\n",
            "(200, 306)\n",
            "[ 1.45087907  1.68676128  4.68674367  2.80196458  2.41487704  3.75616049\n",
            "  1.51706268  1.85134039  7.30933535  1.45619101  1.47758504  4.44476796\n",
            "  3.13954769  2.09266324  3.79818429  1.45573028  1.55140817  7.27237415\n",
            "  1.56362115  1.43067559  4.3120926   3.46395137  2.25670277  4.2761775\n",
            "  1.53549324  1.63962857  6.20949475  1.5550797   1.50695511  4.10303858\n",
            "  3.05832382  2.07643567  3.98319982  1.43625713  1.62544159  7.58669572\n",
            "  1.61751753  1.45955865  4.58258872  3.26543626  2.18628609  3.98912055\n",
            "  1.50435674  1.4958856   7.34798814  1.72552634  1.53892635  4.31699356\n",
            "  2.88147816  2.4636648   3.61927398  1.53470222  1.76134452  6.55687274\n",
            "  1.59752657  1.51446236  4.86454065  3.59993397  2.20230163  3.88863202\n",
            "  1.56718754  1.81931574  7.12651228  1.57600107  1.59542538  6.40834934\n",
            "  3.61647993  3.88421819  4.42486274  1.65269591  2.36512122 11.79575763\n",
            "  1.58140556  1.45659513  3.90147743  3.73918976  2.10457564  3.32216499\n",
            "  1.48720068  1.6258488   6.80636321  1.41712528  1.39293172  4.21957461\n",
            "  2.83103432  1.92965312  3.998492    1.34986578  1.85266919  6.41498688\n",
            "  1.51034781  1.51427366  4.72457977  3.24002108  2.08801414  3.71977452\n",
            "  1.57488291  1.45701178  6.84834985  1.57530959  1.42899381  4.77068038\n",
            "  3.40565991  2.56866014  3.97395982  1.47322643  1.95933338  7.58363563\n",
            "  1.58883355  1.56952842  4.46341932  2.90518476  1.86742899  3.4336589\n",
            "  1.53210011  1.70838387  7.43486548  1.55794096  1.69973804  4.29803827\n",
            "  3.03622651  2.06249478  3.56750091  1.65671319  1.65642313  7.28163661\n",
            "  1.66563556  1.37964105  4.2525102   3.25869731  2.13059236  3.32387423\n",
            "  1.39745507  1.79720113  6.97885279  1.67274024  1.45441172  4.59814214\n",
            "  2.68921154  2.02668995  3.66709088  1.69562274  1.47891916  7.01724106\n",
            "  1.45269476  1.41683113  4.31600652  3.4611187   2.50938378  3.35071912\n",
            "  1.55045188  1.60812389  6.54464668  1.52313287  1.37487354  4.60492032\n",
            "  3.16202197  2.04530989  3.88416812  1.49909952  1.53191502  7.0688702\n",
            "  1.52707127  1.4455195   4.80526688  4.04650656  2.04119976  3.74355236\n",
            "  1.57310087  1.79793326  7.20343014  1.62778755  1.42899959  6.51216915\n",
            "  3.96111221  2.29872714  3.64726018  1.58858057  1.5695799   7.25397225\n",
            "  1.56264317  1.4935262   3.73520851  3.00318977  2.10595404  3.51075625\n",
            "  1.59003499  1.49744099  6.22134109  1.49190644  1.49378921  4.55509948\n",
            "  2.8171685   1.92027697  3.73899534  1.39420242  1.4213626   6.13176125\n",
            "  1.6803015   1.49146107  4.16049194  3.23663439  2.35896805  3.66769398\n",
            "  1.50470882  1.79969374  7.16788711  1.58725084  1.3974125   4.15609769\n",
            "  2.77906285  2.14183359  3.92044308  1.38978749  1.44791597  6.78616219\n",
            "  1.55729003  1.41029045  4.10481213  3.07397109  2.28299838  3.40926193\n",
            "  1.43602763  1.4594371   6.83402771  1.568207    1.40412246  4.05677945\n",
            "  3.20937807  2.35098071  3.44872325  1.26977281  1.41132867  5.81751977\n",
            "  1.53972161  1.42574932  4.20885698  2.77019236  2.14433983  3.33676734\n",
            "  1.56240019  1.57553906  6.6367394   1.54796242  1.49916866  3.8515915\n",
            "  3.44898076  2.32663581  3.61793616  1.54316484  1.45343878  6.8566775\n",
            "  1.54787266  1.51966703  4.14373093  3.34370662  2.13505636  4.05609958\n",
            "  1.55218922  1.36041561  6.73234973  1.42995915  1.38013621  4.21816318\n",
            "  3.05710317  2.14281622  3.66866769  1.54271333  1.57016088  7.6530885\n",
            "  1.6505227   1.54063492  4.81616637  2.97449679  2.24119773  3.6971207\n",
            "  1.42414138  1.63791965  6.70356163  1.60238007  1.62339057  3.85019431\n",
            "  3.09657589  2.12226959  3.58805633  1.55809837  1.85352791  7.42585731\n",
            "  1.55626245  1.6439093   4.22298856  2.94676691  2.14386551  3.57222248\n",
            "  1.48607007  1.66090399  7.84087175  1.4680749   1.59874048  5.01755537\n",
            "  3.59757997  2.03233047  3.78279352  1.5067272   1.50078879  6.99598389]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 1.87059301e+00 -6.23799385e-01 -3.24138664e-01 ... -2.69043138e-01\n",
            "    6.84301882e-01 -1.56197864e-01]\n",
            "  [-7.78849031e-01 -5.36685691e-01  2.09726717e-01 ...  3.16175471e-01\n",
            "    3.41543783e-01 -3.54014410e-02]\n",
            "  [-6.36738813e-01  3.16195600e-01 -4.21355906e-01 ...  4.00328178e-01\n",
            "    2.92912582e-01  1.67432620e-02]\n",
            "  ...\n",
            "  [ 1.07075160e+00  5.61664614e-02  3.91706277e-01 ... -7.14240733e-01\n",
            "   -3.77059270e-01  5.35521674e-02]\n",
            "  [ 1.19301558e+00  7.61197503e-01  1.38893495e-01 ...  3.29018508e-01\n",
            "    2.67309898e-01  2.30546904e-01]\n",
            "  [ 8.66142812e-01  2.46530539e-01 -3.93361523e-01 ...  3.33346012e+00\n",
            "   -3.14185524e-01 -9.99825759e-01]]\n",
            "\n",
            " [[ 2.04445122e-02  1.33171480e-02 -1.48646209e-01 ...  1.76293131e-01\n",
            "    2.09868373e-03  1.28751939e-02]\n",
            "  [ 3.61653077e-02  5.77686317e-02 -1.23641616e-01 ...  9.07391275e-02\n",
            "   -1.07251181e-01  6.84631570e-03]\n",
            "  [ 4.61554175e-02  5.76391108e-02 -2.48369787e-02 ...  3.71791822e-02\n",
            "   -4.67201833e-02 -6.59871465e-02]\n",
            "  ...\n",
            "  [-7.03128252e-02 -2.64098189e-02 -1.88632454e-03 ... -1.02793993e-01\n",
            "    1.65572709e-01  5.15268142e-02]\n",
            "  [-7.91841487e-02  1.59264649e-03  3.18447398e-03 ... -1.15587715e-02\n",
            "    8.06805537e-02  1.19854605e-01]\n",
            "  [ 1.85204544e-02  3.96621746e-02 -8.12952129e-02 ...  1.26041478e-01\n",
            "   -6.16744284e-02 -5.40529830e-02]]\n",
            "\n",
            " [[-1.56826846e+00 -2.52309710e-01 -9.14020544e-01 ...  1.52255927e-01\n",
            "   -1.56527568e+00  1.12114997e+00]\n",
            "  [-3.37250058e-01 -4.38569548e-01 -8.64894731e-01 ...  1.09809559e-01\n",
            "   -1.60505479e+00  6.54330123e-02]\n",
            "  [ 5.86163156e-01 -1.40997996e-01 -1.62652872e-01 ... -1.38167226e-01\n",
            "    3.30245006e-01 -4.86945787e-01]\n",
            "  ...\n",
            "  [ 5.45517778e-01  9.75671969e-01  8.29007207e-01 ... -2.60046425e-01\n",
            "   -4.36561064e-02  8.31384778e-01]\n",
            "  [-4.81260120e-01  5.70203569e-02  3.10099161e-01 ... -6.82325202e-01\n",
            "    1.24836103e+00  5.73218460e-01]\n",
            "  [ 1.43751441e+00  9.80952856e-01 -1.56160162e+00 ... -3.32411420e-01\n",
            "    3.59265360e+00 -4.10525517e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-9.21940702e-01  1.42417877e+00  1.37345658e+00 ... -2.00936007e-01\n",
            "   -3.11910334e-01 -6.90340665e-01]\n",
            "  [-9.61911400e-01  4.16846640e-01  1.06839228e+00 ... -4.89896438e-02\n",
            "    2.91666008e-01 -9.56523184e-02]\n",
            "  [ 1.88694385e-01 -7.43628636e-01 -7.21794393e-01 ...  9.23504704e-01\n",
            "   -2.70790043e-01 -9.94576694e-01]\n",
            "  ...\n",
            "  [-2.08422003e+00 -1.44875617e+00 -5.88976969e+00 ... -6.44596042e-01\n",
            "   -1.81618487e+00  1.66760434e+00]\n",
            "  [ 4.15562749e-01 -5.20744320e-01  3.95102964e+00 ... -1.58356397e+00\n",
            "   -5.58434859e+00 -1.48673150e+00]\n",
            "  [ 1.78570609e+00  4.01260854e-01  6.46055785e+00 ... -1.64409116e+00\n",
            "   -2.15560763e+00 -4.22804791e+00]]\n",
            "\n",
            " [[-1.33348110e+00 -2.31509860e+00 -2.52148390e-01 ...  1.74654275e+00\n",
            "   -8.50844487e-01  7.02185891e-01]\n",
            "  [-1.82849010e-01 -6.53214626e-01  2.28430839e-01 ... -9.22429800e-01\n",
            "   -6.11016078e-02  3.79106264e-01]\n",
            "  [ 1.48293031e+00 -1.43975918e-02  4.15522543e-01 ... -5.42051449e-01\n",
            "    9.98701441e-01  1.12459810e-01]\n",
            "  ...\n",
            "  [ 2.95103541e-01 -1.46990909e+00 -2.15673342e-01 ... -1.01808662e+00\n",
            "   -4.61800502e-01 -7.12079814e-01]\n",
            "  [ 9.23285099e-01 -1.53906542e+00 -4.33497204e-01 ... -1.86417895e+00\n",
            "   -1.18346816e-02 -4.96782327e-01]\n",
            "  [ 1.89677202e+00 -1.66790183e+00 -3.02403824e-01 ... -1.93917355e+00\n",
            "    6.48796512e-01 -3.82070411e-01]]\n",
            "\n",
            " [[ 1.15047811e-02 -2.17210505e-02 -1.16271255e-02 ... -1.98179336e-02\n",
            "   -6.07719840e-02  7.92767671e-02]\n",
            "  [ 7.94078854e-02 -4.41023205e-02  1.56829601e-01 ...  4.56188437e-02\n",
            "   -3.49988068e-02  9.03860016e-02]\n",
            "  [ 5.16156115e-02  6.17925437e-02  3.57408337e-02 ...  4.98891639e-02\n",
            "    7.76726579e-02 -6.71590267e-02]\n",
            "  ...\n",
            "  [ 3.61653077e-02  5.77686317e-02 -1.23641616e-01 ...  9.07391275e-02\n",
            "   -1.07251181e-01  6.84631570e-03]\n",
            "  [ 4.61554175e-02  5.76391108e-02 -2.48369787e-02 ...  3.71791822e-02\n",
            "   -4.67201833e-02 -6.59871465e-02]\n",
            "  [ 1.91049002e-02 -2.92845225e-02  3.56196863e-02 ... -1.93658704e-02\n",
            "   -6.85788323e-02 -3.30032020e-02]]]---------------------\n",
            "\n",
            "\n",
            "(158, 200, 9)\n",
            "(33, 200, 9)\n",
            "(34, 200, 9)\n",
            "yc.shape: (34, 200, 9)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-1.03828302e-18  9.12413527e-18 -1.33675750e-18 -2.86943671e-18\n",
            "  2.46738904e-18  8.76953738e-18  3.23015715e-18 -1.10940668e-17\n",
            " -6.77358496e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 15\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 200\n",
            "\t\tFF Dim: 700\n",
            "\t\tEnc Layers: 5\n",
            "\t\tDec Layers: 6\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [164, 89, 6, 4, 65, 62, 36, 76, 157, 97, 28, 120, 68, 172, 29, 98, 141, 178, 59, 22, 41, 183, 108, 43, 31, 115, 129, 69, 177, 128, 48, 30, 7, 94, 54, 75, 95, 154, 56, 125, 111, 169, 52, 101, 74, 181, 116, 144, 45, 146, 113, 21, 87, 132, 147, 104, 18, 37, 159, 13, 142, 171, 182, 47, 55, 58, 63, 72, 46, 110, 161, 26, 166, 5, 153, 84, 19, 82, 44, 138, 17, 32, 14, 150, 93, 2, 118, 80, 50, 60, 100, 23, 88, 35, 38, 25, 130, 77, 15, 121, 127, 78, 67, 134, 109, 148, 165, 136, 27, 133, 137, 122, 86, 51, 1, 173, 92, 180, 10, 81, 70, 24, 53, 131, 112, 105, 103, 12, 119, 160, 83, 126, 9, 167, 124, 135, 140, 155, 66, 16, 149, 42, 174, 107, 102, 8, 163, 145, 96, 114, 57, 170, 40, 175, 49, 152, 0, 179, 106, 184, 79, 33, 158, 162, 151, 91, 73, 156, 90, 123, 61, 34, 71, 139, 11, 85, 20, 117, 3, 176, 143, 39, 64, 168, 99]\n",
            "self.series.num_trials(split): 34\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 7.2 M \n",
            "----------------------------------------------------\n",
            "7.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 M     Total params\n",
            "28.924    Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [13, 130, 142, 136, 79, 17, 74, 113, 18, 35, 27, 34, 104, 121, 114, 184, 106, 162, 80, 129, 140, 14, 78, 178, 94, 41, 15, 28, 146, 25, 132, 12, 109, 40, 93, 161, 46, 123, 70, 62, 6, 3, 170, 24, 153, 135, 90, 59, 22, 112, 44, 43, 125, 45, 105, 168, 154, 31, 98, 100, 116, 85, 60, 69, 88, 36, 175, 150, 163, 180, 71, 89, 52, 141, 33, 63, 8, 119, 32, 72, 107, 156, 23, 0, 84, 143, 183, 151, 77, 49, 61, 171, 99, 68, 137, 158, 54, 95, 160, 76, 122, 66, 177, 86, 11, 73, 4, 118, 67, 48, 64, 47, 58, 83, 111, 19, 102, 20, 110, 42, 147, 157, 145, 1, 167, 5, 128, 57, 131, 169, 21, 182, 120, 75, 97, 39, 172, 155, 181, 9, 149, 81, 87, 2, 56, 138, 176, 148, 174, 82, 126, 133, 108, 103, 144, 166, 124, 152, 165, 38, 139, 92, 96, 117, 50, 26, 37, 159, 55, 10, 164, 53, 65, 101, 127, 30, 51, 173, 115, 134, 7, 179, 16, 29, 91]\n",
            "self.series.num_trials(split): 33\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:01<00:00,  1.02it/s]self.total_samples: 16\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -15\n",
            "self._slice_start_points: [13, 130, 142, 136, 79, 17, 74, 113, 18, 35, 27, 34, 104, 121, 114, 184, 106, 162, 80, 129, 140, 14, 78, 178, 94, 41, 15, 28, 146, 25, 132, 12, 109, 40, 93, 161, 46, 123, 70, 62, 6, 3, 170, 24, 153, 135, 90, 59, 22, 112, 44, 43, 125, 45, 105, 168, 154, 31, 98, 100, 116, 85, 60, 69, 88, 36, 175, 150, 163, 180, 71, 89, 52, 141, 33, 63, 8, 119, 32, 72, 107, 156, 23, 0, 84, 143, 183, 151, 77, 49, 61, 171, 99, 68, 137, 158, 54, 95, 160, 76, 122, 66, 177, 86, 11, 73, 4, 118, 67, 48, 64, 47, 58, 83, 111, 19, 102, 20, 110, 42, 147, 157, 145, 1, 167, 5, 128, 57, 131, 169, 21, 182, 120, 75, 97, 39, 172, 155, 181, 9, 149, 81, 87, 2, 56, 138, 176, 148, 174, 82, 126, 133, 108, 103, 144, 166, 124, 152, 165, 38, 139, 92, 96, 117, 50, 26, 37, 159, 55, 10, 164, 53, 65, 101, 127, 30, 51, 173, 115, 134, 7, 179, 16, 29, 91]\n",
            "self.series.num_trials(split): 158\n",
            "Epoch 0:   0% 0/277 [00:00<?, ?it/s] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 0:   7% 20/277 [00:08<01:50,  2.32it/s, loss=1.06, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 0:  79% 220/277 [01:19<00:20,  2.77it/s, loss=0.868, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  87% 240/277 [01:23<00:12,  2.86it/s, loss=0.868, v_num=sjxu]\n",
            "Epoch 0:  94% 260/277 [01:26<00:05,  3.01it/s, loss=0.868, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.19it/s]\u001b[A\n",
            "Epoch 0: 100% 277/277 [01:28<00:00,  3.13it/s, loss=0.868, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 0: 100% 277/277 [01:32<00:00,  2.99it/s, loss=0.841, v_num=sjxu]\n",
            "Epoch 1:  65% 180/277 [01:05<00:35,  2.76it/s, loss=0.602, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 1:  79% 220/277 [01:20<00:20,  2.74it/s, loss=0.571, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  87% 240/277 [01:24<00:13,  2.82it/s, loss=0.571, v_num=sjxu]\n",
            "Epoch 1:  94% 260/277 [01:27<00:05,  2.98it/s, loss=0.571, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.29it/s]\u001b[A\n",
            "Epoch 1: 100% 277/277 [01:29<00:00,  3.10it/s, loss=0.571, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 1: 100% 277/277 [01:33<00:00,  2.96it/s, loss=0.581, v_num=sjxu]\n",
            "Epoch 2:  51% 140/277 [00:50<00:49,  2.75it/s, loss=0.546, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 2:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.52, v_num=sjxu] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.52, v_num=sjxu]\n",
            "Epoch 2:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.52, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.28it/s]\u001b[A\n",
            "Epoch 2: 100% 277/277 [01:29<00:00,  3.08it/s, loss=0.52, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 2: 100% 277/277 [01:34<00:00,  2.94it/s, loss=0.533, v_num=sjxu]\n",
            "Epoch 3:  65% 180/277 [01:05<00:35,  2.76it/s, loss=0.582, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 3:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.518, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/48 [00:00<?, ?it/s]\u001b[APLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 3:  87% 240/277 [01:26<00:13,  2.78it/s, loss=0.518, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 3:  94% 260/277 [01:29<00:05,  2.89it/s, loss=0.518, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:07<00:01,  5.55it/s]\u001b[A\n",
            "Epoch 3: 100% 277/277 [01:31<00:00,  3.01it/s, loss=0.518, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 3: 100% 277/277 [01:36<00:00,  2.88it/s, loss=0.568, v_num=sjxu]\n",
            "Epoch 4:  14% 40/277 [00:14<01:28,  2.69it/s, loss=0.521, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 4:  72% 200/277 [01:13<00:28,  2.72it/s, loss=0.55, v_num=sjxu] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 4:  79% 220/277 [01:21<00:21,  2.69it/s, loss=0.506, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  87% 240/277 [01:26<00:13,  2.78it/s, loss=0.506, v_num=sjxu]\n",
            "Epoch 4:  94% 260/277 [01:28<00:05,  2.93it/s, loss=0.506, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.26it/s]\u001b[A\n",
            "Epoch 4: 100% 277/277 [01:30<00:00,  3.05it/s, loss=0.506, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 4: 100% 277/277 [01:35<00:00,  2.91it/s, loss=0.496, v_num=sjxu]\n",
            "Epoch 5:  36% 100/277 [00:36<01:04,  2.73it/s, loss=0.494, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 5:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.498, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  87% 240/277 [01:25<00:13,  2.82it/s, loss=0.498, v_num=sjxu]\n",
            "Epoch 5:  94% 260/277 [01:27<00:05,  2.97it/s, loss=0.498, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.28it/s]\u001b[A\n",
            "Epoch 5: 100% 277/277 [01:29<00:00,  3.09it/s, loss=0.498, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 5: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.513, v_num=sjxu]\n",
            "Epoch 6:   0% 0/277 [00:00<?, ?it/s, loss=0.513, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 6:  79% 220/277 [01:20<00:20,  2.74it/s, loss=0.487, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  87% 240/277 [01:25<00:13,  2.82it/s, loss=0.487, v_num=sjxu]\n",
            "Epoch 6:  94% 260/277 [01:27<00:05,  2.97it/s, loss=0.487, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.32it/s]\u001b[A\n",
            "Epoch 6: 100% 277/277 [01:29<00:00,  3.10it/s, loss=0.487, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 6: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.489, v_num=sjxu]\n",
            "Epoch 7:  65% 180/277 [01:05<00:35,  2.75it/s, loss=0.45, v_num=sjxu] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 7:  79% 220/277 [01:21<00:21,  2.70it/s, loss=0.461, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  87% 240/277 [01:26<00:13,  2.79it/s, loss=0.461, v_num=sjxu]\n",
            "Epoch 7:  94% 260/277 [01:28<00:05,  2.94it/s, loss=0.461, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.31it/s]\u001b[A\n",
            "Epoch 7: 100% 277/277 [01:30<00:00,  3.06it/s, loss=0.461, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 7: 100% 277/277 [01:34<00:00,  2.92it/s, loss=0.438, v_num=sjxu]\n",
            "Epoch 8:   7% 20/277 [00:07<01:36,  2.67it/s, loss=0.48, v_num=sjxu] PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 8:  36% 100/277 [00:37<01:06,  2.66it/s, loss=0.463, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 8:  58% 160/277 [01:00<00:43,  2.66it/s, loss=0.485, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 8:  79% 220/277 [01:22<00:21,  2.67it/s, loss=0.472, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  87% 240/277 [01:27<00:13,  2.76it/s, loss=0.472, v_num=sjxu]\n",
            "Epoch 8:  94% 260/277 [01:29<00:05,  2.91it/s, loss=0.472, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.29it/s]\u001b[A\n",
            "Epoch 8: 100% 277/277 [01:31<00:00,  3.03it/s, loss=0.472, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 8: 100% 277/277 [01:35<00:00,  2.89it/s, loss=0.463, v_num=sjxu]\n",
            "Epoch 9:  43% 120/277 [00:43<00:57,  2.74it/s, loss=0.476, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 9:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.447, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  87% 240/277 [01:25<00:13,  2.82it/s, loss=0.447, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 9:  94% 260/277 [01:28<00:05,  2.93it/s, loss=0.447, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:05<00:01,  6.71it/s]\u001b[A\n",
            "Epoch 9: 100% 277/277 [01:30<00:00,  3.05it/s, loss=0.447, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 9: 100% 277/277 [01:35<00:00,  2.91it/s, loss=0.466, v_num=sjxu]\n",
            "Epoch 10:  72% 200/277 [01:12<00:27,  2.76it/s, loss=0.459, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 10:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.456, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  87% 240/277 [01:25<00:13,  2.82it/s, loss=0.456, v_num=sjxu]\n",
            "Epoch 10:  94% 260/277 [01:27<00:05,  2.97it/s, loss=0.456, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.29it/s]\u001b[A\n",
            "Epoch 10: 100% 277/277 [01:29<00:00,  3.10it/s, loss=0.456, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 10: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.466, v_num=sjxu]\n",
            "Epoch 11:  79% 220/277 [01:19<00:20,  2.77it/s, loss=0.471, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  87% 240/277 [01:24<00:12,  2.85it/s, loss=0.471, v_num=sjxu]\n",
            "Epoch 11:  94% 260/277 [01:26<00:05,  3.00it/s, loss=0.471, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.33it/s]\u001b[A\n",
            "Epoch 11: 100% 277/277 [01:28<00:00,  3.13it/s, loss=0.471, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 11: 100% 277/277 [01:32<00:00,  2.99it/s, loss=0.47, v_num=sjxu] \n",
            "Epoch 12:  14% 40/277 [00:14<01:28,  2.69it/s, loss=0.457, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 12:  72% 200/277 [01:13<00:28,  2.72it/s, loss=0.474, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 12:  79% 220/277 [01:21<00:21,  2.70it/s, loss=0.466, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  87% 240/277 [01:26<00:13,  2.79it/s, loss=0.466, v_num=sjxu]\n",
            "Epoch 12:  94% 260/277 [01:28<00:05,  2.94it/s, loss=0.466, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.34it/s]\u001b[A\n",
            "Epoch 12: 100% 277/277 [01:30<00:00,  3.06it/s, loss=0.466, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 12: 100% 277/277 [01:34<00:00,  2.92it/s, loss=0.456, v_num=sjxu]\n",
            "Epoch 13:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.486, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  87% 240/277 [01:24<00:12,  2.85it/s, loss=0.486, v_num=sjxu]\n",
            "Epoch 13:  94% 260/277 [01:26<00:05,  3.00it/s, loss=0.486, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.11it/s]\u001b[A\n",
            "Epoch 13: 100% 277/277 [01:28<00:00,  3.12it/s, loss=0.486, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 13: 100% 277/277 [01:32<00:00,  2.98it/s, loss=0.465, v_num=sjxu]\n",
            "Epoch 14:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.442, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  87% 240/277 [01:24<00:12,  2.85it/s, loss=0.442, v_num=sjxu]\n",
            "Epoch 14:  94% 260/277 [01:26<00:05,  3.00it/s, loss=0.442, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.35it/s]\u001b[A\n",
            "Epoch 14: 100% 277/277 [01:28<00:00,  3.13it/s, loss=0.442, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 14: 100% 277/277 [01:33<00:00,  2.98it/s, loss=0.449, v_num=sjxu]\n",
            "Epoch 15:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.458, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  87% 240/277 [01:24<00:12,  2.85it/s, loss=0.458, v_num=sjxu]\n",
            "Epoch 15:  94% 260/277 [01:26<00:05,  3.00it/s, loss=0.458, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.32it/s]\u001b[A\n",
            "Epoch 15: 100% 277/277 [01:28<00:00,  3.12it/s, loss=0.458, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 15: 100% 277/277 [01:32<00:00,  2.98it/s, loss=0.467, v_num=sjxu]\n",
            "Epoch 16:  79% 220/277 [01:20<00:20,  2.75it/s, loss=0.477, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  87% 240/277 [01:24<00:13,  2.83it/s, loss=0.477, v_num=sjxu]\n",
            "Validation DataLoader 0:  42% 20/48 [00:02<00:03,  8.28it/s]\u001b[APLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 16:  94% 260/277 [01:28<00:05,  2.94it/s, loss=0.477, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:06<00:01,  6.49it/s]\u001b[A\n",
            "Epoch 16: 100% 277/277 [01:30<00:00,  3.06it/s, loss=0.477, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 16: 100% 277/277 [01:34<00:00,  2.92it/s, loss=0.468, v_num=sjxu]\n",
            "Epoch 17:   0% 0/277 [00:00<?, ?it/s, loss=0.468, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 17:  58% 160/277 [00:59<00:43,  2.70it/s, loss=0.479, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 17:  79% 220/277 [01:21<00:21,  2.70it/s, loss=0.432, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  87% 240/277 [01:26<00:13,  2.79it/s, loss=0.432, v_num=sjxu]\n",
            "Epoch 17:  94% 260/277 [01:28<00:05,  2.93it/s, loss=0.432, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.06it/s]\u001b[A\n",
            "Epoch 17: 100% 277/277 [01:30<00:00,  3.06it/s, loss=0.432, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 17: 100% 277/277 [01:34<00:00,  2.92it/s, loss=0.441, v_num=sjxu]\n",
            "Epoch 18:  29% 80/277 [00:29<01:12,  2.71it/s, loss=0.434, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 18:  79% 220/277 [01:21<00:21,  2.69it/s, loss=0.456, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  87% 240/277 [01:26<00:13,  2.78it/s, loss=0.456, v_num=sjxu]\n",
            "Epoch 18:  94% 260/277 [01:28<00:05,  2.93it/s, loss=0.456, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.08it/s]\u001b[A\n",
            "Epoch 18: 100% 277/277 [01:30<00:00,  3.05it/s, loss=0.456, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 18: 100% 277/277 [01:34<00:00,  2.92it/s, loss=0.456, v_num=sjxu]\n",
            "Epoch 19:  36% 100/277 [00:36<01:05,  2.72it/s, loss=0.438, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 19:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.462, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.462, v_num=sjxu]\n",
            "Epoch 19:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.462, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.25it/s]\u001b[A\n",
            "Epoch 19: 100% 277/277 [01:29<00:00,  3.09it/s, loss=0.462, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 19: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.454, v_num=sjxu]\n",
            "Epoch 20:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.437, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  87% 240/277 [01:24<00:13,  2.85it/s, loss=0.437, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 20:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.437, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:05<00:01,  6.67it/s]\u001b[A\n",
            "Epoch 20: 100% 277/277 [01:29<00:00,  3.08it/s, loss=0.437, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 20: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.44, v_num=sjxu] \n",
            "Epoch 21:  43% 120/277 [00:43<00:57,  2.73it/s, loss=0.429, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 21:  58% 160/277 [00:59<00:43,  2.71it/s, loss=0.438, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 21:  79% 220/277 [01:21<00:21,  2.70it/s, loss=0.436, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  87% 240/277 [01:26<00:13,  2.79it/s, loss=0.436, v_num=sjxu]\n",
            "Epoch 21:  94% 260/277 [01:28<00:05,  2.94it/s, loss=0.436, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.31it/s]\u001b[A\n",
            "Epoch 21: 100% 277/277 [01:30<00:00,  3.06it/s, loss=0.436, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 21: 100% 277/277 [01:34<00:00,  2.93it/s, loss=0.441, v_num=sjxu]\n",
            "Epoch 22:   0% 0/277 [00:00<?, ?it/s, loss=0.441, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22:   7% 20/277 [00:08<01:50,  2.32it/s, loss=0.434, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22:  22% 60/277 [00:24<01:26,  2.50it/s, loss=0.434, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22:  79% 220/277 [01:22<00:21,  2.67it/s, loss=0.434, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  87% 240/277 [01:27<00:13,  2.76it/s, loss=0.434, v_num=sjxu]\n",
            "Epoch 22:  94% 260/277 [01:29<00:05,  2.91it/s, loss=0.434, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.29it/s]\u001b[A\n",
            "Epoch 22: 100% 277/277 [01:31<00:00,  3.03it/s, loss=0.434, v_num=sjxu]self.total_samples: 16\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 22: 100% 277/277 [01:36<00:00,  2.86it/s, loss=0.435, v_num=sjxu]\n",
            "Epoch 23:  14% 40/277 [00:14<01:28,  2.69it/s, loss=0.425, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  58% 160/277 [00:59<00:43,  2.69it/s, loss=0.447, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  72% 200/277 [01:14<00:28,  2.68it/s, loss=0.445, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  79% 220/277 [01:22<00:21,  2.66it/s, loss=0.431, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0% 0/48 [00:00<?, ?it/s]\u001b[APLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 23:  87% 240/277 [01:28<00:13,  2.70it/s, loss=0.431, v_num=sjxu]\n",
            "Epoch 23:  94% 260/277 [01:31<00:05,  2.85it/s, loss=0.431, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:06<00:01,  6.32it/s]\u001b[A\n",
            "Epoch 23: 100% 277/277 [01:33<00:00,  2.97it/s, loss=0.431, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 23: 100% 277/277 [01:37<00:00,  2.84it/s, loss=0.433, v_num=sjxu]\n",
            "Epoch 24:  79% 220/277 [01:19<00:20,  2.75it/s, loss=0.415, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  87% 240/277 [01:24<00:13,  2.84it/s, loss=0.415, v_num=sjxu]\n",
            "Epoch 24:  94% 260/277 [01:26<00:05,  2.99it/s, loss=0.415, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.24it/s]\u001b[A\n",
            "Epoch 24: 100% 277/277 [01:28<00:00,  3.11it/s, loss=0.415, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 24: 100% 277/277 [01:33<00:00,  2.96it/s, loss=0.42, v_num=sjxu] \n",
            "Epoch 25:   7% 20/277 [00:07<01:36,  2.67it/s, loss=0.409, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 25:  79% 220/277 [01:20<00:20,  2.74it/s, loss=0.434, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  87% 240/277 [01:24<00:13,  2.82it/s, loss=0.434, v_num=sjxu]\n",
            "Epoch 25:  94% 260/277 [01:27<00:05,  2.97it/s, loss=0.434, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Validation DataLoader 0:  83% 40/48 [00:05<00:01,  6.77it/s]\u001b[A\n",
            "Epoch 25: 100% 277/277 [01:30<00:00,  3.06it/s, loss=0.434, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 25: 100% 277/277 [01:34<00:00,  2.93it/s, loss=0.44, v_num=sjxu] \n",
            "Epoch 26:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.41, v_num=sjxu] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  87% 240/277 [01:24<00:13,  2.84it/s, loss=0.41, v_num=sjxu]\n",
            "Epoch 26:  94% 260/277 [01:26<00:05,  2.99it/s, loss=0.41, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.28it/s]\u001b[A\n",
            "Epoch 26: 100% 277/277 [01:28<00:00,  3.12it/s, loss=0.41, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 26: 100% 277/277 [01:33<00:00,  2.96it/s, loss=0.426, v_num=sjxu]\n",
            "Epoch 27:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.425, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  87% 240/277 [01:24<00:12,  2.85it/s, loss=0.425, v_num=sjxu]\n",
            "Epoch 27:  94% 260/277 [01:26<00:05,  3.00it/s, loss=0.425, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.30it/s]\u001b[A\n",
            "Epoch 27: 100% 277/277 [01:28<00:00,  3.12it/s, loss=0.425, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 27: 100% 277/277 [01:32<00:00,  2.99it/s, loss=0.435, v_num=sjxu]\n",
            "Epoch 28:  29% 80/277 [00:29<01:12,  2.70it/s, loss=0.412, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 28:  79% 220/277 [01:20<00:20,  2.72it/s, loss=0.41, v_num=sjxu] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.41, v_num=sjxu]\n",
            "Epoch 28:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.41, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.28it/s]\u001b[A\n",
            "Epoch 28: 100% 277/277 [01:29<00:00,  3.08it/s, loss=0.41, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 28: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.423, v_num=sjxu]\n",
            "Epoch 29:  79% 220/277 [01:20<00:20,  2.75it/s, loss=0.408, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  87% 240/277 [01:24<00:13,  2.84it/s, loss=0.408, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "\n",
            "Epoch 29:  94% 260/277 [01:28<00:05,  2.95it/s, loss=0.408, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:05<00:01,  6.81it/s]\u001b[A\n",
            "Epoch 29: 100% 277/277 [01:30<00:00,  3.07it/s, loss=0.408, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 29: 100% 277/277 [01:34<00:00,  2.94it/s, loss=0.413, v_num=sjxu]\n",
            "Epoch 30:   7% 20/277 [00:07<01:39,  2.58it/s, loss=0.405, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 30:  79% 220/277 [01:20<00:20,  2.72it/s, loss=0.423, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.423, v_num=sjxu]\n",
            "Epoch 30:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.423, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.31it/s]\u001b[A\n",
            "Epoch 30: 100% 277/277 [01:29<00:00,  3.08it/s, loss=0.423, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 30: 100% 277/277 [01:34<00:00,  2.94it/s, loss=0.424, v_num=sjxu]\n",
            "Epoch 31:  79% 220/277 [01:19<00:20,  2.75it/s, loss=0.428, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  87% 240/277 [01:24<00:13,  2.84it/s, loss=0.428, v_num=sjxu]\n",
            "Epoch 31:  94% 260/277 [01:26<00:05,  2.99it/s, loss=0.428, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.30it/s]\u001b[A\n",
            "Epoch 31: 100% 277/277 [01:28<00:00,  3.12it/s, loss=0.428, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 31: 100% 277/277 [01:32<00:00,  2.99it/s, loss=0.431, v_num=sjxu]\n",
            "Epoch 32:  65% 180/277 [01:05<00:35,  2.75it/s, loss=0.395, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 32:  79% 220/277 [01:20<00:20,  2.72it/s, loss=0.406, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.406, v_num=sjxu]\n",
            "Epoch 32:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.406, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.11it/s]\u001b[A\n",
            "Epoch 32: 100% 277/277 [01:29<00:00,  3.08it/s, loss=0.406, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 32: 100% 277/277 [01:34<00:00,  2.93it/s, loss=0.415, v_num=sjxu]\n",
            "Epoch 33:  79% 220/277 [01:19<00:20,  2.76it/s, loss=0.409, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  87% 240/277 [01:24<00:12,  2.85it/s, loss=0.409, v_num=sjxu]\n",
            "Epoch 33:  94% 260/277 [01:26<00:05,  3.00it/s, loss=0.409, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.33it/s]\u001b[A\n",
            "Epoch 33: 100% 277/277 [01:28<00:00,  3.13it/s, loss=0.409, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 33: 100% 277/277 [01:32<00:00,  2.98it/s, loss=0.409, v_num=sjxu]\n",
            "Epoch 34:  58% 160/277 [00:58<00:42,  2.75it/s, loss=0.404, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 34:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.392, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  87% 240/277 [01:25<00:13,  2.82it/s, loss=0.392, v_num=sjxu]\n",
            "Epoch 34:  94% 260/277 [01:27<00:05,  2.97it/s, loss=0.392, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.30it/s]\u001b[A\n",
            "Epoch 34: 100% 277/277 [01:29<00:00,  3.09it/s, loss=0.392, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 34: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.381, v_num=sjxu]\n",
            "Epoch 35:  14% 40/277 [00:15<01:29,  2.65it/s, loss=0.392, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 35:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.397, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  87% 240/277 [01:25<00:13,  2.82it/s, loss=0.397, v_num=sjxu]\n",
            "Epoch 35:  94% 260/277 [01:27<00:05,  2.97it/s, loss=0.397, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.25it/s]\u001b[A\n",
            "Epoch 35: 100% 277/277 [01:29<00:00,  3.09it/s, loss=0.397, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 35: 100% 277/277 [01:33<00:00,  2.96it/s, loss=0.408, v_num=sjxu]\n",
            "Epoch 36:   7% 20/277 [00:07<01:39,  2.57it/s, loss=0.381, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 36:  79% 220/277 [01:20<00:20,  2.73it/s, loss=0.391, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.391, v_num=sjxu]\n",
            "Epoch 36:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.391, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.00it/s]\u001b[A\n",
            "Epoch 36: 100% 277/277 [01:29<00:00,  3.09it/s, loss=0.391, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 36: 100% 277/277 [01:33<00:00,  2.95it/s, loss=0.399, v_num=sjxu]\n",
            "Epoch 37:  72% 200/277 [01:12<00:28,  2.75it/s, loss=0.371, v_num=sjxu]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Epoch 37:  79% 220/277 [01:20<00:20,  2.72it/s, loss=0.416, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  87% 240/277 [01:25<00:13,  2.81it/s, loss=0.416, v_num=sjxu]\n",
            "Epoch 37:  94% 260/277 [01:27<00:05,  2.96it/s, loss=0.416, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.31it/s]\u001b[A\n",
            "Epoch 37: 100% 277/277 [01:29<00:00,  3.08it/s, loss=0.416, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 37: 100% 277/277 [01:34<00:00,  2.95it/s, loss=0.409, v_num=sjxu]\n",
            "Epoch 38:  79% 220/277 [01:19<00:20,  2.75it/s, loss=0.391, v_num=sjxu]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/48 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  87% 240/277 [01:24<00:13,  2.84it/s, loss=0.391, v_num=sjxu]\n",
            "Epoch 38:  94% 260/277 [01:26<00:05,  2.99it/s, loss=0.391, v_num=sjxu]\n",
            "Validation DataLoader 0:  83% 40/48 [00:04<00:00,  8.26it/s]\u001b[A\n",
            "Epoch 38: 100% 277/277 [01:28<00:00,  3.11it/s, loss=0.391, v_num=sjxu]self.total_samples: 16\n",
            "Epoch 38: 100% 277/277 [01:33<00:00,  2.97it/s, loss=0.391, v_num=sjxu]\n",
            "Epoch 38: 100% 277/277 [01:33<00:00,  2.97it/s, loss=0.391, v_num=sjxu]\n",
            "torch.Size([128, 15, 9])\n",
            "torch.Size([15, 1152])\n",
            "[1.6708594  1.52380115 1.74496912 ... 1.80628531 0.95369818 2.49001359]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▃▄▆▇██████████▇▇▅▅▅▅▅▅▅▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁███████████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▅▄▃▂▂▂▃▂▁▄▂▂▂▂▃▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▅▄▃▂▂▂▃▂▁▄▂▂▂▂▃▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▆▅▃▂▃▂▃▂▁▂▃▃▂▃▃▂▂▂▂▂▂▂▂▂▃▃▂▃▂▂▃▂▂▁▂▁▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▃▂▁▂▂▃▁▂▂▁▃▂▃▂▃█▅▂▂▁▂▅▂▂▂▄█▁▁▃▁▂▄▁▁▂▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▅▄▃▂▂▂▃▂▁▄▂▂▂▂▃▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▆▅▃▂▃▂▃▂▁▂▃▃▂▃▃▂▂▂▂▂▂▂▂▂▃▃▂▃▂▂▃▂▂▁▂▁▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▅▄▃▂▂▂▃▂▁▄▂▂▂▂▃▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▆▃▄▄▃▃▃▃▃▃▃▂▁▃▄▃▂▂▃▃▃▃▃▂▂▃▃▃▃▂▂▃▁▃▃▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape █▅▂▃▂▂▂▂▂▁▂▂▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▃████▁█████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▄▃▄▃▅▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▄▃▄▃▅▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▅▃▅▅▆▃▃▂▃▂▂▂▂▂▃▃▂▂▁▂▂▁▁▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▆▃▂▅▅█▆▃▄▂▃▃▂▃▂▄▂▃▂▂▃▂▂▁▂▂▅▂▂▂▁▃▁▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▄▃▄▃▅▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▅▃▅▅▆▃▃▂▃▂▂▂▂▂▃▃▂▂▁▂▂▁▁▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▄▃▄▃▅▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▆▄▇▆█▅▄▃▄▃▃▃▃▃▅▄▃▂▂▃▃▂▁▂▃▄▃▁▃▁▂▂▂▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape █▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▂▂▁▁▁▂▁▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         global_step 8931\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.00017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.00011\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.42611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.42611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.48377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 3.2426\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.42611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.48377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.42611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.63017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.01863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 8930\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.47038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.47038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.50655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 5.29151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.47038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.50655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.47038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.72377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.05296\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33meager-thunder-266\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/ussqsjxu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 1040 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./plots_checkpoints_logs/generated, 9ch, enc_layers 5, dec_layers 6, heads 10, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6/wandb_logs/917174274/wandb/run-20220924_135702-ussqsjxu/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python train_eeg.py spacetimeformer eeg_social_memory --run_name spatiotemporal_eeg_social_memory --wandb --debug --base_lr 5e-4 --d_model 100 --d_ff 400 --enc_layers 4 --dec_layers 4 --batch_size 192 --start_token_len 0 --n_heads 4 --grad_clip_norm 1 --trials 1"
      ],
      "metadata": {
        "id": "2NJxZ09JbjGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_eeg.py spacetimeformer eeg_social_memory --d_model 200 --d_ff 700 --enc_layers 5 --dec_layers 6 --base_lr 1e-3 --l2_coeff 1e-3 --loss mae --d_qk 30 --d_v 30 --n_heads 10 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --class_loss_imp 0.0001 --local_self_attn none --local_cross_attn none --attn_plot"
      ],
      "metadata": {
        "id": "vmkDDgrau2ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1889b748-edaa-4bf1-aa84-658e80fec3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.run_name: generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae\n",
            "checkpoint_path: ./plots_checkpoints_logs/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae/checkpoints/382799060/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae_epoch=35-val/norm_mse=0.28.ckpt\n",
            "(24, 200, 29)\n",
            "yc.shape: (24, 200, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.07126684 -0.22690296 -0.07169651 -0.05365932 -0.07396834 -0.17957911\n",
            "  0.21191181  0.18385186 -0.11053615 -0.17894138 -0.16338775 -0.09776089\n",
            "  0.18514045 -0.0220533  -0.12806695  0.00927274  0.00092259 -0.01896504\n",
            " -0.03056345  0.09450471 -0.00216401 -0.11408559 -0.1642275   0.02089684\n",
            "  0.12531941  0.09108039 -0.0969246   0.27002579 -0.09008729]\n",
            "std_dev yc: [4.97892579 4.89188955 5.17618502 6.92346932 3.25139539 5.93315232\n",
            " 3.76279886 7.24034524 8.21194295 4.32116047 3.70894047 6.37004798\n",
            " 3.70481925 5.09176944 3.62738185 8.69721373 4.88683327 3.50844629\n",
            " 7.2718065  6.72302239 5.89281726 5.69385106 6.44041685 3.45530692\n",
            " 7.723593   4.23763204 5.81854761 3.73412429 3.84615901]\n",
            "\n",
            "(110, 200, 29)\n",
            "(200, 3190)\n",
            "[4.67765822 4.40718493 5.4226775  ... 5.50380407 3.50488549 4.03797252]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 4.22528247e-02 -4.42631694e-01  4.00643870e-01 ...  2.79478033e-01\n",
            "   -5.93316940e-01 -2.91580778e-01]\n",
            "  [-2.83703615e-01 -2.28151603e-01 -2.66956272e-01 ... -3.32164907e-01\n",
            "   -1.98188426e-01  6.89446705e-02]\n",
            "  [ 1.53366495e-01 -4.74248383e-01  3.68675334e-02 ... -4.36803907e-02\n",
            "   -6.04921704e-02  3.72299306e-01]\n",
            "  ...\n",
            "  [-2.66721080e-01  2.65056658e-01 -1.36330388e+00 ... -1.58361798e-01\n",
            "   -5.72639332e-01 -1.77001797e+00]\n",
            "  [-6.01510192e-01  6.40185473e-01 -1.63024730e+00 ...  4.75193244e-01\n",
            "   -1.22624364e+00 -1.02391398e+00]\n",
            "  [-1.32182185e+00 -5.03770189e-01  3.31174544e-01 ... -2.95278662e-01\n",
            "   -8.38696876e-01 -1.05303028e+00]]\n",
            "\n",
            " [[-2.81327126e-01  1.85371474e-01 -7.97886815e-02 ...  2.10195560e-03\n",
            "   -9.13773918e-01 -1.90603776e-01]\n",
            "  [-3.29718098e-01  5.22289711e-01  1.80993395e-01 ... -4.54784958e-01\n",
            "   -4.21054096e-01 -4.57964899e-01]\n",
            "  [-1.18253576e-01  3.87117117e-01  3.53714038e-01 ...  3.43931270e-01\n",
            "    1.09667697e-01 -4.49392341e-01]\n",
            "  ...\n",
            "  [-3.95979722e-02 -1.19514614e+00 -1.40662448e-01 ... -1.26392208e-02\n",
            "   -2.21362717e-02 -5.39035948e-01]\n",
            "  [ 1.45280335e-01 -1.19580414e+00  1.69109480e+00 ...  1.32564442e+00\n",
            "    3.67329731e-01 -5.79159304e-01]\n",
            "  [-8.16222647e-01  5.13135991e-01  1.06051368e+00 ... -3.87811000e-01\n",
            "    1.15191563e-01 -1.59543473e-02]]\n",
            "\n",
            " [[ 6.34788186e-01 -6.59118208e-02  6.81668163e-02 ... -1.47642879e-01\n",
            "   -2.05595642e-01  3.98981707e-01]\n",
            "  [-4.37970493e-01  2.41339156e-01 -1.82223033e-01 ...  5.97408339e-01\n",
            "    4.32034420e-01  7.77246963e-02]\n",
            "  [-4.03802744e-01  3.51243308e-02  6.83301292e-01 ... -8.93110732e-01\n",
            "   -4.08576752e-01 -5.96028394e-01]\n",
            "  ...\n",
            "  [ 1.14994861e+00  8.70002725e-01  1.11336850e+00 ...  1.78088541e+00\n",
            "   -5.76948022e-01  6.83225546e-01]\n",
            "  [ 2.15986445e+00  1.09982786e-01 -1.15098524e+00 ...  1.58825305e+00\n",
            "   -4.29138789e-01  1.38191232e-01]\n",
            "  [ 1.20364308e+00 -1.06751537e+00  5.24499696e-01 ...  1.51071601e+00\n",
            "   -3.57611916e-02 -6.62682961e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.75190467e-01 -2.43801532e-01  1.08621431e-01 ... -2.32996251e-01\n",
            "    4.19244557e-01 -1.01918398e+00]\n",
            "  [-1.52809970e-01 -1.89722925e-01 -3.52970911e-01 ... -2.99880079e-01\n",
            "   -5.07516113e-01  2.92904224e-01]\n",
            "  [-1.79701647e-01 -5.67896866e-01 -1.62628468e-01 ...  6.68072189e-02\n",
            "    2.34515147e-01 -2.31665542e-01]\n",
            "  ...\n",
            "  [-1.56807487e+00  8.65286101e-01 -1.75652800e+00 ...  3.42600372e-01\n",
            "    1.64458140e+00 -1.29776881e+00]\n",
            "  [-1.99371502e+00  2.94717655e-01 -1.66085085e+00 ...  1.64207466e+00\n",
            "    1.54722470e+00 -1.84186898e+00]\n",
            "  [-2.29118664e+00  6.35615724e-01 -1.08388451e+00 ...  4.96789158e-01\n",
            "    1.66157804e+00 -3.93644057e-01]]\n",
            "\n",
            " [[ 3.39499597e-01 -6.08779442e-02 -1.47527674e-01 ... -1.71885333e-01\n",
            "   -5.95471479e-01  2.28733243e-01]\n",
            "  [ 5.89125143e-02  7.56633885e-01 -1.05301305e-01 ...  1.17887684e-01\n",
            "   -3.64681068e-01 -4.52821739e-01]\n",
            "  [-3.14441888e-01  3.40633585e-01 -6.67966947e-01 ...  9.21617638e-01\n",
            "    1.69083855e-01 -5.66021746e-01]\n",
            "  ...\n",
            "  [-5.51754580e-01 -1.67730492e-01 -1.48026126e+00 ... -8.53976244e-01\n",
            "    8.49015392e-01 -5.91402487e-01]\n",
            "  [-6.79145238e-01 -1.88366337e+00  1.60413168e+00 ...  1.46198330e-01\n",
            "    9.09214842e-01 -5.63358528e-01]\n",
            "  [ 3.49674149e-01 -7.44510337e-01  1.60058754e+00 ... -8.40138287e-01\n",
            "   -7.24673752e-01 -1.39478786e+00]]\n",
            "\n",
            " [[-2.63111089e-01 -1.25598621e-01  2.80932842e-01 ... -4.14560139e-01\n",
            "   -7.82553533e-01 -5.17685502e-01]\n",
            "  [-3.91393139e-01  3.62743389e-01 -4.52161415e-02 ... -3.89015896e-01\n",
            "   -1.52989960e-01 -4.80891554e-01]\n",
            "  [-4.02571991e-01 -2.14946908e-01  7.26525037e-01 ...  5.50583156e-02\n",
            "   -1.59760221e-01  3.17664814e-01]\n",
            "  ...\n",
            "  [-5.38351728e-01  1.31873201e+00 -9.04689289e-01 ... -1.24284098e+00\n",
            "    1.17804819e-01 -1.16587934e+00]\n",
            "  [-4.58922760e-02 -1.87444766e+00 -2.93068102e-01 ...  6.53602849e-01\n",
            "    7.31066659e-01 -4.67101794e-01]\n",
            "  [-5.49507957e-01  8.97885400e-02  2.87872933e+00 ...  6.29619199e-02\n",
            "    3.66258953e-01 -4.62147144e-01]]]---------------------\n",
            "\n",
            "\n",
            "(23, 200, 29)\n",
            "(200, 667)\n",
            "[5.01891135 4.96296554 5.13968299 6.71460578 3.30555814 5.12656798\n",
            " 3.8170401  7.11172966 7.39290585 4.16822704 3.60181626 5.82675222\n",
            " 3.99472549 4.92581213 3.67190241 9.03075414 4.48332423 4.08573436\n",
            " 6.99771876 6.73474445 6.04811105 5.32435429 6.01614217 3.25880948\n",
            " 7.85309454 4.41090577 5.53702441 3.64917388 3.62501232 5.08716664\n",
            " 4.24269368 5.12045289 6.23098891 3.06642818 5.83539319 3.8324931\n",
            " 7.36944277 8.51926657 4.12600325 3.43020417 6.05247965 4.00105609\n",
            " 4.88617582 3.65025448 8.51284087 4.55857098 4.05057157 7.35822837\n",
            " 6.48726714 5.33069237 5.26955742 5.67817728 3.46921333 8.12686248\n",
            " 4.12460431 5.60249148 3.25864457 3.68528156 5.14711368 4.50260605\n",
            " 5.1443816  6.67224426 3.51011448 5.59071246 3.9632572  7.26936454\n",
            " 8.09193402 3.8322104  3.43864096 5.60924697 3.90354008 4.87047913\n",
            " 3.71466481 8.2487736  4.86031015 3.90072029 6.78444354 5.98449168\n",
            " 6.06566221 5.14376118 5.72134609 3.56578135 7.47821971 4.14049418\n",
            " 5.55217492 3.32969729 3.84905146 4.59088016 4.63623648 4.71454832\n",
            " 6.66961366 3.52293155 5.80258501 3.87129021 7.39522912 8.81658021\n",
            " 4.24661099 3.35517821 6.53646506 3.99232491 4.63224423 3.53412935\n",
            " 8.69061515 4.70487697 3.59280256 6.98498669 6.6392719  5.95930712\n",
            " 5.78808258 5.5024572  3.48932813 7.6754423  4.13418858 5.03787752\n",
            " 3.49929716 3.69463146 5.15944579 4.65374874 4.6455468  7.12722117\n",
            " 3.30726261 5.66568922 4.08558183 7.1505213  7.4880917  4.53496487\n",
            " 3.13899153 6.11458609 3.98303804 4.70435851 3.28185806 8.86789666\n",
            " 4.40134428 3.64952987 7.01994922 6.3660364  6.11777439 5.1669107\n",
            " 5.53990147 3.54495167 7.67447583 4.31555914 5.62835743 3.21510388\n",
            " 3.50792685 4.92266153 4.58214724 4.95317922 6.69855654 3.35324116\n",
            " 5.73044955 4.14713388 6.82938249 7.94530003 4.26994265 3.22034836\n",
            " 5.57099907 3.95568843 4.59722535 3.4300483  9.33654308 4.68764402\n",
            " 3.49407162 7.50249134 6.82793378 6.19749095 5.51633617 5.94421154\n",
            " 3.6114205  7.80394176 4.22747065 5.35012397 3.33783888 3.41535517\n",
            " 4.76392702 4.18943309 5.56468768 6.61221987 3.57809677 5.91329054\n",
            " 3.79851746 6.32438325 8.3290449  4.59139878 3.40293521 6.12853059\n",
            " 3.92769364 4.7065313  3.74060657 8.90994578 4.67496658 3.62819881\n",
            " 7.15153769 6.43665087 6.09528683 5.20225917 5.92885658 3.21212723\n",
            " 7.42251365 4.27470889 5.36390054 3.71663661 3.70078461 5.37461349\n",
            " 4.46011528 4.96724758 6.37677217 3.38868689 6.20069331 3.69765263\n",
            " 7.28669823 8.33642161 4.44259035 3.40745578 6.20130049 3.98240274\n",
            " 4.47355984 3.74594778 9.57718621 5.00342862 3.49765066 7.46385236\n",
            " 7.31056844 5.49284155 5.8205874  6.07547226 3.30597247 7.85474432\n",
            " 4.50312694 5.38501593 3.58313613 3.73005134 5.01989281 4.11413883\n",
            " 5.07564198 7.03151905 3.59098341 5.39847608 3.65721349 7.502382\n",
            " 8.52751608 4.16832579 3.25387428 6.14390374 4.09545409 4.95352097\n",
            " 3.44355276 9.4753466  5.13569481 3.39316507 7.42073239 6.75966032\n",
            " 5.72928866 5.6340525  6.42631071 3.349274   7.65166291 4.33542551\n",
            " 5.28424646 3.4610332  3.61594707 4.85888607 4.6157303  5.27199767\n",
            " 7.25504988 3.57972985 5.93421888 3.85754164 7.31176925 7.80630404\n",
            " 4.55536936 3.48838589 6.30690917 4.1893878  4.67600698 3.33456831\n",
            " 9.06860132 4.94376341 3.49836182 7.06151033 6.33395274 6.14124097\n",
            " 5.15219413 6.34330673 3.28789063 7.98618843 4.17775596 5.22557259\n",
            " 3.61576572 3.72504184 4.83036463 4.38143785 4.97035925 6.85082414\n",
            " 3.23051567 6.11407428 3.84078786 7.23614004 8.27770081 4.30126982\n",
            " 3.40249717 6.41887206 4.14273589 4.86103011 3.60207653 8.97165607\n",
            " 5.19531178 3.32896085 6.79842909 6.42911236 5.87948618 5.71206725\n",
            " 5.89007546 3.41742507 8.0216675  4.19348242 5.49707084 3.66457969\n",
            " 3.9744471  4.95938519 5.09737396 5.42534121 7.18989518 3.15950543\n",
            " 6.04363571 3.70551813 7.26215118 8.21193918 4.35113075 3.33729487\n",
            " 6.01870357 4.56016727 4.74484536 3.80582564 8.3030052  4.57738804\n",
            " 3.14666766 7.28382478 6.70535828 5.96020523 5.47007221 6.00334421\n",
            " 3.45799844 7.79961244 4.35040146 5.62677342 3.76060156 4.13829376\n",
            " 4.78108711 4.43298577 4.97391571 7.14354219 3.17157668 5.43457103\n",
            " 3.56382132 7.30258278 8.27860529 4.35652209 3.24297727 5.85065283\n",
            " 4.20750149 5.13601334 3.85469163 8.94566536 4.7387798  3.35787035\n",
            " 7.3322008  6.44924102 6.25273203 5.6821111  6.07757022 3.50656608\n",
            " 7.39188238 4.34531338 5.64990451 3.7104335  3.86683705 4.74929694\n",
            " 4.68107365 5.09093531 6.71065696 3.21442391 5.88454651 3.78780814\n",
            " 7.1776405  8.23221513 4.7395622  3.21344984 5.61668453 3.93567749\n",
            " 4.8018652  3.83139692 8.48454847 4.87704707 3.55112976 6.91590994\n",
            " 6.55419663 6.14350631 5.31227197 5.99583848 3.42993397 7.69506385\n",
            " 4.20078143 5.76884826 3.70992617 3.6709731  4.76788013 4.8136544\n",
            " 5.17602757 6.70089853 3.20219787 6.05166266 3.63199174 6.66299697\n",
            " 8.4487278  4.40216011 3.3291564  5.81837744 4.02916839 4.5903981\n",
            " 3.70570559 8.26371202 4.41638939 3.42661174 7.16929933 6.38874997\n",
            " 5.71074837 5.16284824 5.61707944 3.4273059  8.24258602 4.12149906\n",
            " 5.70163233 3.46907574 3.58104126 4.95504243 4.72028504 5.29720569\n",
            " 6.99171178 3.25955579 5.99124143 3.85853353 6.46244895 8.37306829\n",
            " 4.1619579  3.3427546  6.18204585 3.96714046 4.48431596 3.64851392\n",
            " 8.48732545 4.68665978 3.57390331 7.01312876 6.52162453 6.20596459\n",
            " 5.21429012 5.7883821  3.42019396 7.55313245 4.26087949 5.50411019\n",
            " 3.40475975 3.39233302 4.64629499 4.77539296 5.2397774  6.63342883\n",
            " 3.30168168 5.92607053 3.72727937 6.90900424 7.55397227 4.13509354\n",
            " 3.16873547 6.33030906 3.97445055 5.00190272 3.66349984 9.12968048\n",
            " 4.4923839  3.65880028 6.79426294 6.75387654 5.98520017 5.63015095\n",
            " 6.05301022 3.39018862 7.61132693 4.14528178 5.37795248 3.38175902\n",
            " 3.59069438 4.71864226 4.63345529 4.61956858 6.68242738 3.13984227\n",
            " 6.20772519 3.63695639 7.22347    8.17130383 4.33746129 3.24473662\n",
            " 6.49090072 4.1969128  4.99247996 3.64111    8.86855217 4.68903889\n",
            " 3.79080659 6.5038711  7.14783517 5.57652297 5.37249562 5.84080214\n",
            " 3.41835164 8.09409156 4.12868231 5.49479386 3.57377468 3.63390505\n",
            " 5.07621312 4.56494833 5.02315121 6.5101063  3.13606173 5.67684606\n",
            " 3.62497662 6.94414985 7.98772617 4.20199215 3.36061225 5.90034599\n",
            " 3.85540949 5.36400533 3.61026135 8.46139777 4.50688275 3.77329578\n",
            " 7.65392352 6.92226425 6.16865891 5.46596984 5.89785644 3.33777946\n",
            " 8.35007908 4.22725933 5.18697508 3.51605052 3.39356022 4.99901138\n",
            " 4.73645728 5.19366401 6.60083344 3.41247244 5.89171406 3.49060297\n",
            " 6.66778879 8.42839452 3.98389292 3.47085018 6.40927129 3.99933572\n",
            " 5.03385613 3.55817488 8.83315416 4.75517542 3.78809551 7.1734865\n",
            " 6.68649669 5.98176429 5.65263418 6.03698393 3.2861804  7.78349803\n",
            " 4.42839667 4.84059152 3.51782666 3.40089329 4.62647342 4.59709874\n",
            " 4.97513258 6.6533854  3.32379818 5.47973201 3.63861466 7.34040389\n",
            " 7.99164806 4.24782035 3.51874359 6.40218489 4.23794997 5.49415887\n",
            " 3.47478115 8.99180963 4.66739092 3.8476462  7.14326974 6.79635179\n",
            " 5.74258281 5.23742874 6.00189136 3.52003597 7.48702444 4.00409085\n",
            " 5.23444094 3.54260618 3.36216773 4.77238099 4.45455161 5.13468601\n",
            " 7.02146083 3.52188909 6.06309181 3.7290862  6.87504454 8.04814016\n",
            " 4.23499847 3.22373608 6.08462941 4.30970817 5.25585512 3.04357402\n",
            " 9.20301344 4.57413971 3.85747218 7.51891805 6.58876529 5.9445067\n",
            " 5.60674963 6.15363768 3.49474358 7.52623499 4.09095841 5.25164422\n",
            " 3.74285256 3.48035284 4.77851675 4.42074253 5.51809006 6.92926208\n",
            " 3.53308675 5.3199193  3.82591356 6.82853152 8.21416532 4.30126409\n",
            " 3.3582728  5.85007282 4.18025909 4.72069377 3.38865617 8.74574965\n",
            " 4.72235904 3.94975096 6.73483818 6.41278369 6.53665445 5.1769986\n",
            " 6.00903328 3.16086308 7.71766904 4.41530691 5.5936525  3.46386125\n",
            " 3.43014781]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.3464624   0.08491965 -0.36466161 ... -0.15752108 -0.37300071\n",
            "    0.14509891]\n",
            "  [ 0.1801796   0.25066403 -0.059273   ...  0.17355741  0.14484524\n",
            "    0.07590768]\n",
            "  [ 0.1272066  -0.05609343 -0.60208946 ... -0.59290435 -0.24484573\n",
            "   -0.5015327 ]\n",
            "  ...\n",
            "  [-0.10680622 -2.96621197  0.30108722 ...  1.14093247  1.44731186\n",
            "   -0.40859373]\n",
            "  [ 1.0042425  -0.87184475  1.85668969 ... -0.38325201  0.81386246\n",
            "    0.78042633]\n",
            "  [-0.62686599 -0.41927001  0.64618307 ... -1.31094165  0.07466104\n",
            "    1.05913314]]\n",
            "\n",
            " [[ 0.01881824  0.27262412 -0.25891577 ... -0.25388299  1.1053239\n",
            "   -0.43040621]\n",
            "  [-0.1133739  -0.06565374 -0.22832188 ... -0.66174156  0.25734235\n",
            "   -0.46206946]\n",
            "  [-0.11639857 -0.09721694 -0.16361838 ...  0.48389745  0.18696832\n",
            "    0.0543532 ]\n",
            "  ...\n",
            "  [-0.57771973 -1.65978937 -1.14896907 ...  0.10738424 -0.35638045\n",
            "    0.90469194]\n",
            "  [ 0.13874355 -0.77880498  1.45772221 ...  0.18708361  0.29471124\n",
            "    0.88225485]\n",
            "  [-0.55480457  1.08334279  2.74398111 ... -1.5756705   0.50177373\n",
            "    0.43830556]]\n",
            "\n",
            " [[ 0.38643618 -0.0581258  -0.64037132 ... -0.02167845 -0.02681984\n",
            "   -0.47997041]\n",
            "  [-0.03814569  0.04480802 -0.70908781 ... -0.47503286  0.02723372\n",
            "    0.32308086]\n",
            "  [-0.10680844 -0.2764007  -0.49196185 ... -0.13268036  0.33450677\n",
            "    0.31036298]\n",
            "  ...\n",
            "  [ 0.20997902 -0.3533651  -0.81037539 ... -0.53897877  0.51780791\n",
            "    0.54329913]\n",
            "  [-0.52081008  0.12381541  1.69616546 ... -0.50732956  0.38096644\n",
            "    0.76067803]\n",
            "  [-0.0677006   2.14839291 -0.28926333 ... -2.26227163  1.03189786\n",
            "    0.90912654]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.02453044  0.19405594 -0.1558015  ... -0.30078065  0.13966854\n",
            "    0.41499995]\n",
            "  [ 0.05893942  0.21865846  0.39594658 ... -0.12819493  0.00713263\n",
            "   -0.69649313]\n",
            "  [ 0.38572277 -0.44605257  0.31076937 ...  0.39092231 -1.00788882\n",
            "   -0.54072276]\n",
            "  ...\n",
            "  [ 0.72628014  1.659761   -0.23442781 ... -0.71154338 -1.1817209\n",
            "   -0.23330855]\n",
            "  [-0.51817348 -0.00807582 -0.48069595 ... -0.95805837 -1.08657312\n",
            "    0.1819822 ]\n",
            "  [ 1.06762938  1.94756111 -0.29619678 ... -0.35354056 -2.22013417\n",
            "    0.18736158]]\n",
            "\n",
            " [[-0.0228136   0.00356558  0.41486436 ...  0.46238662  0.23199766\n",
            "    0.48120901]\n",
            "  [-0.56893576  0.04244524 -0.48585788 ... -0.14696176  0.17745188\n",
            "    0.3417655 ]\n",
            "  [ 0.14846096 -0.0475633   0.60933664 ... -0.23615555 -0.51158907\n",
            "    0.07362565]\n",
            "  ...\n",
            "  [ 0.90502161 -2.26662874  1.43812707 ...  0.04319022  0.9954907\n",
            "    1.52540221]\n",
            "  [-0.33002084 -0.48977557  0.21623774 ... -3.06043531  1.16684775\n",
            "    1.03796329]\n",
            "  [ 1.01984273 -0.25231348 -2.18865079 ...  0.1522443   0.48530119\n",
            "   -0.3410399 ]]\n",
            "\n",
            " [[ 0.18805139  0.21543021 -0.05793271 ...  0.29367565 -0.09677479\n",
            "    0.26757926]\n",
            "  [ 0.16499055  0.09613273  0.1234057  ... -0.18614147 -0.43933994\n",
            "    0.58666519]\n",
            "  [ 0.19241071 -0.106849    0.05907962 ... -0.99337377  0.20342222\n",
            "   -0.18577803]\n",
            "  ...\n",
            "  [ 0.55811553 -2.58556944  0.27543808 ...  0.22567678  0.34253583\n",
            "   -1.82149666]\n",
            "  [-0.01911357 -0.35192007 -0.43985462 ...  0.53457547  0.32598302\n",
            "   -1.56344318]\n",
            "  [-2.94215435  1.14703506  0.56752989 ... -0.65771442  0.35215927\n",
            "   -1.41592417]]]---------------------\n",
            "\n",
            "\n",
            "(24, 200, 29)\n",
            "(200, 696)\n",
            "[4.88572122 5.12443178 5.00852834 6.87076969 3.09716264 6.0501031\n",
            " 3.61982636 7.36865108 7.98737666 4.45853354 3.44801023 6.06763326\n",
            " 3.73497131 4.99862379 3.29662534 8.80204539 4.94972331 3.40977456\n",
            " 6.44207121 6.62985075 6.00042176 5.72623677 6.12280732 3.40888179\n",
            " 7.59807955 3.96618658 5.88432166 3.57157737 3.77376855 4.70389176\n",
            " 4.7515084  4.8492006  6.1230299  3.05472038 5.77512118 3.5418696\n",
            " 7.05124354 8.00197261 4.46434062 3.44248678 5.40514473 3.43387647\n",
            " 5.39833099 3.42338605 8.57934957 4.69199917 3.23251822 7.17272273\n",
            " 6.96273025 5.53728109 5.67404317 5.82328934 3.58335777 8.22531721\n",
            " 4.04961318 5.52753659 3.55936403 3.42289252 4.90048738 5.12158545\n",
            " 4.88804447 6.08697506 3.24485407 5.71645328 3.40976814 7.46370599\n",
            " 9.01111354 4.17862829 3.64350172 6.02472467 3.98127806 5.05739047\n",
            " 3.64334516 8.97783218 4.81218583 3.54883489 7.32869667 6.5650816\n",
            " 5.7440653  5.64102463 6.36418234 3.5153313  8.03490333 4.21645261\n",
            " 5.96924972 3.87470838 4.11046594 4.96712914 4.97810204 4.58585602\n",
            " 6.99374996 3.31244385 5.87370364 3.53684586 7.50971696 8.26871683\n",
            " 3.81886002 3.78713541 6.56584836 4.11920447 5.24697021 3.54143235\n",
            " 8.33232322 5.43895816 3.91407154 6.90405038 6.70950935 5.60409444\n",
            " 5.56695646 6.28276796 3.31526816 7.69230024 4.34113577 6.22675997\n",
            " 3.8641521  4.37969851 5.0323832  4.50155082 5.24956101 6.2098273\n",
            " 3.2875965  5.6910852  3.61788876 7.5733034  8.49952776 3.86447112\n",
            " 3.63678202 6.03202304 3.86700371 5.34580423 3.43391242 7.92302061\n",
            " 5.039939   3.48313798 6.8823038  6.65079132 6.10523298 5.8120403\n",
            " 6.3686478  3.11669044 8.23596155 4.12842073 6.10837192 3.87994973\n",
            " 3.80589434 4.84317063 4.76664553 5.28557147 7.55312301 3.23116839\n",
            " 5.75245456 3.68681241 7.66548387 8.11554963 4.39790397 3.57808537\n",
            " 6.54249993 3.67742582 4.75716619 3.52553972 8.90711195 4.97518188\n",
            " 3.71877961 7.79731057 6.40846054 5.9900894  5.7138295  6.1815379\n",
            " 3.23200494 7.48074332 4.37231803 5.73492059 3.83079335 3.79400768\n",
            " 5.04667495 4.92715907 5.32691234 7.27494804 3.13312935 6.31842719\n",
            " 3.78471236 7.8392222  8.18464768 4.55961044 3.8490805  6.56830494\n",
            " 3.66654743 5.06057659 3.37854703 8.77461001 5.12639623 3.41642293\n",
            " 6.96322917 6.75251103 6.03694727 5.7526085  6.30363732 3.1563753\n",
            " 7.49840752 4.25209825 5.92654385 3.80934907 3.6021812  5.30982588\n",
            " 4.72260084 5.35345494 6.61868678 3.20261538 5.96371954 3.796667\n",
            " 7.03753393 8.52239117 4.30063808 3.76810613 6.01557381 3.74013203\n",
            " 5.27550663 3.22989531 8.65280188 5.00701044 3.54815131 7.85117567\n",
            " 6.28450586 5.6960899  5.9982277  6.4389676  3.29409507 8.04214177\n",
            " 4.2674432  5.80453862 3.38346258 3.71875673 5.03482978 4.88238327\n",
            " 4.84543914 7.28810473 3.21289658 6.0066969  3.67708671 6.72180429\n",
            " 7.23509767 4.18320109 3.85621392 6.43623929 3.57975385 5.09082586\n",
            " 3.45342244 8.85968848 5.07995786 3.47796665 6.69850977 6.34953748\n",
            " 6.01480409 5.35723965 6.70835163 3.25882362 7.07466118 4.37051622\n",
            " 5.82811808 3.46283121 3.89850563 4.63510524 4.98976333 5.22117691\n",
            " 6.88547874 3.0149325  6.07383866 3.85891751 6.86076668 8.46265671\n",
            " 4.242613   3.86268859 6.51266687 3.49062368 5.23427229 3.70841105\n",
            " 8.80209432 4.98899909 3.38437804 6.79397061 6.87998074 5.97683306\n",
            " 5.53253036 6.1648104  3.49380356 7.55963028 4.34635924 5.8432171\n",
            " 3.45775075 3.65656167 4.79602242 4.77204985 5.5016379  7.03743745\n",
            " 3.46797188 5.8194639  4.15793887 7.41210326 8.23824662 4.27949143\n",
            " 3.91668069 6.62384972 3.49243291 5.04271122 4.07016462 8.87597585\n",
            " 4.27263745 3.65196511 7.04526361 6.86715756 5.43130297 5.5812678\n",
            " 6.84403944 3.872093   7.88042876 4.29382036 5.963066   3.66485184\n",
            " 3.9960631  4.88801045 4.87545698 5.32197845 7.50535229 3.63162545\n",
            " 6.1833824  4.34112964 7.20916031 8.75930627 4.29697637 3.73325145\n",
            " 6.55025634 3.91837316 5.02411909 3.89515023 9.16551618 4.75061255\n",
            " 3.50524436 7.45249758 6.66788988 5.86652455 5.51164118 7.48424177\n",
            " 3.96420983 7.52555978 4.15428609 5.7643465  4.02575076 4.13235383\n",
            " 5.30916147 4.76466232 5.44735398 7.14593931 3.1464969  5.44450936\n",
            " 3.87205421 7.08396037 8.5250142  4.50429633 3.57028406 6.12051811\n",
            " 3.88938834 4.66691284 3.60209635 8.66477938 5.10462871 3.53976964\n",
            " 7.0403949  7.41106055 6.28945687 5.7012062  6.60869017 3.61929718\n",
            " 7.76131144 4.09112356 5.60039429 3.78737353 3.92202722 4.96819181\n",
            " 4.84803559 4.82688377 6.64792037 3.19475671 6.1999224  3.6848877\n",
            " 7.29121269 8.44017331 4.65915009 3.62828759 6.86202132 3.73429151\n",
            " 4.55469678 3.85399167 8.86378065 5.1989824  3.31580067 6.85617089\n",
            " 7.02886618 6.21199159 5.04427195 6.52180845 3.41299415 7.7051855\n",
            " 4.24106757 5.62578651 3.78438894 3.94541139 5.03818586 5.08489512\n",
            " 5.11764363 6.39485642 3.30472143 6.02093122 3.61262527 7.3443644\n",
            " 8.51522342 4.53018352 3.43073313 6.715045   3.68691165 4.9848147\n",
            " 3.83657446 8.8096082  4.84864589 3.46355069 7.90268855 7.10545291\n",
            " 6.17841273 5.70280315 6.32664607 3.62842208 7.55707876 4.14308661\n",
            " 5.75704419 3.91734063 3.75905229 5.07765576 4.93866347 5.24201288\n",
            " 7.03836084 3.14492461 6.08427168 3.76963793 6.55840534 8.2120426\n",
            " 4.21112809 3.76876555 6.53127261 3.65043138 5.10228685 3.84161302\n",
            " 9.26119706 5.10042335 3.54221698 7.09669523 7.04472484 5.92878684\n",
            " 5.57880689 6.9088066  3.49587934 7.45576476 4.57321969 5.82927179\n",
            " 3.46401518 3.71698151 4.84179445 4.8252291  5.19418364 6.61696853\n",
            " 3.3996015  6.10785247 3.60007998 7.17636623 8.01617769 4.26359184\n",
            " 3.83250664 6.10848973 3.409653   5.1145667  3.73886841 7.87796541\n",
            " 4.77265321 3.60366612 7.31785008 6.61953342 5.19527642 5.8826314\n",
            " 6.94852    3.26720043 7.71740579 4.29899278 5.65480883 3.49802182\n",
            " 3.84904961 5.272895   4.70712403 5.35115667 7.16291289 3.48606414\n",
            " 6.14309154 3.4984146  7.12066877 7.6607154  4.14061851 3.57154753\n",
            " 6.4464333  3.34914803 5.12441309 3.75523608 8.30817396 4.15687583\n",
            " 3.5452718  7.72283955 6.66846076 5.52866222 5.74920571 6.39998355\n",
            " 3.23598519 7.57994451 3.98260605 5.44647167 3.44430848 3.96853923\n",
            " 5.28148625 5.13483805 5.21262013 6.93002133 3.37248327 5.94716596\n",
            " 3.72465228 7.40123462 8.32282748 4.32259485 3.77584459 6.85222774\n",
            " 3.83631993 5.37117769 3.70712684 8.16125557 4.62707607 3.71258442\n",
            " 6.72773392 6.32222774 6.04866729 5.78049352 5.85455863 3.52736793\n",
            " 7.38462669 4.1877028  5.05057355 3.67335605 4.09775781 5.15155948\n",
            " 4.94369055 5.29317911 6.95089736 3.24817425 6.10498428 3.85076469\n",
            " 7.43984079 7.5626797  4.34234716 3.81360343 6.68070822 3.89438168\n",
            " 5.20653548 3.59110501 8.17626159 4.8495053  3.71294414 7.07556695\n",
            " 6.6336046  6.03546632 5.93529031 5.84047821 3.5695167  7.91975477\n",
            " 3.94424402 5.57040265 4.07715955 4.14251147 4.40367831 4.65550876\n",
            " 4.55155145 7.12166559 3.08824047 5.31885799 4.03393844 6.4647521\n",
            " 8.21896327 4.24184021 3.77340078 5.85288678 3.43997234 5.14026738\n",
            " 3.66968059 9.10541305 4.63370917 3.38888926 8.13404324 6.76026592\n",
            " 6.24812657 5.75294894 6.48541162 3.42405442 7.60465283 4.5722105\n",
            " 5.66786255 3.84922685 3.62514943 4.97649335 4.61092168 5.05071351\n",
            " 6.76609857 3.19588185 5.81863636 3.89165679 7.35972245 8.11230104\n",
            " 4.57169131 3.774284   6.00288966 3.6730158  5.36498379 3.59452779\n",
            " 8.48018521 4.74894133 3.41419998 7.44636071 6.69601731 6.23342428\n",
            " 5.74429484 6.67371737 3.49982368 7.0631688  4.24108387 5.96643811\n",
            " 3.66337446 3.63035529 4.85043384 4.89872615 5.55970571 7.29870114\n",
            " 3.262783   5.83987101 3.77998003 6.95511234 8.16166208 4.34169888\n",
            " 3.80248583 6.17652996 3.7436179  5.02337412 3.56600304 8.58805752\n",
            " 5.04754599 3.19361827 7.65744026 6.43445996 5.47869861 5.95558618\n",
            " 6.36031889 3.51247809 7.7425133  4.38932056 6.62197406 4.00300548\n",
            " 3.60285097 4.98402122 5.28963711 5.62642572 7.11206037 3.13021304\n",
            " 5.71273829 3.69773009 7.29514783 7.65520097 4.27110586 3.6239492\n",
            " 6.70476573 3.68354354 4.62848305 3.48890255 9.16255547 4.70849647\n",
            " 3.2437514  7.65039616 6.41936473 5.7534361  5.65939366 6.10781288\n",
            " 3.25689392 8.55832807 4.06390459 5.72636714 3.86674162 3.49668183]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 3.52440143e-01  3.36425715e-01 -3.18446007e-01 ... -2.16344203e-01\n",
            "    5.24963916e-01 -3.88466452e-01]\n",
            "  [ 2.64000379e-01  2.93591939e-01  6.56989418e-01 ... -1.99919362e-01\n",
            "    1.83012957e-02  3.65407661e-01]\n",
            "  [-3.00633017e-01  1.96503856e-01 -1.58946104e-01 ...  2.41445515e-02\n",
            "   -5.88538418e-01  1.17295266e-01]\n",
            "  ...\n",
            "  [ 2.83763018e-01  6.03423624e-01 -9.89634678e-02 ... -2.22504190e+00\n",
            "    4.63545211e-01 -6.03982482e-01]\n",
            "  [-3.18100915e-01 -5.95866745e-02 -2.98241146e+00 ...  2.27349079e-01\n",
            "   -2.53604650e-01 -9.31796132e-01]\n",
            "  [-1.35892281e+00 -1.26780357e+00  1.63093402e+00 ...  1.27647460e+00\n",
            "   -1.00968118e+00 -4.47174500e-01]]\n",
            "\n",
            " [[ 4.02520528e-01 -3.11679598e-03 -1.30504340e-01 ...  1.59318473e-01\n",
            "    6.23522720e-01 -7.99039954e-01]\n",
            "  [-1.48895299e-01  2.18677930e-01  4.33059485e-01 ... -1.00383387e-01\n",
            "   -3.51984429e-01  2.08950476e-01]\n",
            "  [-8.87917519e-02  5.03236137e-01 -6.22894250e-01 ...  1.24799312e-02\n",
            "   -1.01732344e+00  2.25048550e-02]\n",
            "  ...\n",
            "  [-5.73250207e-01  1.00360690e+00 -3.15148758e+00 ... -3.93069117e-01\n",
            "    4.14873138e-01  9.30355523e-01]\n",
            "  [-1.98623147e+00 -4.45953176e-01 -1.56492240e+00 ...  1.39096985e+00\n",
            "    5.10799976e-01  1.98083172e+00]\n",
            "  [-6.68733962e-01  1.07665718e+00  5.45597965e-01 ...  1.29086430e+00\n",
            "    1.05594400e+00  6.97742268e-01]]\n",
            "\n",
            " [[ 5.92384233e-01 -2.59442839e-01  9.43378018e-01 ... -2.56457567e-01\n",
            "    7.63708362e-01 -5.73309627e-01]\n",
            "  [-2.55391832e-01  3.66120840e-02  2.53262750e-01 ...  1.03564468e-01\n",
            "    1.96019270e-01  3.96959160e-01]\n",
            "  [-8.23919683e-01  1.74901720e-01 -1.25680909e-01 ... -7.43563714e-02\n",
            "   -3.65338496e-01  2.06799780e-02]\n",
            "  ...\n",
            "  [ 3.68357676e+00 -4.66872086e-01 -8.35626571e-01 ... -1.49964795e+00\n",
            "    7.26948061e-01  3.12760916e-01]\n",
            "  [ 3.44247695e+00 -3.76002227e-01 -1.58245339e+00 ... -1.50137734e+00\n",
            "    3.79516342e-01  7.18140103e-01]\n",
            "  [ 7.33424259e-01 -1.94933807e+00  1.31778803e+00 ... -1.46441203e+00\n",
            "   -3.77293401e-01  1.13549490e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 4.24054868e-01  4.47519135e-01 -4.31623058e-01 ...  1.49173059e-01\n",
            "    2.18651161e-01 -8.30967366e-02]\n",
            "  [ 2.00153623e-01 -3.80736074e-01 -2.63009084e-01 ... -6.02410291e-01\n",
            "    5.32972701e-01 -2.87661753e-02]\n",
            "  [-4.52270691e-02 -1.89346816e-01  1.60942802e-01 ... -2.89192198e-01\n",
            "   -4.11669740e-01 -3.11584146e-01]\n",
            "  ...\n",
            "  [-1.79933950e+00 -1.62234781e-01 -4.51765497e-01 ... -1.44259079e+00\n",
            "   -6.64398694e-01  9.62094281e-01]\n",
            "  [ 2.87718532e-02 -5.78166183e-01  2.29844963e+00 ... -4.20071483e-01\n",
            "   -2.52793466e-01  3.37757912e-01]\n",
            "  [-1.19535540e-01  1.19696312e+00 -1.56104905e+00 ... -1.04909101e+00\n",
            "   -3.86737417e-01  1.10759292e-01]]\n",
            "\n",
            " [[ 9.72176475e-02  1.09460431e-01  1.25602169e-01 ...  7.71413275e-01\n",
            "   -1.37596297e-01  7.31887487e-01]\n",
            "  [ 7.96586126e-02 -2.20433650e-01 -1.31633356e-01 ...  1.19402101e-01\n",
            "   -6.26289576e-01  3.58529043e-01]\n",
            "  [ 3.96568554e-01  5.25129541e-01  1.15071855e-01 ... -2.80138752e-01\n",
            "   -2.17721807e-01  2.47913949e-01]\n",
            "  ...\n",
            "  [-1.23222743e+00 -2.74343777e-01  2.12474591e+00 ... -2.49689745e+00\n",
            "    6.00730701e-01  1.93813830e+00]\n",
            "  [ 4.22679213e-01 -3.99274281e-01 -4.94732122e-02 ... -9.80389788e-01\n",
            "    8.19426969e-01  2.03518838e+00]\n",
            "  [ 2.60341308e-02  1.04439793e+00 -5.30142006e-01 ... -2.06397702e-01\n",
            "    5.06046910e-01  1.46297877e+00]]\n",
            "\n",
            " [[ 6.84905835e-02  5.83337328e-01  1.38420813e-01 ...  9.42048513e-02\n",
            "   -4.83679690e-01  3.16089154e-01]\n",
            "  [ 1.03149828e+00  9.57431613e-02 -1.41328034e-01 ... -2.52594960e-01\n",
            "   -5.81935412e-01 -4.40889611e-01]\n",
            "  [-4.02492241e-01  6.99513642e-01 -2.49271376e-01 ...  1.48776661e-01\n",
            "   -6.41190670e-01 -1.52300835e-01]\n",
            "  ...\n",
            "  [ 7.76036806e-01  9.66247877e-01  1.20375072e-01 ...  8.36512578e-01\n",
            "    1.84519281e+00  4.31304175e-02]\n",
            "  [ 2.06389565e-01  1.28224373e-01  5.32068105e-01 ... -5.05816072e-01\n",
            "    1.93584157e+00 -4.01393511e-01]\n",
            "  [ 6.30802054e-01 -9.22252156e-01  3.92804886e-02 ...  1.21144146e+00\n",
            "    8.80771584e-01  2.19977734e-01]]]---------------------\n",
            "\n",
            "\n",
            "(110, 200, 29)\n",
            "(23, 200, 29)\n",
            "(24, 200, 29)\n",
            "yc.shape: (24, 200, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-6.30745456e-17  9.71445147e-18  4.70832863e-18 -1.45254179e-17\n",
            "  5.13478149e-18  1.14260453e-17 -1.46410661e-17  1.12410081e-17\n",
            " -3.42434414e-17 -3.00685403e-19  8.70831185e-18  2.97447252e-17\n",
            "  2.51650552e-17  2.05969501e-17  9.72601629e-17 -1.64914378e-17\n",
            "  8.32667268e-19 -1.66996047e-17  2.45174251e-18  2.22565022e-17\n",
            "  1.68383825e-17  8.08149843e-17  2.32221649e-17  5.45859654e-17\n",
            " -2.94209102e-17 -8.98933705e-17 -2.96984659e-17  3.83952129e-18\n",
            " -1.18539438e-17]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 8\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -8\n",
            "self._slice_start_points: [82, 58, 134, 176, 2, 22, 146, 150, 184, 23, 50, 189, 55, 35, 164, 65, 142, 170, 34, 147, 190, 32, 132, 151, 162, 37, 74, 168, 110, 80, 27, 156, 96, 149, 1, 105, 123, 124, 185, 163, 126, 19, 169, 70, 38, 7, 12, 102, 69, 112, 68, 54, 93, 131, 171, 13, 116, 140, 26, 160, 60, 159, 73, 166, 177, 101, 129, 6, 59, 143, 148, 63, 88, 103, 127, 62, 67, 107, 98, 33, 8, 191, 21, 178, 84, 175, 92, 157, 4, 106, 72, 118, 90, 48, 57, 115, 28, 125, 138, 42, 167, 24, 76, 0, 161, 83, 181, 95, 186, 133, 136, 16, 137, 109, 81, 114, 75, 121, 86, 36, 78, 45, 61, 182, 97, 144, 172, 71, 11, 85, 20, 117, 141, 39, 153, 99, 188, 79, 130, 104, 154, 3, 44, 113, 30, 64, 135, 87, 41, 40, 14, 52, 43, 145, 120, 91, 179, 51, 100, 174, 152, 10, 119, 128, 180, 46, 111, 89, 77, 165, 18, 56, 17, 66, 25, 158, 108, 139, 187, 173, 15, 53, 122, 31, 183, 47, 49, 5, 155, 9, 29, 94]\n",
            "self.series.num_trials(split): 24\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae/checkpoints/382799060/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae_epoch=35-val/norm_mse=0.28.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (key_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (value_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (out_projection): Linear(in_features=300, out_features=200, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): PerformerAttention(\n",
            "    (kernel_fn): ReLU()\n",
            "  )\n",
            "  (query_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (key_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (value_projection): Linear(in_features=200, out_features=300, bias=True)\n",
            "  (out_projection): Linear(in_features=300, out_features=200, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: None\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 200\n",
            "\t\tFF Dim: 700\n",
            "\t\tEnc Layers: 5\n",
            "\t\tDec Layers: 6\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "self.total_samples: 128\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "input_mat.shape: torch.Size([29, 232])\n",
            "num_samples: 8\n",
            "input_mat: tensor([[0.3728, 0.3183, 0.5426, 0.4925, 0.3043, 0.3612, 0.4246, 0.4812],\n",
            "        [0.1994, 0.2062, 0.4398, 0.2146, 0.2982, 0.3252, 0.2937, 0.3030],\n",
            "        [0.2756, 0.3115, 0.5920, 0.3787, 0.3261, 0.3058, 0.3590, 0.3028],\n",
            "        [0.2663, 0.2303, 0.5444, 0.3119, 0.3117, 0.2845, 0.3428, 0.3275],\n",
            "        [0.2277, 0.2439, 0.4148, 0.2460, 0.2911, 0.2392, 0.2947, 0.2380],\n",
            "        [0.2939, 0.2732, 0.5669, 0.3150, 0.3707, 0.4563, 0.3004, 0.3527],\n",
            "        [0.1605, 0.1687, 0.3127, 0.1850, 0.2039, 0.1918, 0.2206, 0.1771],\n",
            "        [0.4295, 0.3979, 0.7361, 0.4453, 0.4436, 0.4128, 0.3944, 0.4283],\n",
            "        [0.2754, 0.2480, 0.4917, 0.3473, 0.2709, 0.3857, 0.3415, 0.3241],\n",
            "        [0.2900, 0.3213, 0.9124, 0.4436, 0.3793, 0.3915, 0.4292, 0.3700],\n",
            "        [0.1092, 0.1451, 0.2432, 0.1301, 0.1700, 0.1818, 0.2201, 0.1558],\n",
            "        [0.3676, 0.3642, 0.5598, 0.4584, 0.3059, 0.3609, 0.3886, 0.4204],\n",
            "        [0.1612, 0.2017, 0.2898, 0.1761, 0.2064, 0.2161, 0.1903, 0.1756],\n",
            "        [0.2565, 0.3129, 0.5123, 0.3391, 0.2326, 0.3460, 0.2935, 0.2797],\n",
            "        [0.1472, 0.1495, 0.2588, 0.1504, 0.1587, 0.1951, 0.1739, 0.1800],\n",
            "        [0.3936, 0.3306, 0.5783, 0.4437, 0.3322, 0.3746, 0.3348, 0.3252],\n",
            "        [0.3477, 0.2904, 0.5557, 0.5018, 0.3198, 0.2635, 0.3787, 0.4028],\n",
            "        [0.1609, 0.2056, 0.3368, 0.2009, 0.2079, 0.2171, 0.1757, 0.2357],\n",
            "        [0.4366, 0.3891, 0.5861, 0.4336, 0.3552, 0.5094, 0.4606, 0.4270],\n",
            "        [0.4197, 0.3166, 0.5056, 0.3351, 0.3780, 0.4701, 0.3807, 0.5413],\n",
            "        [0.3429, 0.3510, 0.4766, 0.3397, 0.4238, 0.3837, 0.3392, 0.3647],\n",
            "        [0.2667, 0.2878, 0.4800, 0.3628, 0.3018, 0.3036, 0.2738, 0.3486],\n",
            "        [0.3857, 0.3070, 0.5408, 0.4334, 0.3569, 0.3595, 0.3618, 0.3738],\n",
            "        [0.1633, 0.1998, 0.3270, 0.1743, 0.2059, 0.1939, 0.1957, 0.1644],\n",
            "        [0.2854, 0.3568, 0.6366, 0.3517, 0.3910, 0.3648, 0.4693, 0.3658],\n",
            "        [0.3230, 0.3245, 0.5232, 0.3842, 0.2811, 0.4104, 0.3766, 0.2948],\n",
            "        [0.3675, 0.3242, 0.5409, 0.4348, 0.3870, 0.4483, 0.4146, 0.3565],\n",
            "        [0.1570, 0.2130, 0.3150, 0.1668, 0.2310, 0.2250, 0.1859, 0.1901],\n",
            "        [0.1720, 0.1331, 0.3365, 0.1994, 0.2139, 0.2002, 0.1941, 0.2025]])\n",
            "output: tensor([0.4122, 0.2850, 0.3564, 0.3274, 0.2744, 0.3661, 0.2026, 0.4610, 0.3356,\n",
            "        0.4421, 0.1694, 0.4032, 0.2022, 0.3216, 0.1767, 0.3891, 0.3825, 0.2176,\n",
            "        0.4497, 0.4184, 0.3777, 0.3281, 0.3899, 0.2030, 0.4026, 0.3647, 0.4092,\n",
            "        0.2105, 0.2065])\n",
            "SAVING ATTENTION MATRICES...\n",
            "cross_attn_numpy: tensor([[0.4122, 0.4205, 0.4812, 0.5252, 0.3864, 0.3569, 0.3515, 0.4129, 0.4168,\n",
            "         0.3597, 0.4179, 0.3804, 0.3874, 0.4564, 0.3638, 0.4514, 0.3852, 0.4396,\n",
            "         0.5229, 0.4618, 0.3835, 0.4229, 0.4053, 0.3874, 0.4713, 0.3911, 0.4262,\n",
            "         0.3835, 0.3482],\n",
            "        [0.2850, 0.3124, 0.2771, 0.2924, 0.2820, 0.3004, 0.2785, 0.3184, 0.3646,\n",
            "         0.2950, 0.3887, 0.3728, 0.3241, 0.3027, 0.3076, 0.3635, 0.2323, 0.3140,\n",
            "         0.3742, 0.3675, 0.2924, 0.3203, 0.2785, 0.3957, 0.3105, 0.3575, 0.3453,\n",
            "         0.3145, 0.2960],\n",
            "        [0.3564, 0.3362, 0.4435, 0.3938, 0.3357, 0.3331, 0.3481, 0.3840, 0.4518,\n",
            "         0.3450, 0.4211, 0.3865, 0.4362, 0.4128, 0.3525, 0.4626, 0.3703, 0.4432,\n",
            "         0.4406, 0.3959, 0.3496, 0.3822, 0.4016, 0.3335, 0.4022, 0.3971, 0.3635,\n",
            "         0.3705, 0.3274],\n",
            "        [0.3274, 0.3183, 0.3608, 0.3513, 0.3181, 0.2910, 0.2776, 0.3609, 0.3913,\n",
            "         0.2882, 0.4054, 0.3429, 0.3406, 0.3342, 0.2850, 0.3937, 0.2763, 0.3687,\n",
            "         0.3970, 0.3804, 0.3033, 0.3350, 0.3450, 0.3340, 0.3529, 0.3267, 0.3238,\n",
            "         0.3414, 0.2786],\n",
            "        [0.2744, 0.2434, 0.2449, 0.2599, 0.3285, 0.2493, 0.2188, 0.2948, 0.2953,\n",
            "         0.2174, 0.2600, 0.2375, 0.2283, 0.2419, 0.2256, 0.2695, 0.2194, 0.2658,\n",
            "         0.2850, 0.2800, 0.2145, 0.2393, 0.2423, 0.2475, 0.2899, 0.2549, 0.2531,\n",
            "         0.2460, 0.2347],\n",
            "        [0.3661, 0.4081, 0.4214, 0.3995, 0.3612, 0.3566, 0.3791, 0.3935, 0.4365,\n",
            "         0.3863, 0.4420, 0.4400, 0.4368, 0.3779, 0.3977, 0.4295, 0.3267, 0.4002,\n",
            "         0.4353, 0.4581, 0.3502, 0.4066, 0.3580, 0.4088, 0.3675, 0.4313, 0.4530,\n",
            "         0.3587, 0.3210],\n",
            "        [0.2026, 0.2034, 0.2360, 0.2226, 0.2210, 0.1714, 0.2676, 0.2305, 0.2387,\n",
            "         0.1936, 0.1948, 0.2190, 0.2448, 0.2195, 0.2026, 0.2413, 0.1962, 0.2227,\n",
            "         0.2479, 0.2182, 0.1843, 0.2121, 0.2154, 0.2191, 0.2150, 0.2226, 0.2078,\n",
            "         0.2239, 0.1745],\n",
            "        [0.4610, 0.4349, 0.4595, 0.4860, 0.4797, 0.3978, 0.3712, 0.5046, 0.5569,\n",
            "         0.4337, 0.4163, 0.4730, 0.4647, 0.3988, 0.3664, 0.5637, 0.3808, 0.4573,\n",
            "         0.5074, 0.4941, 0.4266, 0.4455, 0.4030, 0.4332, 0.4718, 0.4695, 0.4709,\n",
            "         0.3965, 0.4216],\n",
            "        [0.3356, 0.3368, 0.4648, 0.4505, 0.3440, 0.3256, 0.3294, 0.3974, 0.3876,\n",
            "         0.3215, 0.3883, 0.3733, 0.3789, 0.3882, 0.3398, 0.4136, 0.3162, 0.3788,\n",
            "         0.4475, 0.3901, 0.3213, 0.3773, 0.3514, 0.4050, 0.3795, 0.3934, 0.3739,\n",
            "         0.3654, 0.2979],\n",
            "        [0.4421, 0.3452, 0.4457, 0.4395, 0.3763, 0.3644, 0.3503, 0.4300, 0.5088,\n",
            "         0.3430, 0.4075, 0.3840, 0.3870, 0.4152, 0.3514, 0.4229, 0.3396, 0.4329,\n",
            "         0.4351, 0.4301, 0.3330, 0.3797, 0.3993, 0.3991, 0.3894, 0.4368, 0.3859,\n",
            "         0.3865, 0.3402],\n",
            "        [0.1694, 0.1641, 0.1739, 0.1741, 0.1656, 0.1623, 0.1726, 0.1982, 0.1993,\n",
            "         0.1585, 0.2699, 0.2005, 0.1953, 0.1642, 0.1917, 0.2174, 0.1540, 0.1822,\n",
            "         0.1998, 0.1831, 0.1691, 0.1749, 0.1786, 0.1866, 0.2035, 0.1754, 0.1643,\n",
            "         0.1845, 0.1713],\n",
            "        [0.4032, 0.4111, 0.4568, 0.4289, 0.4095, 0.3533, 0.4163, 0.4422, 0.4648,\n",
            "         0.3522, 0.3470, 0.3913, 0.4768, 0.4797, 0.3830, 0.5282, 0.4331, 0.5058,\n",
            "         0.5008, 0.4092, 0.3834, 0.4821, 0.4157, 0.4241, 0.4505, 0.4030, 0.4119,\n",
            "         0.4055, 0.3870],\n",
            "        [0.2022, 0.2033, 0.2302, 0.2242, 0.2030, 0.1935, 0.1907, 0.2148, 0.2398,\n",
            "         0.1903, 0.2075, 0.2294, 0.3049, 0.1831, 0.1936, 0.2465, 0.1819, 0.2290,\n",
            "         0.2479, 0.2154, 0.1992, 0.2179, 0.1888, 0.2107, 0.2084, 0.2232, 0.2109,\n",
            "         0.1914, 0.1855],\n",
            "        [0.3216, 0.2884, 0.4153, 0.3637, 0.3139, 0.3244, 0.3556, 0.3837, 0.3956,\n",
            "         0.3256, 0.3752, 0.3415, 0.3481, 0.4222, 0.3090, 0.4470, 0.3197, 0.4054,\n",
            "         0.4016, 0.3607, 0.2813, 0.3544, 0.3508, 0.3533, 0.3301, 0.3636, 0.2964,\n",
            "         0.3554, 0.3131],\n",
            "        [0.1767, 0.1708, 0.1798, 0.1861, 0.1951, 0.1602, 0.1647, 0.2155, 0.2230,\n",
            "         0.1615, 0.1717, 0.2007, 0.1920, 0.1797, 0.2685, 0.2320, 0.1471, 0.1845,\n",
            "         0.2151, 0.1833, 0.1512, 0.1936, 0.1779, 0.1787, 0.1827, 0.2027, 0.1630,\n",
            "         0.1828, 0.1690],\n",
            "        [0.3891, 0.3721, 0.4978, 0.4497, 0.4195, 0.4061, 0.3917, 0.4700, 0.4863,\n",
            "         0.3334, 0.3547, 0.3883, 0.3963, 0.4329, 0.3545, 0.4533, 0.4263, 0.5101,\n",
            "         0.5848, 0.3497, 0.3258, 0.4370, 0.3966, 0.4378, 0.4384, 0.3770, 0.3633,\n",
            "         0.3558, 0.3549],\n",
            "        [0.3825, 0.3817, 0.3801, 0.3931, 0.4329, 0.3511, 0.3673, 0.4363, 0.4298,\n",
            "         0.3259, 0.3660, 0.3557, 0.4196, 0.4361, 0.3613, 0.4642, 0.3844, 0.4161,\n",
            "         0.4828, 0.4270, 0.3495, 0.3928, 0.3620, 0.3782, 0.4814, 0.3834, 0.4016,\n",
            "         0.3307, 0.3674],\n",
            "        [0.2176, 0.2229, 0.2286, 0.2224, 0.2310, 0.2051, 0.2109, 0.2618, 0.2665,\n",
            "         0.2030, 0.1992, 0.2345, 0.2323, 0.2075, 0.2165, 0.2892, 0.1877, 0.3108,\n",
            "         0.2631, 0.2542, 0.2172, 0.2295, 0.2107, 0.2255, 0.2335, 0.2220, 0.2027,\n",
            "         0.1971, 0.2076],\n",
            "        [0.4497, 0.4402, 0.4647, 0.4970, 0.4103, 0.3728, 0.3918, 0.4460, 0.4851,\n",
            "         0.3566, 0.3448, 0.4240, 0.4198, 0.4476, 0.4027, 0.4803, 0.4058, 0.4690,\n",
            "         0.5160, 0.4040, 0.3840, 0.4811, 0.4397, 0.3853, 0.4318, 0.4516, 0.4378,\n",
            "         0.3997, 0.3214],\n",
            "        [0.4184, 0.4344, 0.4086, 0.4595, 0.3801, 0.3796, 0.3607, 0.4646, 0.4830,\n",
            "         0.3822, 0.3808, 0.4556, 0.4485, 0.3822, 0.4039, 0.5259, 0.3482, 0.4448,\n",
            "         0.4768, 0.4046, 0.4136, 0.4921, 0.3899, 0.4229, 0.4279, 0.4279, 0.4502,\n",
            "         0.4057, 0.4061],\n",
            "        [0.3777, 0.3720, 0.4103, 0.4046, 0.4134, 0.3594, 0.3622, 0.4690, 0.4559,\n",
            "         0.3789, 0.4119, 0.4321, 0.4460, 0.3724, 0.3532, 0.5348, 0.3286, 0.4377,\n",
            "         0.4789, 0.4390, 0.3759, 0.4372, 0.3727, 0.3834, 0.4103, 0.4462, 0.3796,\n",
            "         0.3583, 0.4030],\n",
            "        [0.3281, 0.3619, 0.4244, 0.3792, 0.3475, 0.3593, 0.3238, 0.3346, 0.4113,\n",
            "         0.3532, 0.3429, 0.3564, 0.3861, 0.3544, 0.3547, 0.4795, 0.2884, 0.3511,\n",
            "         0.4690, 0.3929, 0.3276, 0.4058, 0.3067, 0.3636, 0.3776, 0.4598, 0.4076,\n",
            "         0.3530, 0.3138],\n",
            "        [0.3899, 0.4194, 0.4471, 0.4236, 0.3477, 0.3614, 0.3652, 0.3985, 0.5337,\n",
            "         0.3605, 0.4340, 0.4336, 0.4361, 0.3834, 0.3917, 0.4402, 0.3666, 0.4768,\n",
            "         0.4490, 0.4238, 0.3450, 0.4704, 0.4502, 0.3931, 0.4108, 0.4423, 0.4758,\n",
            "         0.3691, 0.3300],\n",
            "        [0.2030, 0.2019, 0.2300, 0.2155, 0.2179, 0.2040, 0.2120, 0.2242, 0.2357,\n",
            "         0.2005, 0.2025, 0.2102, 0.2183, 0.1965, 0.2048, 0.2412, 0.1856, 0.2256,\n",
            "         0.2478, 0.2291, 0.2069, 0.2142, 0.2051, 0.3177, 0.2317, 0.2038, 0.2079,\n",
            "         0.2210, 0.1954],\n",
            "        [0.4026, 0.3638, 0.4452, 0.4250, 0.3756, 0.3648, 0.3654, 0.4484, 0.4865,\n",
            "         0.3436, 0.4206, 0.4387, 0.4133, 0.4188, 0.3535, 0.4452, 0.3724, 0.4832,\n",
            "         0.4338, 0.3983, 0.3440, 0.4183, 0.4022, 0.3863, 0.4563, 0.4352, 0.4286,\n",
            "         0.3872, 0.3768],\n",
            "        [0.3647, 0.3692, 0.4444, 0.4611, 0.3998, 0.3623, 0.4395, 0.4128, 0.4559,\n",
            "         0.3917, 0.3846, 0.4366, 0.4506, 0.4628, 0.3612, 0.4471, 0.4096, 0.4710,\n",
            "         0.4841, 0.3658, 0.3323, 0.4320, 0.4617, 0.4146, 0.4184, 0.4230, 0.3697,\n",
            "         0.3582, 0.3328],\n",
            "        [0.4092, 0.4115, 0.4350, 0.4608, 0.3521, 0.3648, 0.3899, 0.4599, 0.5002,\n",
            "         0.3504, 0.3878, 0.4007, 0.4719, 0.4646, 0.3715, 0.4482, 0.3839, 0.4618,\n",
            "         0.6011, 0.4162, 0.3958, 0.5280, 0.4032, 0.4112, 0.3987, 0.4581, 0.3998,\n",
            "         0.3614, 0.3433],\n",
            "        [0.2105, 0.2023, 0.2117, 0.2184, 0.1950, 0.2076, 0.2163, 0.2393, 0.2272,\n",
            "         0.2060, 0.2248, 0.2327, 0.2059, 0.2021, 0.2110, 0.2697, 0.1692, 0.2133,\n",
            "         0.2313, 0.2396, 0.1919, 0.2081, 0.1748, 0.2149, 0.2241, 0.2063, 0.2047,\n",
            "         0.2928, 0.1996],\n",
            "        [0.2065, 0.2098, 0.2168, 0.2197, 0.2251, 0.2008, 0.2164, 0.2452, 0.2382,\n",
            "         0.1970, 0.2256, 0.2419, 0.2324, 0.2235, 0.2367, 0.2638, 0.2159, 0.2403,\n",
            "         0.2774, 0.2401, 0.2234, 0.2336, 0.2084, 0.2241, 0.2637, 0.2137, 0.2398,\n",
            "         0.2127, 0.3024]])\n",
            "./plots_checkpoints_logs/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae/plots/382799060/test_cross_attn_matrix/attn_matrix_layer1.png\n",
            "[[[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]]\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 200\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -8\n",
            "self._slice_start_points: [155, 131, 24, 68, 87, 103, 142, 21, 185, 108, 123, 153, 182, 15, 4, 117, 69, 82, 135, 163, 79, 187, 42, 3, 73, 107, 72, 86, 115, 12, 8, 112, 120, 160, 119, 174, 17, 25, 46, 98, 100, 124, 30, 27, 158, 106, 34, 148, 138, 49, 137, 139, 63, 65, 29, 39, 157, 44, 128, 169, 97, 144, 190, 61, 149, 161, 104, 109, 162, 43, 58, 102, 22, 55, 150, 125, 51, 134, 19, 77, 71, 14, 35, 13, 23, 11, 132, 140, 99, 145, 101, 41, 111, 166, 168, 48, 172, 67, 105, 129, 133, 56, 167, 10, 181, 81, 113, 54, 146, 173, 164, 114, 70, 38, 126, 156, 118, 178, 9, 50, 176, 76, 16, 110, 5, 84, 151, 95, 147, 143, 60, 62, 130, 80, 57, 32, 37, 96, 40, 78, 136, 26, 186, 159, 189, 141, 165, 74, 121, 85, 92, 170, 180, 33, 66, 179, 122, 28, 116, 47, 171, 175, 154, 6, 94, 75, 1, 183, 52, 0, 53, 36, 152, 188, 93, 59, 2, 45, 89, 88, 191, 177, 91, 31, 18, 90, 20, 64, 184, 83, 127, 7]\n",
            "self.series.num_trials(split): 24\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae/checkpoints/382799060/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae_epoch=35-val/norm_mse=0.28.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae/checkpoints/382799060/generated, 29ch, huge model, bs 128, seed 0, ws 1000, b_lr 1e-3, initial_downsample_convs 0, local_self_attn none, local_cross_attn none, tp 1, tf 3, loss mae_epoch=35-val/norm_mse=0.28.ckpt\n",
            "Testing DataLoader 0:   0% 0/36 [00:00<?, ?it/s]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Testing DataLoader 0:  56% 20/36 [00:14<00:11,  1.36it/s]PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "PLOTTING...\n",
            "type(forecast_output)  <class 'torch.Tensor'>\n",
            "Testing DataLoader 0: 100% 36/36 [00:32<00:00,  1.12it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc                    1.0\n",
            "     test/class_loss       0.0008099622209556401\n",
            "   test/forecast_loss       0.4071308672428131\n",
            "        test/loss           0.40713098645210266\n",
            "        test/mae            0.4071308672428131\n",
            "        test/mape           3.5187692642211914\n",
            "        test/mse            0.2707141935825348\n",
            "      test/norm_mae         0.4071308672428131\n",
            "      test/norm_mse         0.2707141935825348\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.5134385228157043\n",
            "       test/smape            0.82837975025177\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python test_eeg.py spacetimeformer eeg_social_memory --run_name spatiotemporal_eeg_social_memory --d_model 100 --d_ff 400 --enc_layers 4 --dec_layers 4 --batch_size 192 --start_token_len 4 --n_heads 4 --grad_clip_norm 1 --trials 1"
      ],
      "metadata": {
        "id": "NoxHwP-p0SDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python plot_test_attention_matrix.py spacetimeformer eeg_social_memory --d_model 200 --d_ff 700 --enc_layers 5 --dec_layers 6 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 30 --d_v 30 --n_heads 10 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn none --local_cross_attn none --time_emb_dim 6 --attn_plot\n",
        "!python plot_test_attention_matrix.py spacetimeformer eeg_social_memory --d_model 15 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6 --attn_plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf295cEyvIMA",
        "outputId": "56508217-00c1-4ef8-a5b5-e9b12b52bd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt\n",
            "data_path: ./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch30_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 30)\n",
            "yc.shape: (96, 125, 30)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.11045897 -0.12415218 -0.12058142 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.40830655 11.54580482 10.93482488 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 30)\n",
            "(125, 2010)\n",
            "[ 1.99615518  1.97167855  3.62424836 ...  9.79579117 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.43920224 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.70064388 -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.48073776 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.56348265  0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.19582588 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -0.68676453 -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.66757082  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.65470285  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  1.29259171  1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.90133569  0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.10073837  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ... -0.32523906 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.67949681  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ... -0.17755435  0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.49728133 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.48447895 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  1.02813193  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.70810622 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.12229146  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.23228592  0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.60375175  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.74273023 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 30)\n",
            "(125, 870)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 11.99715269 12.86104958 11.94699925 11.85080883 11.84452559\n",
            "  1.88184776  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705\n",
            "  6.33410066  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318\n",
            "  6.79552098  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289\n",
            "  6.77824521  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921\n",
            "  9.86928682 11.93068806 12.90250279 11.89939114 11.79367808 11.86000642\n",
            "  2.06883189  1.91661711  3.85233037  4.42292723  4.1374595   6.81035074\n",
            "  6.64573315  7.18731759  6.26918517  6.01881812  6.97541996  7.27111397\n",
            "  6.98355443  7.35192665  7.54824275  3.25604588  2.23328104  6.81108219\n",
            "  6.57736118  6.86432759  9.95974759  9.31834055 10.18899777  9.63590846\n",
            "  9.12828423 12.05575209 12.47919508 11.63939716 11.96261054 12.15679174\n",
            "  2.08443065  1.98596134  3.83962243  4.22279771  3.76795588  6.42752564\n",
            "  6.13132809  6.69774275  5.68827778  6.23725527  6.67127157  6.90169847\n",
            "  6.55130188  7.12823226  7.49348426  3.14803762  2.06132263  6.43459622\n",
            "  6.37124187  5.98529942  9.62134554  8.89269879  9.82472116  8.96765425\n",
            "  8.63643789 11.71539883 11.55649842 11.29742551 11.61590913 11.76704991\n",
            "  2.05714907  2.01596769  3.81920868  4.04543441  3.67401784  6.40404012\n",
            "  5.84459665  6.26471958  5.19789279  6.09455435  6.84464184  6.62593371\n",
            "  6.46554987  8.02217773  7.74906949  3.48674251  2.51977878  6.96327518\n",
            "  7.29676781  6.45964461  9.89411476  9.5076942  10.50630294  8.95039971\n",
            "  8.51054961 11.94941264 11.70165593 11.36950736 11.78133177 11.6031362\n",
            "  1.950334    1.89589981  3.86060154  4.18198789  3.57699067  7.10774686\n",
            "  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541  7.29071108\n",
            "  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978  7.45958959\n",
            "  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541   9.80036353\n",
            "  9.69273876 12.86747024 12.61101867 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.25216021 12.08176313 11.82400636 11.50908441 11.46147175\n",
            "  1.99033952  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896\n",
            "  6.32339266  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582\n",
            "  7.29277914  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263\n",
            "  7.71857688  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158\n",
            "  8.64725042 12.11688656 11.52212503 10.798857   11.6021242  11.21125968\n",
            "  1.80121598  2.07652659  3.7605136   4.24542613  4.06859539  7.65918282\n",
            "  6.69040362  7.39731648  6.49430049  6.3643217   7.48429195  7.52232488\n",
            "  7.37332257  8.0513519   7.98824847  3.57452942  2.41812451  7.88520733\n",
            "  8.11533146  6.89586103 11.1710355  10.47776898 11.03357716  9.30173033\n",
            "  8.69311485 12.25332822 11.84272318 10.68338069 11.66820011 11.05561652\n",
            "  1.82791145  2.09208537  3.50735831  4.15594171  4.09518794  6.79583825\n",
            "  6.56195016  6.97421609  6.66097041  6.61589602  8.10152145  8.22978585\n",
            "  8.02613303  8.7729156   8.75035593  3.67001391  2.3804257   7.02003142\n",
            "  7.17652939  6.85871531 10.67842269 10.13450527 10.45496443  9.2346348\n",
            " 10.23483608 11.84921294 11.57971618 11.10154951 11.74282084 11.29401002\n",
            "  1.93100064  2.02021931  3.74552906  4.01431827  3.97064439  6.29691915\n",
            "  5.96857677  6.26159764  5.99635367  6.45736633  7.47985335  7.62232341\n",
            "  7.55674052  8.52537525  8.6450095   3.47750314  2.26931219  7.60395089\n",
            "  7.21287068  6.67717042 11.34364005  9.72940255 10.02639598  9.03840838\n",
            " 10.18675135 11.51147703 11.28135554 10.96719716 11.50737969 11.23066426\n",
            "  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736  6.45863912\n",
            "  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856  6.95333873\n",
            "  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933  7.53669674\n",
            "  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353  9.08023829\n",
            "  9.58226092 11.42916339 10.97640557 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.99304668 11.66070749 11.33506908 11.84901884 11.67320766\n",
            "  1.84477013  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485\n",
            "  5.98492494  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852\n",
            "  7.75480676  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184\n",
            "  7.80570743  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141\n",
            " 10.2976647  13.27774384 12.96302158 12.20277853 12.83705054 12.6330725\n",
            "  1.78979467  2.17622104  4.0068727   4.63627272  4.03731734  6.53891699\n",
            "  5.97634515  7.03584538  5.99649078  6.13472289  7.61490571  7.5650414\n",
            "  7.68145861  9.10214271  8.94473244  3.75972078  2.62109792  7.95489467\n",
            "  7.2869975   7.16152792 11.67771207 10.99031344 11.01307477 10.03977555\n",
            " 10.39463715 13.5985274  13.27036878 12.29980717 13.05854175 12.82895091\n",
            "  1.80468748  2.1451755   4.0995777   4.64108936  3.72679634  6.67297214\n",
            "  6.51383983  7.45839902  6.05540903  6.45660766  7.91017549  7.81849634\n",
            "  7.67847763  9.25570329  8.85866284  3.61671009  2.40130729  8.13099911\n",
            "  7.48708786  7.43859304 11.75584735 11.03327799 11.31430319 10.40234949\n",
            " 10.05417797 13.17285163 13.25988943 12.24764307 12.88071884 12.26067668\n",
            "  1.90395885  2.12960838  3.95577557  4.49204244  4.14381524  7.24982919\n",
            "  6.50271528  7.36651585  6.342633    6.68792736  7.93570197  7.86236322\n",
            "  7.88793578  9.42768574  9.08173909  3.40585311  2.34139841  8.10039798\n",
            "  8.03019026  7.38740474 10.9645403  10.58078801 11.55560702 10.15624638\n",
            "  8.96728021 12.20744883 12.9980197  11.51224922 11.94614962 11.28404435\n",
            "  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508  6.03673841\n",
            "  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181  7.22216747\n",
            "  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362  7.43157486\n",
            "  7.90466022  7.03058532 10.27483583  9.68283273 11.007388    9.75187948\n",
            "  8.8189279  11.80604791 12.70660893 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 12.41179997 13.07154775 11.30195552 11.50207537 11.1120345\n",
            "  1.74263235  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487\n",
            "  6.68735852  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508\n",
            "  8.08984167  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427\n",
            "  7.78653692  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699\n",
            "  9.77417063 12.7285977  12.88490919 11.44445635 11.84196478 11.36618485\n",
            "  1.82340721  2.08091064  3.89253523  4.76246475  4.43146198  5.89471786\n",
            "  6.11866813  7.6460129   6.50855249  6.13933745  7.48120544  8.0305169\n",
            "  8.01999366  8.51148361  8.75731073  3.63449301  2.29258968  6.75669259\n",
            "  7.47531682  7.08705483  9.56218281  9.24274512  9.9277007   9.22632594\n",
            "  8.96505438 11.46346181 11.71290237 10.96461481 10.84702527 10.96869271\n",
            "  1.85265466  2.04588677  3.76453158  4.17742994  4.22585425  5.89983552\n",
            "  5.50978835  7.02875411  5.97987863  5.82679902  6.70321513  7.12044411\n",
            "  6.91637158  7.64008616  7.76636463  3.62038724  2.39178626  7.52954389\n",
            "  7.48654883  6.90144591 10.44061308 10.13180395 10.29338773  9.51773815\n",
            "  9.31238487 12.11335282 11.89242115 11.25800381 11.42653252 11.19620962\n",
            "  1.77877023  1.90417067  3.78619061  4.28783371  4.01827456  6.26846414\n",
            "  5.89338791  7.14622311  6.11958558  6.41843992  6.8310591   7.06351754\n",
            "  6.70989625  7.65707323  7.59317665  3.56359132  2.34372216  7.16470528\n",
            "  7.40622305  6.9304806  11.30085323 10.12922744 10.47519558 10.00240709\n",
            " 10.72385311 12.1556716  12.28089028 11.77264389 11.82849204 11.74066985\n",
            "  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027   6.78068766\n",
            "  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938  7.32308781\n",
            "  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383  6.77399259\n",
            "  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168  9.29551629\n",
            " 10.0615734  10.78465069 11.28879561 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.07824619 10.54847444 10.35848873  9.64748684 10.46887137\n",
            "  1.83425429  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219\n",
            "  6.01757752  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096\n",
            "  6.59436719  7.01664582  7.28995495  3.83778333  2.51717108  7.992437\n",
            "  7.55642944  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672\n",
            "  9.7939879  11.60442643 11.98134946 11.27213917 10.98061171 11.2538474\n",
            "  1.92155723  1.82960492  3.96447411  4.4602094   3.98225844  5.43861127\n",
            "  6.21333084  7.08304519  5.88378244  6.05876977  6.92338628  7.24032222\n",
            "  6.81278692  7.63072764  7.68090723  3.72236349  2.39430203  7.70540307\n",
            "  6.88671585  6.50260885 11.29525907 10.59755963 10.97248782  9.77152655\n",
            "  9.86258568 12.35229027 12.65799275 11.15616714 11.5224307  11.19846076\n",
            "  1.89743321  1.74721883  4.21425752  4.99200531  4.55385035  6.14514044\n",
            "  6.4456965   7.63585205  6.22354469  6.3173814   7.16293758  7.60438664\n",
            "  7.19649234  7.87391972  8.08140706  3.70057102  2.2049803   6.98808832\n",
            "  6.95730381  6.64506007 10.75521731 10.25717163 10.82306247  9.24617346\n",
            " 10.21107356 11.92073779 12.41134609 10.82055414 11.08829494 10.91265827\n",
            "  1.72028084  1.89636765  4.03164913  4.63303388  4.17626539  6.64726211\n",
            "  6.43809779  7.25837738  5.85767341  6.20732657  6.87061902  7.07470772\n",
            "  7.04337365  7.45866082  7.81708581  3.54552947  2.43026236  7.2873142\n",
            "  8.02621394  7.34897411 10.77711788 10.77382234 11.8624731  10.37867082\n",
            " 10.66698994 12.29603346 13.20818118 12.39604745 12.19674886 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 30)\n",
            "(125, 2880)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 30)\n",
            "(29, 125, 30)\n",
            "(96, 125, 30)\n",
            "yc.shape: (96, 125, 30)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18  5.65288557e-18 -1.87627691e-17  2.23339865e-17\n",
            "  1.00382665e-17 -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "args.dec_layers: 2\n",
            "d_layers: range(0, 2)\n",
            "layer: 0\n",
            "self.total_samples: 128\n",
            "input_mat.shape: torch.Size([300, 300])\n",
            "num_samples: 10\n",
            "input_mat: tensor([[0.0184, 0.0092, 0.0061, 0.0201, 0.0153, 0.0119, 0.0054, 0.0095, 0.0113,\n",
            "         0.0437],\n",
            "        [0.0151, 0.0167, 0.0061, 0.0214, 0.0153, 0.0125, 0.0103, 0.0324, 0.0182,\n",
            "         0.0920],\n",
            "        [0.0343, 0.0182, 0.0044, 0.0164, 0.0184, 0.0090, 0.0028, 0.0145, 0.0074,\n",
            "         0.0381],\n",
            "        [0.0126, 0.0133, 0.0038, 0.0115, 0.0099, 0.0065, 0.0044, 0.0132, 0.0109,\n",
            "         0.0497],\n",
            "        [0.0080, 0.0091, 0.0024, 0.0109, 0.0067, 0.0055, 0.0035, 0.0122, 0.0073,\n",
            "         0.0415],\n",
            "        [0.0175, 0.0072, 0.0018, 0.0095, 0.0098, 0.0049, 0.0015, 0.0049, 0.0037,\n",
            "         0.0129],\n",
            "        [0.0084, 0.0078, 0.0025, 0.0086, 0.0061, 0.0054, 0.0044, 0.0116, 0.0068,\n",
            "         0.0212],\n",
            "        [0.0077, 0.0059, 0.0059, 0.0103, 0.0096, 0.0057, 0.0055, 0.0039, 0.0220,\n",
            "         0.0615],\n",
            "        [0.0041, 0.0135, 0.0027, 0.0035, 0.0066, 0.0033, 0.0054, 0.0309, 0.0092,\n",
            "         0.1010],\n",
            "        [0.0242, 0.0531, 0.0299, 0.0240, 0.0755, 0.0177, 0.0288, 0.0482, 0.0776,\n",
            "         0.2911]])\n",
            "output: 0.019422942772507668\n",
            "SAVING ATTENTION MATRICES...\n",
            "input_mat.shape: torch.Size([30, 300])\n",
            "num_samples: 10\n",
            "input_mat: tensor([[1.5152e-02, 1.6686e-02, 2.3517e-02, 2.0068e-02, 1.2193e-02, 2.4412e-02,\n",
            "         2.5871e-02, 7.2350e-02, 4.7492e-02, 8.0434e-03],\n",
            "        [6.7768e-03, 5.7902e-03, 4.6261e-03, 5.7159e-03, 5.4228e-03, 5.0169e-03,\n",
            "         4.2094e-03, 2.0786e-03, 2.2610e-03, 9.0169e-03],\n",
            "        [1.4651e-01, 1.3338e-01, 1.7920e-01, 1.6354e-01, 1.2794e-01, 1.6126e-01,\n",
            "         1.7040e-01, 2.8460e-01, 3.0701e-01, 1.7185e-01],\n",
            "        [1.5807e-03, 1.9872e-03, 1.7533e-03, 1.6149e-03, 2.1566e-03, 1.6150e-03,\n",
            "         1.8204e-03, 1.0563e-02, 2.6187e-02, 5.7337e-03],\n",
            "        [2.2422e-02, 2.0596e-02, 2.5675e-02, 2.6764e-02, 1.5038e-02, 3.1526e-02,\n",
            "         3.3341e-02, 1.0170e-01, 5.2241e-02, 1.0030e-02],\n",
            "        [2.5542e-02, 1.9457e-02, 3.9331e-02, 2.4460e-02, 1.4932e-02, 4.6275e-02,\n",
            "         3.8047e-02, 9.8802e-02, 3.4741e-02, 6.9359e-03],\n",
            "        [8.4416e-02, 7.7469e-02, 1.1009e-01, 7.4217e-02, 7.5959e-02, 9.3080e-02,\n",
            "         9.3406e-02, 2.0822e-01, 2.2830e-01, 9.8788e-02],\n",
            "        [2.3978e-03, 2.9050e-03, 2.3726e-03, 2.4825e-03, 2.9212e-03, 3.2004e-03,\n",
            "         3.3464e-03, 9.6840e-03, 2.1417e-02, 5.7704e-03],\n",
            "        [2.4778e-02, 1.9853e-02, 3.0623e-02, 2.6718e-02, 1.8793e-02, 3.8300e-02,\n",
            "         3.3039e-02, 8.2576e-02, 4.6160e-02, 1.9723e-02],\n",
            "        [2.9717e-03, 2.5768e-03, 4.8217e-03, 4.5944e-03, 1.9893e-03, 6.7587e-03,\n",
            "         7.3628e-03, 3.5035e-02, 7.3947e-03, 1.4589e-03],\n",
            "        [1.1685e-02, 1.1027e-02, 1.2915e-02, 1.3479e-02, 1.2314e-02, 1.6274e-02,\n",
            "         1.5175e-02, 3.0369e-02, 2.1054e-02, 1.3149e-02],\n",
            "        [7.2000e-03, 8.1950e-03, 1.1689e-02, 6.4340e-03, 7.0044e-03, 8.6371e-03,\n",
            "         9.8494e-03, 4.4980e-02, 4.5577e-02, 8.3067e-03],\n",
            "        [1.0993e-02, 1.0292e-02, 1.5347e-02, 1.1849e-02, 1.1053e-02, 2.1796e-02,\n",
            "         1.7713e-02, 4.1409e-02, 2.8564e-02, 1.9902e-02],\n",
            "        [7.8351e-03, 8.4983e-03, 1.1957e-02, 6.8157e-03, 8.4492e-03, 1.1984e-02,\n",
            "         1.1517e-02, 3.9569e-02, 2.0489e-02, 9.7871e-03],\n",
            "        [3.6845e-02, 3.1858e-02, 4.2061e-02, 4.4104e-02, 2.9794e-02, 4.8810e-02,\n",
            "         4.5557e-02, 1.0815e-01, 3.5344e-02, 1.3707e-02],\n",
            "        [5.9262e-03, 4.3419e-03, 4.4113e-03, 5.9577e-03, 5.0736e-03, 5.7811e-03,\n",
            "         3.8636e-03, 3.2407e-03, 3.3027e-03, 4.5427e-03],\n",
            "        [6.0041e-03, 6.0306e-03, 5.0853e-03, 5.6885e-03, 6.1913e-03, 6.5135e-03,\n",
            "         5.3514e-03, 2.9379e-03, 2.7713e-03, 1.1520e-02],\n",
            "        [5.6783e-03, 4.3977e-03, 4.5847e-03, 4.5350e-03, 3.3123e-03, 1.0334e-02,\n",
            "         5.2425e-03, 4.9857e-03, 1.8093e-03, 5.7227e-03],\n",
            "        [7.5838e-04, 1.4499e-03, 4.2113e-04, 5.7697e-04, 9.7158e-04, 4.3331e-04,\n",
            "         6.5301e-04, 1.1012e-03, 9.1694e-03, 2.7800e-03],\n",
            "        [7.2972e-03, 1.1377e-02, 5.8169e-03, 4.9952e-03, 7.8056e-03, 4.8917e-03,\n",
            "         5.5111e-03, 6.2044e-03, 7.1574e-03, 1.2814e-02],\n",
            "        [4.6961e-04, 4.9776e-04, 2.8243e-04, 2.8444e-04, 7.2039e-04, 2.5968e-04,\n",
            "         1.9836e-04, 6.8380e-05, 3.8599e-04, 7.0491e-03],\n",
            "        [7.7192e-03, 8.1960e-03, 9.6279e-03, 1.0730e-02, 1.4479e-02, 8.1587e-03,\n",
            "         7.9870e-03, 9.6189e-03, 7.7848e-03, 1.1888e-02],\n",
            "        [1.6739e-02, 2.2493e-02, 1.8089e-02, 1.7969e-02, 3.0098e-02, 1.0598e-02,\n",
            "         1.6349e-02, 1.5890e-02, 7.2661e-02, 7.1784e-02],\n",
            "        [3.0733e-03, 3.3253e-03, 2.2042e-03, 2.5476e-03, 4.2898e-03, 1.9277e-03,\n",
            "         1.8721e-03, 9.7776e-04, 2.8341e-03, 1.6888e-02],\n",
            "        [2.1958e-02, 2.0000e-02, 1.6511e-02, 1.7496e-02, 2.6269e-02, 1.3939e-02,\n",
            "         1.1687e-02, 5.9534e-03, 1.3539e-02, 9.4037e-02],\n",
            "        [2.3419e-04, 2.1366e-04, 1.3058e-04, 1.4400e-04, 2.3619e-04, 3.6521e-05,\n",
            "         5.4001e-05, 8.0923e-06, 8.8180e-05, 1.9560e-03],\n",
            "        [1.3721e-03, 1.1045e-03, 8.9129e-04, 1.2809e-03, 1.3691e-03, 9.2535e-04,\n",
            "         5.4489e-04, 2.7518e-04, 8.8678e-05, 1.8573e-03],\n",
            "        [9.2848e-03, 1.0700e-02, 7.8795e-03, 9.3405e-03, 1.0916e-02, 6.2848e-03,\n",
            "         6.7342e-03, 4.7249e-03, 1.2435e-02, 4.7477e-02],\n",
            "        [1.8878e-04, 1.6125e-04, 8.9978e-05, 2.2256e-04, 2.3218e-04, 1.2644e-04,\n",
            "         1.0360e-04, 1.1439e-05, 2.3635e-05, 2.7495e-03],\n",
            "        [1.3257e-03, 1.2062e-03, 7.9384e-04, 1.1070e-03, 1.2348e-03, 5.3067e-04,\n",
            "         5.0863e-04, 1.2756e-04, 2.4610e-04, 2.8007e-03]])\n",
            "output: tensor([0.0266, 0.0051, 0.1846, 0.0055, 0.0339, 0.0349, 0.1144, 0.0056, 0.0341,\n",
            "        0.0075, 0.0157, 0.0158, 0.0189, 0.0137, 0.0436, 0.0046, 0.0058, 0.0051,\n",
            "        0.0018, 0.0074, 0.0010, 0.0096, 0.0293, 0.0040, 0.0241, 0.0003, 0.0010,\n",
            "        0.0126, 0.0004, 0.0010])\n",
            "SAVING ATTENTION MATRICES...\n",
            "layer: 1\n",
            "self.total_samples: 128\n",
            "input_mat.shape: torch.Size([300, 300])\n",
            "num_samples: 10\n",
            "input_mat: tensor([[2.3632e-01, 2.4286e-01, 2.6436e-01, 1.7123e-01, 2.8153e-01, 1.9780e-01,\n",
            "         2.5714e-01, 1.4102e-01, 4.5641e-01, 1.0000e+00],\n",
            "        [2.7310e-01, 2.7990e-01, 2.5937e-01, 2.4405e-01, 2.6072e-01, 2.4224e-01,\n",
            "         3.2985e-01, 2.2523e-01, 4.5415e-01, 6.4422e-01],\n",
            "        [3.8737e-02, 4.3423e-02, 6.1648e-02, 5.1075e-02, 5.7650e-02, 5.7530e-02,\n",
            "         7.6569e-02, 5.7411e-02, 1.7461e-01, 1.0000e+00],\n",
            "        [1.1148e-01, 1.0810e-01, 1.3004e-01, 8.0550e-02, 1.9413e-01, 9.0304e-02,\n",
            "         1.1093e-01, 5.1291e-02, 2.5359e-01, 1.0000e+00],\n",
            "        [3.9671e-02, 4.1988e-02, 2.3383e-02, 2.9136e-02, 2.5443e-02, 1.9462e-02,\n",
            "         1.8868e-02, 5.4289e-02, 3.2911e-02, 4.2782e-02],\n",
            "        [4.7976e-02, 5.1785e-02, 7.2302e-02, 5.0472e-02, 7.6999e-02, 5.1937e-02,\n",
            "         8.2234e-02, 4.0892e-02, 1.6750e-01, 1.0000e+00],\n",
            "        [3.7986e-02, 4.0514e-02, 5.5259e-02, 4.2900e-02, 6.5209e-02, 4.1957e-02,\n",
            "         5.1355e-02, 3.9214e-02, 1.2957e-01, 1.0000e+00],\n",
            "        [1.5590e-01, 1.2107e-01, 1.7897e-01, 2.1905e-01, 2.0948e-01, 2.1290e-01,\n",
            "         2.1637e-01, 2.1408e-01, 1.8185e-01, 2.6800e-01],\n",
            "        [6.8453e-03, 7.0798e-03, 5.9325e-03, 2.5734e-03, 3.7553e-03, 3.3658e-03,\n",
            "         7.2438e-03, 6.5263e-03, 8.2274e-03, 4.3298e-02],\n",
            "        [1.0707e-03, 1.9817e-03, 9.6161e-04, 1.5105e-04, 3.3977e-04, 1.9331e-04,\n",
            "         2.6157e-04, 7.6838e-04, 6.4683e-04, 7.2913e-03]])\n",
            "output: 0.15770769119262695\n",
            "SAVING ATTENTION MATRICES...\n",
            "input_mat.shape: torch.Size([30, 300])\n",
            "num_samples: 10\n",
            "input_mat: tensor([[5.9965e-02, 6.1168e-02, 6.2382e-02, 7.2666e-02, 6.7566e-02, 4.2118e-02,\n",
            "         4.8366e-02, 1.5738e-01, 5.5167e-02, 5.9952e-02],\n",
            "        [6.9842e-03, 1.3096e-02, 1.7552e-02, 6.4861e-03, 5.3854e-03, 1.0103e-02,\n",
            "         1.0065e-02, 1.2619e-02, 9.7419e-03, 5.5009e-03],\n",
            "        [1.2190e-02, 9.7859e-03, 1.5935e-02, 2.1342e-02, 1.4326e-02, 9.8564e-03,\n",
            "         1.1969e-02, 7.0581e-02, 1.2923e-02, 7.8193e-03],\n",
            "        [2.0558e-01, 2.5361e-01, 2.3583e-01, 1.7223e-01, 1.7953e-01, 2.4137e-01,\n",
            "         2.2633e-01, 2.8287e-01, 2.3743e-01, 2.3878e-01],\n",
            "        [7.3151e-03, 1.1283e-02, 1.8401e-02, 6.1520e-03, 7.6013e-03, 1.1747e-02,\n",
            "         1.4157e-02, 2.0579e-02, 1.3512e-02, 7.0899e-03],\n",
            "        [3.8249e-03, 4.3676e-03, 7.2356e-03, 2.1920e-03, 2.9049e-03, 4.6245e-03,\n",
            "         5.5481e-03, 1.0685e-02, 3.4306e-03, 1.9949e-03],\n",
            "        [2.7378e-03, 2.6605e-03, 4.9444e-03, 2.3368e-03, 1.9824e-03, 4.4055e-03,\n",
            "         4.6579e-03, 9.8601e-03, 2.1321e-03, 5.5969e-04],\n",
            "        [2.1549e-02, 2.3603e-02, 2.4018e-02, 2.7262e-02, 2.5826e-02, 2.2438e-02,\n",
            "         2.6601e-02, 9.3854e-02, 3.0206e-02, 2.3048e-02],\n",
            "        [1.0810e-02, 1.3190e-02, 2.1160e-02, 1.0514e-02, 9.1153e-03, 2.3588e-02,\n",
            "         2.5834e-02, 1.0072e-01, 3.1591e-02, 6.3718e-03],\n",
            "        [5.2918e-03, 6.4834e-03, 8.0071e-03, 4.0574e-03, 4.6424e-03, 9.5415e-03,\n",
            "         1.0677e-02, 4.9172e-02, 8.7406e-03, 3.5711e-03],\n",
            "        [3.1368e-03, 2.5568e-03, 5.0860e-03, 3.5044e-03, 2.2868e-03, 7.0084e-03,\n",
            "         5.9836e-03, 2.6448e-02, 2.4227e-03, 5.1194e-04],\n",
            "        [7.3586e-04, 8.4315e-04, 1.3016e-03, 4.9302e-04, 5.2798e-04, 1.5448e-03,\n",
            "         1.7283e-03, 6.4375e-03, 6.2754e-04, 1.9506e-04],\n",
            "        [4.2419e-04, 3.9769e-04, 4.8226e-04, 7.8835e-04, 3.3632e-04, 1.3765e-03,\n",
            "         1.3985e-03, 1.7263e-02, 2.9346e-03, 7.1957e-04],\n",
            "        [4.4073e-03, 6.1051e-03, 5.8007e-03, 5.7229e-03, 4.5204e-03, 9.8817e-03,\n",
            "         1.2724e-02, 9.1215e-02, 9.3269e-03, 2.1304e-03],\n",
            "        [2.9748e-05, 3.1436e-05, 6.7530e-05, 6.2086e-05, 2.0754e-05, 1.9462e-04,\n",
            "         2.2703e-04, 8.3245e-03, 3.8754e-04, 1.1227e-05],\n",
            "        [1.2203e-01, 1.2890e-01, 1.6418e-01, 8.9405e-02, 9.9942e-02, 1.4920e-01,\n",
            "         1.6489e-01, 1.0760e-01, 8.9281e-02, 1.1856e-01],\n",
            "        [5.4632e-02, 6.0026e-02, 6.9396e-02, 3.8719e-02, 5.2575e-02, 6.2807e-02,\n",
            "         6.5071e-02, 2.6509e-02, 3.5373e-02, 9.7756e-02],\n",
            "        [2.8513e-02, 2.0685e-02, 2.1797e-02, 1.8746e-02, 2.2985e-02, 2.1549e-02,\n",
            "         1.6055e-02, 7.6154e-03, 1.5073e-02, 6.0239e-02],\n",
            "        [5.9113e-03, 4.5359e-03, 5.3036e-03, 5.6515e-03, 4.3716e-03, 6.3148e-03,\n",
            "         5.0018e-03, 5.1603e-03, 3.7818e-03, 3.8912e-03],\n",
            "        [3.0741e-03, 1.4269e-03, 1.5077e-03, 2.1393e-03, 2.4477e-03, 1.6533e-03,\n",
            "         1.5345e-03, 3.8418e-04, 1.1874e-03, 8.3045e-03],\n",
            "        [1.3168e-02, 2.2539e-02, 2.2022e-02, 2.0152e-03, 9.0916e-03, 6.3612e-03,\n",
            "         4.1648e-03, 8.5245e-05, 2.6476e-03, 1.3094e-02],\n",
            "        [6.0421e-02, 6.4166e-02, 6.0133e-02, 6.9040e-02, 5.5638e-02, 4.7176e-02,\n",
            "         4.2143e-02, 1.6741e-01, 5.7783e-02, 8.7179e-02],\n",
            "        [3.7387e-02, 3.2983e-02, 3.0298e-02, 3.0379e-02, 3.1270e-02, 2.8943e-02,\n",
            "         2.1808e-02, 1.6734e-02, 2.6380e-02, 5.2682e-02],\n",
            "        [2.3034e-02, 1.9367e-02, 1.9815e-02, 1.5285e-02, 2.0580e-02, 1.9015e-02,\n",
            "         2.0549e-02, 6.4630e-03, 1.1911e-02, 3.7503e-02],\n",
            "        [4.2241e-02, 4.1661e-02, 4.8421e-02, 3.0979e-02, 4.1605e-02, 3.1841e-02,\n",
            "         3.4132e-02, 9.8452e-03, 2.6772e-02, 4.6564e-02],\n",
            "        [2.5360e-02, 2.9611e-02, 4.0874e-02, 9.2701e-03, 2.0026e-02, 1.9402e-02,\n",
            "         1.7332e-02, 1.0665e-03, 1.0133e-02, 2.3433e-02],\n",
            "        [3.6041e-02, 4.2279e-02, 3.7756e-02, 1.7079e-02, 2.8256e-02, 2.1205e-02,\n",
            "         1.4929e-02, 8.6714e-03, 1.8437e-02, 5.2458e-02],\n",
            "        [2.4328e-02, 2.5559e-02, 2.9347e-02, 1.2212e-02, 2.0445e-02, 1.8909e-02,\n",
            "         1.7236e-02, 2.7275e-03, 1.0465e-02, 2.4030e-02],\n",
            "        [1.5170e-02, 1.6604e-02, 1.6230e-02, 6.0613e-03, 1.2082e-02, 8.0620e-03,\n",
            "         7.2940e-03, 4.7761e-04, 4.2141e-03, 1.5835e-02],\n",
            "        [2.8530e-02, 3.9066e-02, 4.5432e-02, 7.7704e-03, 2.0873e-02, 1.7271e-02,\n",
            "         1.4720e-02, 4.5221e-04, 7.4495e-03, 2.5110e-02]])\n",
            "output: tensor([0.0687, 0.0098, 0.0187, 0.2274, 0.0118, 0.0047, 0.0036, 0.0318, 0.0253,\n",
            "        0.0110, 0.0059, 0.0014, 0.0026, 0.0152, 0.0009, 0.1234, 0.0563, 0.0233,\n",
            "        0.0050, 0.0024, 0.0095, 0.0711, 0.0309, 0.0194, 0.0354, 0.0197, 0.0277,\n",
            "        0.0185, 0.0102, 0.0207])\n",
            "SAVING ATTENTION MATRICES...\n",
            "./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/plots/30/test_self_attn_matrix/self_attn_matrix_avg.png\n",
            "LOADING ATTENTION MATRIX...\n",
            "[tensor([[0.0194, 0.0261, 0.0157, 0.0269, 0.0393, 0.0809, 0.0208, 0.0158, 0.0241,\n",
            "         0.0244, 0.0149, 0.0142, 0.0274, 0.0122, 0.0202, 0.0540, 0.0186, 0.0478,\n",
            "         0.0625, 0.0176, 0.0242, 0.0192, 0.0280, 0.0345, 0.0815, 0.0204, 0.0162,\n",
            "         0.0340, 0.1275, 0.1069],\n",
            "        [0.1043, 0.1097, 0.0828, 0.1010, 0.1506, 0.1326, 0.1376, 0.0753, 0.1155,\n",
            "         0.1093, 0.0971, 0.1134, 0.1416, 0.0747, 0.1168, 0.1512, 0.0932, 0.1552,\n",
            "         0.1659, 0.0902, 0.1053, 0.0933, 0.0975, 0.1467, 0.1356, 0.0862, 0.0986,\n",
            "         0.0916, 0.2361, 0.2331],\n",
            "        [0.1124, 0.1216, 0.1323, 0.1240, 0.1581, 0.2359, 0.1267, 0.1277, 0.1357,\n",
            "         0.1161, 0.1232, 0.1487, 0.2061, 0.1404, 0.1220, 0.1507, 0.1120, 0.1253,\n",
            "         0.1892, 0.1159, 0.1347, 0.1080, 0.1446, 0.1376, 0.1484, 0.1450, 0.1243,\n",
            "         0.1264, 0.1999, 0.1832],\n",
            "        [0.0773, 0.0923, 0.0874, 0.1034, 0.1412, 0.1719, 0.1128, 0.0785, 0.1075,\n",
            "         0.1041, 0.1033, 0.1068, 0.1675, 0.0785, 0.1029, 0.1566, 0.0856, 0.1157,\n",
            "         0.1589, 0.0963, 0.1058, 0.0810, 0.1249, 0.1145, 0.1302, 0.1016, 0.1008,\n",
            "         0.0941, 0.1708, 0.1908],\n",
            "        [0.1811, 0.2051, 0.2171, 0.1850, 0.2930, 0.2615, 0.2531, 0.2062, 0.2333,\n",
            "         0.2867, 0.2403, 0.3857, 0.3469, 0.3079, 0.2057, 0.1658, 0.1804, 0.1700,\n",
            "         0.2145, 0.1951, 0.1740, 0.2019, 0.1761, 0.1859, 0.1436, 0.2088, 0.2009,\n",
            "         0.1466, 0.1754, 0.1424],\n",
            "        [0.1560, 0.1294, 0.2071, 0.1443, 0.1944, 0.2395, 0.1972, 0.1933, 0.2008,\n",
            "         0.1833, 0.1904, 0.3048, 0.2747, 0.2499, 0.1768, 0.1201, 0.1457, 0.1183,\n",
            "         0.1774, 0.1498, 0.1479, 0.1489, 0.1606, 0.1487, 0.1196, 0.1650, 0.1732,\n",
            "         0.1367, 0.1818, 0.1168],\n",
            "        [0.0798, 0.0918, 0.0705, 0.0845, 0.1283, 0.1213, 0.0980, 0.0724, 0.0991,\n",
            "         0.1003, 0.0779, 0.0914, 0.1309, 0.0738, 0.0932, 0.1300, 0.0792, 0.1190,\n",
            "         0.1137, 0.0787, 0.1088, 0.0673, 0.0863, 0.1076, 0.0966, 0.0931, 0.0885,\n",
            "         0.0850, 0.1512, 0.2167],\n",
            "        [0.0229, 0.0355, 0.0314, 0.0360, 0.0758, 0.1121, 0.0559, 0.0284, 0.0494,\n",
            "         0.0492, 0.0373, 0.0561, 0.0838, 0.0277, 0.0435, 0.0676, 0.0309, 0.0502,\n",
            "         0.0825, 0.0314, 0.0361, 0.0284, 0.0382, 0.0431, 0.0435, 0.0380, 0.0356,\n",
            "         0.0257, 0.0784, 0.0903],\n",
            "        [0.1443, 0.1767, 0.1610, 0.1608, 0.2413, 0.2542, 0.1936, 0.1592, 0.1918,\n",
            "         0.1940, 0.1822, 0.2166, 0.2722, 0.1868, 0.1826, 0.1999, 0.1532, 0.1898,\n",
            "         0.2383, 0.1568, 0.1863, 0.1515, 0.1803, 0.1865, 0.1716, 0.1928, 0.1722,\n",
            "         0.1513, 0.2369, 0.2451],\n",
            "        [0.1049, 0.1759, 0.1356, 0.1498, 0.2768, 0.2881, 0.1792, 0.1355, 0.1780,\n",
            "         0.2592, 0.1653, 0.2631, 0.3190, 0.2082, 0.1485, 0.1902, 0.1383, 0.1519,\n",
            "         0.2455, 0.1520, 0.1316, 0.1431, 0.1423, 0.1389, 0.1197, 0.1940, 0.1447,\n",
            "         0.0876, 0.1379, 0.1354],\n",
            "        [0.2281, 0.2536, 0.2743, 0.2729, 0.3210, 0.3171, 0.3285, 0.2470, 0.2758,\n",
            "         0.3505, 0.3944, 0.3469, 0.4538, 0.3163, 0.2650, 0.3156, 0.2521, 0.2372,\n",
            "         0.2897, 0.3032, 0.3048, 0.2338, 0.3511, 0.2488, 0.2672, 0.3138, 0.3148,\n",
            "         0.2622, 0.2661, 0.2960],\n",
            "        [0.1369, 0.1624, 0.1191, 0.1032, 0.1957, 0.1670, 0.1367, 0.1426, 0.1561,\n",
            "         0.1613, 0.1143, 0.1821, 0.2044, 0.1894, 0.1394, 0.1252, 0.1265, 0.1569,\n",
            "         0.1360, 0.1060, 0.1596, 0.1073, 0.1022, 0.1514, 0.0976, 0.1820, 0.1385,\n",
            "         0.1173, 0.1741, 0.2387],\n",
            "        [0.0903, 0.0757, 0.0839, 0.0406, 0.1738, 0.0650, 0.1263, 0.0897, 0.0933,\n",
            "         0.1540, 0.0847, 0.2996, 0.1812, 0.1701, 0.0760, 0.0313, 0.0649, 0.0479,\n",
            "         0.0642, 0.0462, 0.0457, 0.0782, 0.0420, 0.0793, 0.0326, 0.0648, 0.0821,\n",
            "         0.0442, 0.0644, 0.0595],\n",
            "        [0.0672, 0.1204, 0.0793, 0.0695, 0.2083, 0.1559, 0.1237, 0.0800, 0.1213,\n",
            "         0.1597, 0.0907, 0.1846, 0.2064, 0.1165, 0.0962, 0.1070, 0.0850, 0.1080,\n",
            "         0.1491, 0.0740, 0.0829, 0.0723, 0.0716, 0.1078, 0.0673, 0.1209, 0.1034,\n",
            "         0.0499, 0.1065, 0.1751],\n",
            "        [0.0929, 0.1180, 0.1006, 0.1139, 0.1841, 0.1406, 0.1545, 0.0917, 0.1475,\n",
            "         0.1438, 0.1367, 0.1609, 0.2041, 0.0955, 0.1372, 0.1784, 0.1055, 0.1502,\n",
            "         0.1256, 0.1249, 0.1857, 0.0765, 0.1427, 0.1276, 0.1272, 0.1383, 0.1452,\n",
            "         0.1490, 0.1488, 0.2618],\n",
            "        [0.1717, 0.1719, 0.1579, 0.1137, 0.2261, 0.1579, 0.2477, 0.1446, 0.1784,\n",
            "         0.1889, 0.1833, 0.2953, 0.2629, 0.2019, 0.1703, 0.1330, 0.1426, 0.1582,\n",
            "         0.1881, 0.1247, 0.1256, 0.1455, 0.1263, 0.2132, 0.1083, 0.1286, 0.1906,\n",
            "         0.0994, 0.1916, 0.2525],\n",
            "        [0.0576, 0.0667, 0.0520, 0.0671, 0.0953, 0.1511, 0.0665, 0.0507, 0.0690,\n",
            "         0.0639, 0.0527, 0.0623, 0.0973, 0.0507, 0.0627, 0.1039, 0.0551, 0.0893,\n",
            "         0.1268, 0.0552, 0.0659, 0.0576, 0.0677, 0.0796, 0.1065, 0.0643, 0.0537,\n",
            "         0.0680, 0.1368, 0.1234],\n",
            "        [0.0964, 0.0877, 0.0709, 0.1132, 0.1047, 0.1225, 0.1015, 0.0592, 0.0886,\n",
            "         0.0768, 0.0905, 0.0638, 0.1033, 0.0501, 0.0982, 0.1808, 0.0790, 0.1478,\n",
            "         0.1525, 0.0974, 0.1300, 0.0931, 0.1292, 0.1382, 0.2281, 0.0811, 0.0882,\n",
            "         0.1353, 0.2815, 0.2281],\n",
            "        [0.0749, 0.0497, 0.0715, 0.0870, 0.0533, 0.1091, 0.0571, 0.0660, 0.0648,\n",
            "         0.0425, 0.0695, 0.0458, 0.0735, 0.0624, 0.0717, 0.0941, 0.0565, 0.0691,\n",
            "         0.0878, 0.0770, 0.1532, 0.0563, 0.1125, 0.0708, 0.1576, 0.0852, 0.0679,\n",
            "         0.1532, 0.2841, 0.1060],\n",
            "        [0.1422, 0.1646, 0.1593, 0.1760, 0.2153, 0.3009, 0.1664, 0.1540, 0.1659,\n",
            "         0.1816, 0.1747, 0.1939, 0.2773, 0.1759, 0.1530, 0.2165, 0.1514, 0.1669,\n",
            "         0.2845, 0.1658, 0.1696, 0.1553, 0.2029, 0.1763, 0.2045, 0.2023, 0.1600,\n",
            "         0.1648, 0.2206, 0.2031],\n",
            "        [0.1133, 0.1644, 0.1429, 0.1534, 0.2393, 0.2018, 0.2258, 0.1213, 0.1651,\n",
            "         0.2046, 0.2263, 0.2080, 0.3001, 0.1504, 0.1669, 0.2163, 0.1356, 0.1619,\n",
            "         0.2748, 0.1602, 0.1590, 0.1347, 0.2201, 0.1717, 0.2001, 0.1630, 0.1839,\n",
            "         0.1278, 0.2480, 0.2304],\n",
            "        [0.0464, 0.0493, 0.0376, 0.0581, 0.0632, 0.0618, 0.0551, 0.0335, 0.0504,\n",
            "         0.0437, 0.0489, 0.0347, 0.0666, 0.0276, 0.0541, 0.1108, 0.0403, 0.0757,\n",
            "         0.0585, 0.0544, 0.1095, 0.0363, 0.0728, 0.0708, 0.0930, 0.0569, 0.0571,\n",
            "         0.0895, 0.1195, 0.2103],\n",
            "        [0.1928, 0.1844, 0.2273, 0.1833, 0.2395, 0.2066, 0.2840, 0.1909, 0.2279,\n",
            "         0.2493, 0.3088, 0.3039, 0.3437, 0.2357, 0.2119, 0.2023, 0.1898, 0.1725,\n",
            "         0.1908, 0.2216, 0.2353, 0.1597, 0.2578, 0.2068, 0.1917, 0.2160, 0.2751,\n",
            "         0.2173, 0.2157, 0.2495],\n",
            "        [0.0485, 0.0391, 0.0279, 0.0378, 0.0362, 0.0317, 0.0382, 0.0279, 0.0354,\n",
            "         0.0242, 0.0286, 0.0213, 0.0324, 0.0228, 0.0395, 0.0569, 0.0305, 0.0642,\n",
            "         0.0451, 0.0314, 0.0742, 0.0278, 0.0414, 0.0634, 0.0836, 0.0387, 0.0358,\n",
            "         0.0672, 0.1937, 0.2035],\n",
            "        [0.0940, 0.0736, 0.0828, 0.1044, 0.0666, 0.0834, 0.0760, 0.0690, 0.0684,\n",
            "         0.0540, 0.0996, 0.0485, 0.1003, 0.0630, 0.0755, 0.1280, 0.0677, 0.0863,\n",
            "         0.0851, 0.0980, 0.1767, 0.0657, 0.1613, 0.1022, 0.2850, 0.0996, 0.1097,\n",
            "         0.2233, 0.3389, 0.1998],\n",
            "        [0.0357, 0.0516, 0.0594, 0.0729, 0.1448, 0.1905, 0.1089, 0.0500, 0.0967,\n",
            "         0.1105, 0.0804, 0.1419, 0.1701, 0.0563, 0.0818, 0.1085, 0.0569, 0.0717,\n",
            "         0.1809, 0.0666, 0.0559, 0.0466, 0.0836, 0.0581, 0.0871, 0.0672, 0.0663,\n",
            "         0.0432, 0.1397, 0.0737],\n",
            "        [0.2340, 0.2522, 0.1915, 0.1908, 0.2604, 0.2517, 0.2421, 0.1791, 0.2074,\n",
            "         0.2111, 0.2050, 0.2520, 0.2676, 0.2137, 0.2000, 0.2304, 0.1935, 0.2520,\n",
            "         0.3070, 0.1854, 0.1973, 0.2125, 0.1922, 0.2833, 0.2419, 0.2030, 0.2123,\n",
            "         0.1882, 0.3185, 0.3091],\n",
            "        [0.0390, 0.0436, 0.0453, 0.0829, 0.0781, 0.1472, 0.0560, 0.0326, 0.0515,\n",
            "         0.0499, 0.0635, 0.0336, 0.1133, 0.0234, 0.0527, 0.1612, 0.0423, 0.0745,\n",
            "         0.1277, 0.0683, 0.0706, 0.0464, 0.1193, 0.0635, 0.2019, 0.0581, 0.0526,\n",
            "         0.0942, 0.2066, 0.1313],\n",
            "        [0.0116, 0.0151, 0.0089, 0.0180, 0.0187, 0.0543, 0.0067, 0.0089, 0.0138,\n",
            "         0.0083, 0.0095, 0.0035, 0.0156, 0.0079, 0.0123, 0.0400, 0.0101, 0.0294,\n",
            "         0.0316, 0.0166, 0.0732, 0.0075, 0.0237, 0.0207, 0.0883, 0.0264, 0.0149,\n",
            "         0.0565, 0.1937, 0.0819],\n",
            "        [0.0889, 0.0899, 0.0821, 0.0702, 0.1243, 0.1256, 0.1242, 0.0757, 0.1020,\n",
            "         0.0814, 0.0795, 0.1344, 0.1369, 0.0842, 0.0944, 0.0922, 0.0746, 0.1063,\n",
            "         0.1711, 0.0605, 0.0623, 0.0809, 0.0692, 0.1309, 0.0889, 0.0612, 0.0802,\n",
            "         0.0547, 0.2207, 0.1699]]), tensor([[0.1577, 0.0898, 0.1341, 0.0999, 0.0987, 0.1520, 0.1032, 0.1368, 0.0996,\n",
            "         0.1008, 0.0966, 0.1226, 0.1491, 0.1358, 0.1035, 0.1034, 0.0903, 0.0978,\n",
            "         0.1455, 0.0934, 0.0921, 0.1061, 0.1051, 0.1528, 0.1426, 0.1146, 0.0875,\n",
            "         0.0957, 0.1053, 0.1112],\n",
            "        [0.1272, 0.0210, 0.0653, 0.0221, 0.0220, 0.0263, 0.0249, 0.0797, 0.0237,\n",
            "         0.0308, 0.0237, 0.0233, 0.0493, 0.0665, 0.0194, 0.0193, 0.0190, 0.0176,\n",
            "         0.0282, 0.0200, 0.0198, 0.0155, 0.0200, 0.0200, 0.0199, 0.0333, 0.0186,\n",
            "         0.0224, 0.0290, 0.0441],\n",
            "        [0.0550, 0.0474, 0.0577, 0.0587, 0.0671, 0.0842, 0.0520, 0.0449, 0.0597,\n",
            "         0.0549, 0.0543, 0.0920, 0.0785, 0.0602, 0.0682, 0.0697, 0.0535, 0.0752,\n",
            "         0.1120, 0.0594, 0.0521, 0.0999, 0.0700, 0.1640, 0.1621, 0.0435, 0.0562,\n",
            "         0.0498, 0.0491, 0.0400],\n",
            "        [0.1521, 0.1350, 0.1456, 0.1677, 0.1135, 0.1885, 0.1219, 0.1416, 0.1402,\n",
            "         0.1101, 0.1301, 0.1676, 0.1524, 0.1337, 0.1546, 0.1761, 0.1367, 0.1960,\n",
            "         0.3292, 0.1517, 0.1727, 0.1806, 0.1832, 0.2779, 0.3188, 0.1744, 0.1369,\n",
            "         0.1909, 0.1734, 0.1323],\n",
            "        [0.1312, 0.0105, 0.0497, 0.0052, 0.0146, 0.0106, 0.0174, 0.0467, 0.0127,\n",
            "         0.0222, 0.0134, 0.0215, 0.0381, 0.0598, 0.0096, 0.0035, 0.0074, 0.0034,\n",
            "         0.0029, 0.0070, 0.0051, 0.0044, 0.0047, 0.0035, 0.0022, 0.0177, 0.0094,\n",
            "         0.0058, 0.0071, 0.0285],\n",
            "        [0.0323, 0.0386, 0.0391, 0.0731, 0.0359, 0.0430, 0.0683, 0.0553, 0.0465,\n",
            "         0.0525, 0.0592, 0.0937, 0.0748, 0.0528, 0.0778, 0.0756, 0.0614, 0.0756,\n",
            "         0.1346, 0.0505, 0.0582, 0.1660, 0.0776, 0.1660, 0.1319, 0.0465, 0.0521,\n",
            "         0.0615, 0.0710, 0.0410],\n",
            "        [0.1287, 0.0476, 0.0706, 0.0408, 0.0567, 0.0943, 0.0492, 0.0532, 0.0528,\n",
            "         0.0604, 0.0542, 0.0734, 0.0672, 0.0742, 0.0495, 0.0414, 0.0413, 0.0571,\n",
            "         0.0707, 0.0493, 0.0437, 0.0549, 0.0464, 0.0982, 0.0874, 0.0477, 0.0511,\n",
            "         0.0437, 0.0386, 0.0426],\n",
            "        [0.0377, 0.0391, 0.0317, 0.0324, 0.0450, 0.0864, 0.0209, 0.0166, 0.0360,\n",
            "         0.0320, 0.0305, 0.0454, 0.0423, 0.0253, 0.0334, 0.0447, 0.0295, 0.0652,\n",
            "         0.0968, 0.0404, 0.0397, 0.0477, 0.0441, 0.1486, 0.1577, 0.0351, 0.0374,\n",
            "         0.0379, 0.0259, 0.0257],\n",
            "        [0.1222, 0.0375, 0.0849, 0.0455, 0.0415, 0.0566, 0.0471, 0.0854, 0.0450,\n",
            "         0.0528, 0.0481, 0.0618, 0.0807, 0.0845, 0.0468, 0.0422, 0.0384, 0.0401,\n",
            "         0.0566, 0.0408, 0.0419, 0.0494, 0.0469, 0.0529, 0.0554, 0.0473, 0.0400,\n",
            "         0.0440, 0.0488, 0.0513],\n",
            "        [0.1151, 0.0086, 0.0363, 0.0028, 0.0135, 0.0145, 0.0115, 0.0274, 0.0089,\n",
            "         0.0191, 0.0095, 0.0173, 0.0364, 0.0414, 0.0064, 0.0021, 0.0044, 0.0023,\n",
            "         0.0014, 0.0045, 0.0034, 0.0026, 0.0026, 0.0018, 0.0016, 0.0134, 0.0065,\n",
            "         0.0038, 0.0038, 0.0214],\n",
            "        [0.1283, 0.0488, 0.0842, 0.0496, 0.0634, 0.1027, 0.0550, 0.0601, 0.0614,\n",
            "         0.0639, 0.0561, 0.0841, 0.0864, 0.0842, 0.0587, 0.0488, 0.0488, 0.0510,\n",
            "         0.0577, 0.0560, 0.0468, 0.0572, 0.0539, 0.0679, 0.0740, 0.0491, 0.0543,\n",
            "         0.0469, 0.0489, 0.0493],\n",
            "        [0.1690, 0.0567, 0.1217, 0.0659, 0.0750, 0.1620, 0.0987, 0.1322, 0.0798,\n",
            "         0.0799, 0.0809, 0.1254, 0.1267, 0.1557, 0.0839, 0.0563, 0.0637, 0.0612,\n",
            "         0.0800, 0.0684, 0.0510, 0.0758, 0.0637, 0.0964, 0.0815, 0.0676, 0.0654,\n",
            "         0.0508, 0.0692, 0.0598],\n",
            "        [0.1658, 0.0764, 0.1167, 0.0630, 0.0833, 0.1095, 0.0752, 0.0976, 0.0814,\n",
            "         0.0868, 0.0793, 0.0985, 0.1147, 0.1114, 0.0714, 0.0582, 0.0638, 0.0600,\n",
            "         0.0652, 0.0709, 0.0656, 0.0588, 0.0676, 0.0687, 0.0663, 0.0866, 0.0760,\n",
            "         0.0671, 0.0667, 0.0866],\n",
            "        [0.1022, 0.0164, 0.0477, 0.0081, 0.0263, 0.0697, 0.0209, 0.0327, 0.0183,\n",
            "         0.0272, 0.0183, 0.0276, 0.0514, 0.0492, 0.0129, 0.0069, 0.0098, 0.0084,\n",
            "         0.0071, 0.0120, 0.0089, 0.0061, 0.0078, 0.0081, 0.0084, 0.0222, 0.0129,\n",
            "         0.0091, 0.0100, 0.0281],\n",
            "        [0.1549, 0.0349, 0.0820, 0.0321, 0.0419, 0.0608, 0.0487, 0.0770, 0.0435,\n",
            "         0.0479, 0.0429, 0.0503, 0.0606, 0.0772, 0.0374, 0.0275, 0.0325, 0.0277,\n",
            "         0.0319, 0.0361, 0.0299, 0.0295, 0.0323, 0.0399, 0.0415, 0.0455, 0.0351,\n",
            "         0.0319, 0.0396, 0.0487],\n",
            "        [0.1511, 0.0390, 0.0851, 0.0422, 0.0326, 0.0428, 0.0412, 0.1241, 0.0374,\n",
            "         0.0463, 0.0427, 0.0411, 0.0616, 0.0835, 0.0378, 0.0402, 0.0316, 0.0388,\n",
            "         0.0777, 0.0344, 0.0448, 0.0418, 0.0419, 0.0427, 0.0657, 0.0621, 0.0305,\n",
            "         0.0537, 0.0510, 0.0691],\n",
            "        [0.1475, 0.0303, 0.0712, 0.0315, 0.0286, 0.0407, 0.0328, 0.0795, 0.0338,\n",
            "         0.0369, 0.0325, 0.0312, 0.0464, 0.0586, 0.0273, 0.0286, 0.0270, 0.0301,\n",
            "         0.0441, 0.0302, 0.0312, 0.0228, 0.0305, 0.0356, 0.0384, 0.0440, 0.0294,\n",
            "         0.0350, 0.0417, 0.0465],\n",
            "        [0.1379, 0.0101, 0.0547, 0.0144, 0.0115, 0.0141, 0.0128, 0.0832, 0.0129,\n",
            "         0.0164, 0.0134, 0.0098, 0.0186, 0.0445, 0.0094, 0.0110, 0.0088, 0.0094,\n",
            "         0.0232, 0.0106, 0.0107, 0.0079, 0.0113, 0.0108, 0.0118, 0.0230, 0.0078,\n",
            "         0.0133, 0.0219, 0.0287],\n",
            "        [0.0006, 0.0021, 0.0008, 0.0069, 0.0007, 0.0028, 0.0017, 0.0006, 0.0019,\n",
            "         0.0009, 0.0020, 0.0135, 0.0028, 0.0010, 0.0049, 0.0093, 0.0031, 0.0103,\n",
            "         0.0511, 0.0030, 0.0056, 0.0536, 0.0083, 0.1031, 0.0749, 0.0032, 0.0023,\n",
            "         0.0083, 0.0074, 0.0021],\n",
            "        [0.1600, 0.0618, 0.1260, 0.0849, 0.0615, 0.0890, 0.0787, 0.1488, 0.0748,\n",
            "         0.0771, 0.0761, 0.0891, 0.1174, 0.1211, 0.0775, 0.0795, 0.0669, 0.0712,\n",
            "         0.1252, 0.0697, 0.0765, 0.0790, 0.0831, 0.0889, 0.0994, 0.0882, 0.0627,\n",
            "         0.0859, 0.0998, 0.0911],\n",
            "        [0.1364, 0.0288, 0.0747, 0.0327, 0.0274, 0.0354, 0.0296, 0.0934, 0.0309,\n",
            "         0.0343, 0.0312, 0.0292, 0.0471, 0.0745, 0.0280, 0.0302, 0.0259, 0.0280,\n",
            "         0.0465, 0.0291, 0.0306, 0.0250, 0.0316, 0.0315, 0.0373, 0.0409, 0.0255,\n",
            "         0.0342, 0.0405, 0.0496],\n",
            "        [0.1349, 0.0700, 0.0854, 0.0602, 0.0700, 0.1680, 0.0530, 0.0701, 0.0675,\n",
            "         0.0600, 0.0577, 0.0609, 0.0697, 0.0627, 0.0544, 0.0606, 0.0517, 0.0732,\n",
            "         0.0997, 0.0687, 0.0580, 0.0573, 0.0596, 0.1176, 0.1046, 0.0865, 0.0546,\n",
            "         0.0609, 0.0671, 0.0756],\n",
            "        [0.2108, 0.1048, 0.1647, 0.1395, 0.1046, 0.1693, 0.1210, 0.1532, 0.1277,\n",
            "         0.1049, 0.1159, 0.1401, 0.1320, 0.1420, 0.1337, 0.1348, 0.1222, 0.1313,\n",
            "         0.1911, 0.1290, 0.1228, 0.1429, 0.1428, 0.1980, 0.1933, 0.1241, 0.1146,\n",
            "         0.1357, 0.1643, 0.1249],\n",
            "        [0.1382, 0.0814, 0.1132, 0.1410, 0.0572, 0.1115, 0.1021, 0.1243, 0.0939,\n",
            "         0.0650, 0.0878, 0.1358, 0.1221, 0.1077, 0.1162, 0.1227, 0.1089, 0.1295,\n",
            "         0.2745, 0.1022, 0.1092, 0.1470, 0.1319, 0.2468, 0.1962, 0.1144, 0.0983,\n",
            "         0.1418, 0.2053, 0.0952],\n",
            "        [0.1384, 0.1014, 0.1410, 0.1922, 0.1065, 0.1495, 0.1398, 0.1723, 0.1260,\n",
            "         0.1090, 0.1251, 0.1451, 0.1505, 0.1395, 0.1644, 0.1819, 0.1443, 0.1661,\n",
            "         0.2852, 0.1394, 0.1416, 0.2181, 0.1733, 0.2418, 0.2321, 0.1285, 0.1129,\n",
            "         0.1546, 0.2287, 0.1284],\n",
            "        [0.1518, 0.0567, 0.1022, 0.0770, 0.0459, 0.0730, 0.0679, 0.1418, 0.0642,\n",
            "         0.0613, 0.0591, 0.0752, 0.0829, 0.0984, 0.0621, 0.0809, 0.0715, 0.0780,\n",
            "         0.1773, 0.0626, 0.0742, 0.0642, 0.0796, 0.1309, 0.1262, 0.0822, 0.0712,\n",
            "         0.0905, 0.1057, 0.0962],\n",
            "        [0.1224, 0.0093, 0.0429, 0.0073, 0.0117, 0.0100, 0.0140, 0.0463, 0.0111,\n",
            "         0.0163, 0.0124, 0.0148, 0.0200, 0.0446, 0.0093, 0.0050, 0.0073, 0.0045,\n",
            "         0.0048, 0.0079, 0.0063, 0.0059, 0.0062, 0.0044, 0.0037, 0.0150, 0.0079,\n",
            "         0.0069, 0.0093, 0.0196],\n",
            "        [0.1504, 0.0548, 0.1092, 0.0756, 0.0536, 0.0865, 0.0602, 0.1204, 0.0616,\n",
            "         0.0606, 0.0587, 0.0655, 0.0799, 0.0967, 0.0625, 0.0785, 0.0587, 0.0743,\n",
            "         0.1447, 0.0619, 0.0677, 0.0715, 0.0750, 0.1208, 0.1438, 0.0728, 0.0521,\n",
            "         0.0773, 0.0919, 0.0844],\n",
            "        [0.0035, 0.0076, 0.0042, 0.0128, 0.0045, 0.0086, 0.0051, 0.0031, 0.0067,\n",
            "         0.0040, 0.0063, 0.0150, 0.0046, 0.0032, 0.0103, 0.0156, 0.0092, 0.0219,\n",
            "         0.0586, 0.0097, 0.0137, 0.0425, 0.0159, 0.1188, 0.1026, 0.0086, 0.0088,\n",
            "         0.0162, 0.0112, 0.0057],\n",
            "        [0.1303, 0.0396, 0.0661, 0.0388, 0.0307, 0.0397, 0.0357, 0.0892, 0.0371,\n",
            "         0.0396, 0.0366, 0.0408, 0.0484, 0.0648, 0.0367, 0.0425, 0.0372, 0.0465,\n",
            "         0.0711, 0.0363, 0.0464, 0.0357, 0.0435, 0.0636, 0.0724, 0.0501, 0.0393,\n",
            "         0.0525, 0.0478, 0.0586]])]\n",
            "torch.Size([2, 30, 30])\n",
            "SAVING ATTENTION MATRIX...\n",
            "./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/plots/30/test_cross_attn_matrix/cross_attn_matrix_avg.png\n",
            "LOADING ATTENTION MATRIX...\n",
            "[tensor([[2.6578e-02, 1.8384e-02, 5.5011e-02, 7.9700e-03, 2.9855e-02, 6.5689e-03,\n",
            "         3.2348e-02, 1.7796e-01, 1.5438e-02, 6.7617e-02, 3.2931e-02, 5.5892e-02,\n",
            "         2.4815e-02, 1.7460e-01, 1.3834e-02, 7.1633e-03, 1.3451e-02, 1.7190e-02,\n",
            "         2.7304e-03, 9.5841e-03, 8.6905e-03, 1.5140e-02, 5.9876e-03, 8.4382e-03,\n",
            "         4.5487e-03, 8.8031e-03, 1.7865e-02, 6.6299e-03, 3.3849e-03, 8.2828e-03],\n",
            "        [5.0915e-03, 1.1592e-02, 5.4130e-03, 1.0164e-02, 8.7840e-03, 1.1736e-02,\n",
            "         3.7807e-03, 3.8436e-03, 6.2075e-03, 5.6614e-03, 3.5610e-03, 3.3069e-03,\n",
            "         4.3319e-03, 3.2977e-03, 6.8687e-03, 1.1320e-01, 1.0857e-02, 1.0884e-01,\n",
            "         9.3501e-02, 9.2744e-03, 5.4933e-02, 6.2926e-03, 8.8417e-03, 2.6687e-02,\n",
            "         3.5132e-02, 1.0896e-02, 1.2258e-02, 1.1817e-02, 1.4031e-01, 1.2741e-02],\n",
            "        [1.8457e-01, 1.4631e-01, 3.0699e-01, 1.2280e-01, 1.6742e-01, 9.4829e-02,\n",
            "         2.9489e-01, 3.7493e-01, 1.3018e-01, 2.3133e-01, 3.3536e-01, 2.0313e-01,\n",
            "         1.5670e-01, 2.8258e-01, 1.4584e-01, 8.6438e-02, 1.2220e-01, 1.1782e-01,\n",
            "         1.5884e-01, 1.0938e-01, 1.2467e-01, 1.9035e-01, 1.1861e-01, 9.7385e-02,\n",
            "         1.0255e-01, 1.1816e-01, 1.4695e-01, 1.2437e-01, 1.8698e-01, 1.2845e-01],\n",
            "        [5.5011e-03, 2.4272e-03, 1.6670e-02, 8.1564e-03, 2.3319e-03, 9.8914e-03,\n",
            "         2.4303e-02, 8.9310e-02, 4.8476e-03, 1.3771e-02, 1.5462e-02, 9.7733e-03,\n",
            "         2.7916e-02, 1.2208e-01, 3.5403e-03, 9.3654e-03, 3.1889e-03, 4.7647e-03,\n",
            "         2.4015e-02, 6.2728e-03, 6.1794e-03, 5.4168e-03, 5.8029e-03, 9.4348e-03,\n",
            "         1.0603e-02, 3.7028e-03, 2.6336e-03, 4.8009e-03, 6.6889e-03, 2.9580e-03],\n",
            "        [3.3933e-02, 4.1465e-02, 5.7871e-02, 2.2267e-02, 5.9769e-02, 1.5829e-02,\n",
            "         1.0005e-01, 1.4579e-01, 2.8694e-02, 7.3226e-02, 9.7767e-02, 7.7883e-02,\n",
            "         4.7438e-02, 1.2581e-01, 2.8144e-02, 1.0115e-01, 2.4743e-02, 4.9678e-02,\n",
            "         7.0374e-03, 2.5316e-02, 3.8664e-02, 3.5035e-02, 2.0805e-02, 1.9132e-02,\n",
            "         2.4533e-02, 1.9842e-02, 2.9486e-02, 2.1022e-02, 8.9148e-03, 1.9144e-02],\n",
            "        [3.4852e-02, 1.8122e-02, 2.0438e-02, 1.1783e-02, 4.4334e-02, 1.0304e-02,\n",
            "         3.9707e-02, 4.7374e-02, 1.7539e-02, 1.1075e-01, 4.6059e-02, 5.6118e-02,\n",
            "         5.1279e-02, 2.7054e-01, 1.8993e-02, 1.8083e-02, 1.0635e-02, 5.6716e-03,\n",
            "         1.7338e-03, 1.2897e-02, 1.0561e-02, 2.2095e-02, 1.3703e-02, 5.9923e-03,\n",
            "         8.1717e-03, 1.0104e-02, 1.1785e-02, 9.3795e-03, 3.1850e-03, 9.2932e-03],\n",
            "        [1.1440e-01, 5.3842e-02, 1.5796e-01, 5.1600e-02, 6.9510e-02, 4.8693e-02,\n",
            "         1.5990e-01, 2.7791e-01, 5.8689e-02, 1.3877e-01, 1.8063e-01, 1.1544e-01,\n",
            "         8.2089e-02, 2.3406e-01, 6.8077e-02, 3.5134e-02, 4.7248e-02, 2.9177e-02,\n",
            "         3.6871e-02, 4.9090e-02, 4.3801e-02, 8.1497e-02, 6.1687e-02, 3.9811e-02,\n",
            "         4.6803e-02, 5.7821e-02, 4.9114e-02, 5.6577e-02, 6.2885e-02, 5.7420e-02],\n",
            "        [5.6497e-03, 3.1514e-03, 1.4865e-02, 7.7580e-03, 3.3857e-03, 8.6791e-03,\n",
            "         2.4237e-02, 1.1808e-01, 6.2164e-03, 1.3128e-02, 1.4964e-02, 1.7568e-02,\n",
            "         2.9629e-02, 6.4401e-02, 5.2829e-03, 8.0315e-03, 3.8743e-03, 4.7375e-03,\n",
            "         1.6060e-02, 6.3247e-03, 6.0223e-03, 7.4453e-03, 6.4457e-03, 7.7879e-03,\n",
            "         9.0613e-03, 4.2904e-03, 3.2572e-03, 5.3771e-03, 6.9681e-03, 3.7747e-03],\n",
            "        [3.4056e-02, 1.9185e-02, 4.3836e-02, 1.2879e-02, 4.4779e-02, 1.3511e-02,\n",
            "         7.2646e-02, 1.3784e-01, 1.8386e-02, 7.1440e-02, 5.8821e-02, 5.3145e-02,\n",
            "         3.6314e-02, 1.1392e-01, 1.9101e-02, 1.3167e-02, 1.3653e-02, 1.0121e-02,\n",
            "         5.4753e-03, 1.3931e-02, 1.1266e-02, 1.9486e-02, 1.3490e-02, 1.0077e-02,\n",
            "         1.0235e-02, 1.2948e-02, 1.4968e-02, 1.1682e-02, 1.0485e-02, 1.2232e-02],\n",
            "        [7.4964e-03, 3.7069e-03, 5.2320e-03, 1.6996e-03, 1.3830e-02, 1.7223e-03,\n",
            "         9.6812e-03, 1.3857e-02, 4.6565e-03, 5.4087e-02, 1.1850e-02, 3.4097e-02,\n",
            "         2.6969e-02, 2.1365e-01, 3.7754e-03, 1.3772e-03, 1.2591e-03, 9.0900e-04,\n",
            "         1.1338e-04, 2.3891e-03, 1.1215e-03, 4.6152e-03, 1.5460e-03, 1.3241e-03,\n",
            "         1.0286e-03, 1.2448e-03, 2.2902e-03, 9.6995e-04, 1.4346e-04, 7.0530e-04],\n",
            "        [1.5744e-02, 1.1545e-02, 2.6834e-02, 1.6727e-02, 1.9622e-02, 1.6429e-02,\n",
            "         2.5723e-02, 2.8154e-02, 1.8739e-02, 5.2483e-02, 3.2389e-02, 3.2804e-02,\n",
            "         9.8793e-02, 1.3994e-01, 1.7437e-02, 1.4680e-02, 1.1008e-02, 9.0515e-03,\n",
            "         1.3765e-02, 1.6252e-02, 1.2919e-02, 2.2558e-02, 1.7227e-02, 1.2987e-02,\n",
            "         1.4994e-02, 1.1056e-02, 1.1441e-02, 1.3186e-02, 1.1600e-02, 1.0480e-02],\n",
            "        [1.5787e-02, 5.7184e-03, 2.6745e-02, 9.7576e-03, 7.4200e-03, 1.1224e-02,\n",
            "         2.4341e-02, 7.6660e-02, 8.1625e-03, 3.0648e-02, 2.6691e-02, 3.9291e-02,\n",
            "         1.6784e-02, 1.8971e-01, 7.5166e-03, 1.0442e-02, 5.0535e-03, 4.7303e-03,\n",
            "         1.1229e-02, 8.9789e-03, 7.6052e-03, 1.0568e-02, 9.9246e-03, 8.6200e-03,\n",
            "         1.1500e-02, 6.9732e-03, 4.8588e-03, 7.6460e-03, 5.6150e-03, 5.4164e-03],\n",
            "        [1.8892e-02, 9.3399e-03, 2.3688e-02, 1.4770e-02, 1.7383e-02, 1.5829e-02,\n",
            "         2.1756e-02, 3.5866e-02, 1.6194e-02, 4.3000e-02, 2.7113e-02, 5.0342e-02,\n",
            "         4.7753e-02, 1.7898e-01, 1.6240e-02, 1.4265e-02, 9.5422e-03, 8.4208e-03,\n",
            "         1.8517e-02, 1.3998e-02, 1.2196e-02, 1.6666e-02, 1.6093e-02, 1.2116e-02,\n",
            "         1.6572e-02, 1.0001e-02, 9.9938e-03, 1.2691e-02, 1.1856e-02, 9.7763e-03],\n",
            "        [1.3690e-02, 6.0494e-03, 1.9539e-02, 1.0167e-02, 2.0498e-02, 1.2394e-02,\n",
            "         1.8492e-02, 2.6207e-02, 8.4188e-03, 6.8303e-02, 2.3519e-02, 2.2414e-02,\n",
            "         7.9360e-02, 1.5186e-01, 8.7860e-03, 9.1847e-03, 4.7133e-03, 2.7109e-03,\n",
            "         1.7126e-02, 8.3178e-03, 6.7406e-03, 1.2795e-02, 1.2898e-02, 8.5541e-03,\n",
            "         1.4121e-02, 8.1347e-03, 4.3686e-03, 9.2638e-03, 1.1024e-02, 6.7022e-03],\n",
            "        [4.3623e-02, 4.5577e-02, 3.5764e-02, 2.8034e-02, 6.6777e-02, 2.7987e-02,\n",
            "         5.5710e-02, 3.9265e-02, 3.7267e-02, 1.0747e-01, 8.1868e-02, 1.0878e-01,\n",
            "         1.0396e-01, 2.1795e-01, 4.0036e-02, 3.9313e-02, 3.5667e-02, 6.2597e-02,\n",
            "         3.2566e-02, 3.1542e-02, 3.3797e-02, 3.5067e-02, 2.8870e-02, 3.5409e-02,\n",
            "         3.1868e-02, 3.1279e-02, 4.3473e-02, 2.8390e-02, 2.4870e-02, 3.3116e-02],\n",
            "        [4.6442e-03, 1.0264e-02, 3.5132e-03, 9.1746e-03, 7.7208e-03, 1.1607e-02,\n",
            "         3.3838e-03, 2.7450e-03, 6.6796e-03, 4.9178e-03, 3.7529e-03, 4.3576e-03,\n",
            "         5.6012e-03, 4.0293e-03, 6.5645e-03, 1.2081e-01, 9.2251e-03, 4.8291e-02,\n",
            "         3.5732e-02, 1.0288e-02, 2.0902e-02, 5.0968e-03, 8.6122e-03, 1.8468e-02,\n",
            "         1.6434e-02, 8.4083e-03, 1.1904e-02, 9.1574e-03, 1.1783e-02, 1.0389e-02],\n",
            "        [5.8094e-03, 1.4953e-02, 8.3481e-03, 1.4628e-02, 1.3866e-02, 1.6461e-02,\n",
            "         4.5969e-03, 5.2308e-03, 1.0350e-02, 5.8521e-03, 4.7446e-03, 6.2468e-03,\n",
            "         8.6586e-03, 4.2216e-03, 9.9844e-03, 1.2824e-01, 1.1967e-02, 4.0191e-02,\n",
            "         5.6572e-02, 1.4555e-02, 2.7170e-02, 8.2891e-03, 1.5107e-02, 2.0257e-02,\n",
            "         3.1966e-02, 1.0909e-02, 1.3499e-02, 1.5372e-02, 4.2742e-02, 1.2512e-02],\n",
            "        [5.0602e-03, 9.0741e-03, 4.5751e-03, 9.9600e-03, 1.2936e-02, 1.0810e-02,\n",
            "         4.0814e-03, 3.8357e-03, 8.0895e-03, 1.3085e-02, 4.0250e-03, 7.7666e-03,\n",
            "         1.8488e-02, 1.3127e-02, 7.9195e-03, 4.1351e-02, 7.0842e-03, 1.1976e-01,\n",
            "         8.6549e-02, 8.9845e-03, 2.7713e-02, 9.0792e-03, 9.9595e-03, 2.9669e-02,\n",
            "         3.6222e-02, 6.3447e-03, 7.4771e-03, 8.2828e-03, 7.6350e-02, 6.4832e-03],\n",
            "        [1.8315e-03, 4.1274e-03, 7.4778e-03, 3.2688e-03, 9.5267e-04, 6.3298e-03,\n",
            "         2.4263e-03, 3.2282e-02, 1.4341e-03, 2.3179e-03, 3.0404e-03, 1.0629e-02,\n",
            "         4.1713e-03, 8.3548e-02, 1.2123e-03, 9.3831e-03, 4.2564e-03, 3.8947e-02,\n",
            "         1.3295e-01, 2.0411e-03, 5.8147e-03, 8.2304e-04, 1.2039e-03, 2.7673e-02,\n",
            "         1.7718e-02, 6.2505e-03, 2.9845e-03, 2.8026e-03, 1.7765e-02, 4.0508e-03],\n",
            "        [7.3870e-03, 1.8562e-02, 1.3157e-02, 1.7183e-02, 7.4435e-03, 2.2452e-02,\n",
            "         9.4296e-03, 2.2256e-02, 9.4507e-03, 1.7081e-02, 1.1002e-02, 2.6820e-02,\n",
            "         3.7935e-02, 1.5510e-01, 9.2719e-03, 2.2195e-02, 1.5368e-02, 4.5900e-02,\n",
            "         8.4704e-02, 1.2071e-02, 2.1240e-02, 9.6209e-03, 1.6070e-02, 3.8755e-02,\n",
            "         4.3824e-02, 2.1489e-02, 1.4531e-02, 1.8843e-02, 3.9090e-02, 1.9477e-02],\n",
            "        [1.0216e-03, 1.1285e-03, 2.5130e-03, 3.1746e-03, 5.9723e-04, 5.0446e-03,\n",
            "         2.6872e-04, 1.4269e-03, 6.7093e-04, 2.0617e-04, 3.4904e-04, 1.4510e-04,\n",
            "         2.8040e-04, 1.0887e-04, 9.1555e-04, 2.7585e-02, 1.7277e-03, 1.4903e-01,\n",
            "         5.2114e-02, 1.5960e-03, 1.3802e-02, 6.5745e-04, 2.6412e-03, 2.2613e-02,\n",
            "         2.2391e-02, 1.9480e-03, 1.9215e-03, 3.6627e-03, 4.6775e-02, 3.2085e-03],\n",
            "        [9.6189e-03, 1.1695e-02, 1.3277e-02, 1.6785e-02, 1.1090e-02, 2.3833e-02,\n",
            "         8.3739e-03, 5.4816e-03, 1.6165e-02, 6.5727e-03, 9.5882e-03, 1.2830e-02,\n",
            "         1.6087e-02, 5.9911e-03, 1.3000e-02, 3.8740e-02, 1.3099e-02, 1.9856e-02,\n",
            "         1.0001e-01, 2.0542e-02, 2.2799e-02, 8.2522e-03, 1.9540e-02, 2.7643e-02,\n",
            "         4.5655e-02, 1.0629e-02, 1.7515e-02, 1.4668e-02, 1.3691e-01, 1.2650e-02],\n",
            "        [2.9267e-02, 1.9819e-02, 5.5670e-02, 2.0057e-02, 1.1533e-02, 3.0236e-02,\n",
            "         2.0090e-02, 3.9907e-02, 1.4130e-02, 9.4139e-03, 1.8166e-02, 1.1748e-02,\n",
            "         8.7234e-03, 8.3942e-03, 1.2626e-02, 2.0091e-02, 1.9368e-02, 2.9575e-02,\n",
            "         1.7132e-01, 1.4909e-02, 1.7610e-02, 1.1719e-02, 1.4034e-02, 4.0501e-02,\n",
            "         4.9214e-02, 2.4398e-02, 1.8041e-02, 1.8773e-02, 2.0593e-01, 2.2756e-02],\n",
            "        [3.9940e-03, 5.0801e-03, 8.5177e-03, 1.1864e-02, 3.0565e-03, 1.6928e-02,\n",
            "         2.4536e-03, 5.7320e-03, 4.1984e-03, 1.7751e-03, 2.7689e-03, 1.4972e-03,\n",
            "         2.3062e-03, 1.1627e-03, 5.0690e-03, 7.4459e-02, 7.4840e-03, 1.3893e-01,\n",
            "         1.0645e-01, 7.0687e-03, 1.7585e-02, 4.0504e-03, 9.8526e-03, 2.0393e-02,\n",
            "         3.6499e-02, 7.6960e-03, 6.7694e-03, 1.2168e-02, 7.8800e-02, 1.1424e-02],\n",
            "        [2.4139e-02, 4.3771e-02, 3.9260e-02, 5.6749e-02, 2.7755e-02, 7.2218e-02,\n",
            "         1.5327e-02, 3.0211e-02, 3.1676e-02, 1.4673e-02, 1.9116e-02, 1.3560e-02,\n",
            "         1.6649e-02, 7.7011e-03, 3.5621e-02, 1.2385e-01, 4.9740e-02, 1.8890e-01,\n",
            "         2.9326e-01, 5.4428e-02, 1.0839e-01, 2.6693e-02, 6.8431e-02, 1.2895e-01,\n",
            "         1.7740e-01, 5.0747e-02, 6.3770e-02, 7.8912e-02, 2.7356e-01, 7.2863e-02],\n",
            "        [3.1014e-04, 1.8875e-03, 9.6525e-04, 1.0994e-03, 3.4024e-04, 2.9734e-03,\n",
            "         1.1216e-04, 4.0584e-04, 3.9187e-04, 9.8234e-05, 1.4639e-04, 2.9017e-05,\n",
            "         6.4571e-05, 2.4501e-05, 4.7369e-04, 2.8009e-03, 1.2795e-03, 2.0150e-02,\n",
            "         1.3920e-01, 7.4630e-04, 5.4933e-03, 2.5730e-04, 1.0401e-03, 9.2431e-03,\n",
            "         1.3192e-02, 1.4549e-03, 2.9714e-03, 2.7876e-03, 1.2421e-01, 4.2153e-03],\n",
            "        [9.7093e-04, 7.7578e-03, 8.4627e-04, 3.1647e-03, 3.6672e-03, 4.9544e-03,\n",
            "         6.2250e-04, 3.2631e-04, 2.3203e-03, 1.1351e-03, 5.9482e-04, 1.0253e-03,\n",
            "         9.6065e-04, 3.2275e-04, 2.4491e-03, 6.2406e-02, 4.9801e-03, 6.7207e-02,\n",
            "         3.1366e-02, 4.9554e-03, 3.9023e-02, 1.3585e-03, 3.4869e-03, 1.4708e-02,\n",
            "         1.7964e-02, 3.9234e-03, 8.5183e-03, 4.3555e-03, 1.1495e-01, 5.0596e-03],\n",
            "        [1.2578e-02, 2.4764e-02, 2.0401e-02, 1.9266e-02, 1.2142e-02, 2.3249e-02,\n",
            "         9.4228e-03, 1.8255e-02, 1.2076e-02, 8.2569e-03, 9.0146e-03, 5.7847e-03,\n",
            "         5.6636e-03, 4.6111e-03, 1.2874e-02, 3.2388e-02, 2.5701e-02, 1.2198e-01,\n",
            "         3.1192e-01, 1.6118e-02, 3.6257e-02, 1.1631e-02, 1.4452e-02, 4.7101e-02,\n",
            "         4.1025e-02, 2.1256e-02, 3.0035e-02, 2.2827e-02, 2.8952e-01, 3.1711e-02],\n",
            "        [3.9093e-04, 2.2752e-03, 7.0208e-04, 2.7902e-03, 8.9557e-04, 4.7029e-03,\n",
            "         9.0476e-05, 3.0146e-04, 1.4313e-03, 1.4680e-04, 9.6473e-05, 2.2935e-04,\n",
            "         4.5997e-04, 4.3974e-05, 1.0310e-03, 2.9525e-02, 2.6174e-03, 8.0115e-02,\n",
            "         9.3742e-02, 3.2194e-03, 2.1253e-02, 3.6998e-04, 2.3484e-03, 2.3028e-02,\n",
            "         2.5588e-02, 1.7870e-03, 5.1440e-03, 3.5474e-03, 1.1752e-01, 3.0077e-03],\n",
            "        [9.8813e-04, 7.1341e-03, 1.5721e-03, 4.8845e-03, 3.4817e-03, 7.1289e-03,\n",
            "         7.1360e-04, 6.6913e-04, 2.7302e-03, 1.0297e-03, 7.4953e-04, 6.8069e-04,\n",
            "         8.6103e-04, 2.6491e-04, 3.0963e-03, 2.2046e-02, 6.6993e-03, 6.8500e-02,\n",
            "         7.2013e-02, 5.2935e-03, 1.7622e-02, 1.8442e-03, 4.9672e-03, 2.3674e-02,\n",
            "         3.1315e-02, 5.7966e-03, 1.0229e-02, 9.0484e-03, 1.2532e-01, 1.0113e-02]]), tensor([[6.8673e-02, 9.5805e-02, 1.0121e-01, 5.7352e-02, 7.2315e-02, 5.3111e-02,\n",
            "         1.7716e-01, 8.9870e-02, 5.1806e-02, 5.7578e-02, 8.7947e-02, 1.0444e-01,\n",
            "         4.0759e-02, 4.9517e-02, 5.5972e-02, 5.8178e-02, 7.0043e-02, 1.3942e-01,\n",
            "         1.0916e-01, 5.3947e-02, 6.9093e-02, 8.6739e-02, 4.2550e-02, 1.0002e-01,\n",
            "         6.2463e-02, 6.5026e-02, 9.5079e-02, 5.4888e-02, 1.0298e-01, 8.4197e-02],\n",
            "        [9.7533e-03, 1.1032e-01, 4.9962e-03, 4.6704e-03, 2.8519e-02, 5.7032e-03,\n",
            "         7.2548e-03, 4.2830e-03, 1.2137e-02, 3.3991e-02, 8.8045e-03, 1.8025e-02,\n",
            "         1.5828e-02, 1.9959e-02, 5.8411e-03, 5.7174e-03, 5.9688e-03, 1.0417e-02,\n",
            "         1.8734e-03, 8.4242e-03, 7.5209e-03, 6.3554e-03, 4.7936e-03, 5.6422e-03,\n",
            "         4.8450e-03, 1.8997e-02, 1.2941e-02, 5.2676e-03, 1.0807e-03, 9.4128e-03],\n",
            "        [1.8673e-02, 2.3970e-02, 6.8549e-02, 2.5681e-02, 2.6488e-02, 1.7545e-02,\n",
            "         1.3686e-01, 5.9370e-02, 1.8216e-02, 2.5740e-02, 6.2106e-02, 6.0189e-02,\n",
            "         2.0854e-02, 2.4095e-02, 2.3868e-02, 1.6603e-02, 1.8813e-02, 3.0993e-02,\n",
            "         6.4858e-02, 1.8317e-02, 1.8122e-02, 6.1760e-02, 1.8293e-02, 3.1840e-02,\n",
            "         2.5410e-02, 9.2229e-03, 2.6831e-02, 1.5298e-02, 4.9656e-02, 1.7192e-02],\n",
            "        [2.2736e-01, 2.8758e-01, 1.5350e-01, 1.7797e-01, 3.0505e-01, 2.2735e-01,\n",
            "         1.9115e-01, 1.5424e-01, 2.5356e-01, 3.4398e-01, 2.0355e-01, 3.1614e-01,\n",
            "         3.3698e-01, 3.1564e-01, 1.7069e-01, 2.3659e-01, 1.6688e-01, 1.9328e-01,\n",
            "         1.0466e-01, 2.1813e-01, 1.8266e-01, 1.8386e-01, 1.6552e-01, 1.7761e-01,\n",
            "         1.5825e-01, 2.3372e-01, 1.9913e-01, 1.6567e-01, 7.6674e-02, 1.6210e-01],\n",
            "        [1.1784e-02, 2.4813e-02, 7.7044e-03, 5.8332e-03, 1.1358e-01, 6.1998e-03,\n",
            "         1.1273e-02, 1.0016e-02, 9.4033e-03, 7.3371e-02, 1.0977e-02, 1.9424e-02,\n",
            "         2.3374e-02, 3.1559e-02, 5.9739e-03, 5.8149e-03, 5.3666e-03, 3.5547e-03,\n",
            "         3.0369e-03, 6.7332e-03, 4.7616e-03, 7.1402e-03, 5.4737e-03, 4.7126e-03,\n",
            "         4.0909e-03, 7.2023e-03, 6.6602e-03, 4.9597e-03, 3.1206e-03, 6.1580e-03],\n",
            "        [4.6808e-03, 4.5159e-03, 4.2431e-03, 1.5979e-03, 1.4482e-02, 1.5037e-03,\n",
            "         1.0490e-01, 6.8226e-03, 2.7456e-03, 2.9293e-02, 2.0977e-02, 6.1337e-02,\n",
            "         1.3811e-02, 8.8095e-02, 2.5713e-03, 2.5123e-03, 1.6027e-03, 1.0257e-03,\n",
            "         4.3115e-04, 1.9674e-03, 1.2342e-03, 4.5114e-03, 2.0646e-03, 1.2793e-03,\n",
            "         1.1958e-03, 2.2594e-03, 2.2863e-03, 1.3409e-03, 7.1924e-04, 2.7038e-03],\n",
            "        [3.6277e-03, 3.9688e-03, 1.0639e-02, 1.9650e-03, 1.3566e-02, 1.6236e-03,\n",
            "         1.0366e-01, 7.8558e-03, 3.2323e-03, 1.9257e-02, 4.1779e-02, 6.1409e-02,\n",
            "         1.3182e-02, 1.4110e-02, 2.6048e-03, 2.6803e-03, 1.3590e-03, 1.0451e-03,\n",
            "         3.0605e-03, 2.3578e-03, 1.4020e-03, 1.4122e-02, 2.0440e-03, 1.9818e-03,\n",
            "         1.4217e-03, 1.5306e-03, 2.5068e-03, 1.2051e-03, 7.3771e-04, 1.5489e-03],\n",
            "        [3.1840e-02, 2.0323e-02, 4.2640e-02, 2.5766e-02, 2.4111e-02, 2.2204e-02,\n",
            "         7.3727e-02, 4.1757e-02, 2.6041e-02, 3.8624e-02, 7.8466e-02, 1.5234e-01,\n",
            "         5.2186e-02, 6.7744e-02, 2.4822e-02, 1.8089e-02, 1.8051e-02, 1.4991e-02,\n",
            "         1.9251e-02, 2.1588e-02, 1.3500e-02, 4.3522e-02, 2.0157e-02, 2.4777e-02,\n",
            "         1.8511e-02, 1.6807e-02, 1.7145e-02, 1.5820e-02, 1.8910e-02, 1.8109e-02],\n",
            "        [2.5289e-02, 1.7953e-02, 1.5437e-02, 6.5489e-03, 7.2805e-02, 6.7587e-03,\n",
            "         6.7278e-02, 2.2917e-02, 1.6377e-02, 1.5656e-01, 4.4724e-02, 1.1225e-01,\n",
            "         8.5520e-02, 1.2995e-01, 8.7122e-03, 8.8576e-03, 5.0606e-03, 2.5418e-03,\n",
            "         7.8147e-04, 9.1216e-03, 4.1326e-03, 1.5023e-02, 5.8992e-03, 4.2603e-03,\n",
            "         3.2595e-03, 5.9860e-03, 8.8695e-03, 3.8494e-03, 1.0378e-03, 5.2280e-03],\n",
            "        [1.1018e-02, 8.1958e-03, 9.0257e-03, 5.0340e-03, 3.7778e-02, 4.7682e-03,\n",
            "         6.8234e-02, 1.1968e-02, 7.4256e-03, 1.2575e-01, 2.8806e-02, 9.2356e-02,\n",
            "         4.4299e-02, 9.0748e-02, 4.7220e-03, 7.6200e-03, 2.9444e-03, 1.5754e-03,\n",
            "         1.2096e-03, 5.4989e-03, 3.0478e-03, 1.1265e-02, 4.6618e-03, 3.6700e-03,\n",
            "         2.3831e-03, 4.7902e-03, 3.5031e-03, 2.7106e-03, 1.0734e-03, 4.1918e-03],\n",
            "        [5.8945e-03, 5.2131e-03, 1.6769e-02, 4.2939e-03, 2.2299e-02, 3.4014e-03,\n",
            "         1.0582e-01, 1.3565e-02, 6.5030e-03, 3.1251e-02, 5.3101e-02, 1.1432e-01,\n",
            "         3.9255e-02, 4.3998e-02, 5.0598e-03, 6.2629e-03, 1.8086e-03, 1.2623e-03,\n",
            "         6.1011e-03, 4.7216e-03, 2.3297e-03, 1.8406e-02, 4.0132e-03, 4.8865e-03,\n",
            "         2.4848e-03, 1.7670e-03, 3.4605e-03, 1.7804e-03, 1.6390e-03, 1.6379e-03],\n",
            "        [1.4435e-03, 2.6107e-03, 2.9918e-03, 6.0087e-04, 1.4628e-02, 6.0516e-04,\n",
            "         5.9146e-02, 2.6233e-03, 1.4538e-03, 2.7250e-02, 2.5226e-02, 1.0282e-01,\n",
            "         2.5164e-02, 1.7897e-02, 6.6437e-04, 2.4400e-03, 3.1114e-04, 1.4678e-04,\n",
            "         3.8044e-04, 9.2600e-04, 4.5413e-04, 3.4050e-03, 6.6965e-04, 6.8530e-04,\n",
            "         2.3732e-04, 5.4315e-04, 6.3952e-04, 2.3984e-04, 1.1300e-04, 5.9547e-04],\n",
            "        [2.6121e-03, 3.5268e-03, 8.1259e-03, 2.1009e-03, 3.2143e-02, 1.8761e-03,\n",
            "         7.4476e-02, 7.9395e-03, 4.2232e-03, 5.7643e-02, 2.8914e-02, 1.0593e-01,\n",
            "         1.2077e-01, 5.6650e-02, 1.6356e-03, 4.8009e-03, 4.1382e-04, 1.8902e-04,\n",
            "         1.4532e-03, 1.9464e-03, 6.8201e-04, 1.1400e-02, 1.3334e-03, 2.2244e-03,\n",
            "         7.1802e-04, 3.5639e-04, 8.4741e-04, 5.3550e-04, 2.7435e-04, 2.7681e-04],\n",
            "        [1.5183e-02, 1.3721e-02, 1.6491e-02, 6.7424e-03, 3.4391e-02, 7.3116e-03,\n",
            "         1.0495e-01, 2.2233e-02, 1.4575e-02, 9.2935e-02, 4.9626e-02, 1.5479e-01,\n",
            "         1.1870e-01, 1.5656e-01, 6.9255e-03, 1.3426e-02, 3.2801e-03, 1.6464e-03,\n",
            "         1.4327e-03, 8.5791e-03, 4.6360e-03, 1.6749e-02, 5.2853e-03, 4.4274e-03,\n",
            "         2.3245e-03, 5.3938e-03, 4.7413e-03, 2.6444e-03, 8.9481e-04, 3.6775e-03],\n",
            "        [9.3565e-04, 1.3971e-03, 1.8079e-03, 1.6323e-04, 3.3319e-02, 1.0313e-04,\n",
            "         8.4919e-02, 2.2817e-03, 7.0800e-04, 8.8158e-02, 1.5803e-02, 1.1142e-01,\n",
            "         1.1086e-01, 9.6844e-02, 2.0789e-04, 1.6394e-03, 2.3995e-05, 8.0511e-06,\n",
            "         5.5658e-05, 2.1779e-04, 6.9053e-05, 2.0617e-03, 9.2496e-05, 2.1321e-04,\n",
            "         2.3541e-05, 2.6323e-05, 1.2647e-04, 2.0195e-05, 6.7511e-06, 3.9171e-05],\n",
            "        [1.2340e-01, 2.3337e-01, 7.3623e-02, 1.0401e-01, 2.9291e-01, 1.3468e-01,\n",
            "         8.8353e-02, 8.2996e-02, 1.3607e-01, 2.1128e-01, 9.6743e-02, 1.1046e-01,\n",
            "         1.3520e-01, 1.6868e-01, 1.1008e-01, 1.9307e-01, 1.3897e-01, 3.0746e-01,\n",
            "         2.9692e-01, 1.2975e-01, 2.4606e-01, 8.7422e-02, 1.2304e-01, 1.8700e-01,\n",
            "         1.7011e-01, 2.2925e-01, 2.2251e-01, 1.6574e-01, 1.7429e-01, 2.2574e-01],\n",
            "        [5.6286e-02, 7.7072e-02, 3.9771e-02, 3.5412e-02, 8.5523e-02, 4.7249e-02,\n",
            "         2.7339e-02, 4.7799e-02, 4.7412e-02, 5.0085e-02, 2.3646e-02, 3.0154e-02,\n",
            "         3.5032e-02, 4.2310e-02, 4.3845e-02, 5.1779e-02, 6.5269e-02, 6.8978e-02,\n",
            "         1.2375e-01, 4.5216e-02, 1.0631e-01, 2.7518e-02, 5.0696e-02, 4.7439e-02,\n",
            "         4.6746e-02, 8.5387e-02, 9.1237e-02, 6.9837e-02, 3.2312e-01, 8.7645e-02],\n",
            "        [2.3326e-02, 4.5413e-02, 4.1054e-02, 5.0838e-02, 3.2382e-02, 6.3805e-02,\n",
            "         1.7606e-02, 3.0503e-02, 2.8838e-02, 1.6091e-02, 1.8654e-02, 9.9205e-03,\n",
            "         1.4360e-02, 1.1694e-02, 3.8727e-02, 7.5071e-02, 5.6063e-02, 1.0913e-01,\n",
            "         3.5523e-01, 4.1010e-02, 1.2860e-01, 3.4741e-02, 6.4363e-02, 7.6491e-02,\n",
            "         1.0933e-01, 7.2346e-02, 7.1369e-02, 9.5382e-02, 3.4372e-01, 7.7049e-02],\n",
            "        [4.9924e-03, 7.1664e-03, 4.1153e-03, 6.2770e-03, 6.9207e-03, 7.4423e-03,\n",
            "         4.2400e-03, 3.1029e-03, 7.0679e-03, 5.5984e-03, 5.1579e-03, 6.7094e-03,\n",
            "         6.3776e-03, 5.9333e-03, 7.2776e-03, 9.4814e-03, 6.9449e-03, 1.1074e-02,\n",
            "         1.1602e-01, 7.6615e-03, 1.3306e-02, 5.4499e-03, 9.6755e-03, 7.5262e-03,\n",
            "         1.2495e-02, 8.7168e-03, 9.5575e-03, 1.1894e-02, 8.2103e-03, 7.3041e-03],\n",
            "        [2.3660e-03, 3.6607e-03, 6.4940e-03, 4.5630e-03, 2.3819e-03, 7.4390e-03,\n",
            "         2.3601e-03, 3.7480e-03, 2.1774e-03, 9.4515e-04, 1.6550e-03, 8.4738e-04,\n",
            "         9.6574e-04, 8.1214e-04, 3.5811e-03, 6.7229e-03, 5.9019e-03, 1.2498e-02,\n",
            "         1.3711e-01, 3.1383e-03, 1.3266e-02, 6.4940e-03, 6.4579e-03, 8.0558e-03,\n",
            "         1.2871e-02, 7.9596e-03, 7.3809e-03, 1.1091e-02, 1.0466e-01, 1.0312e-02],\n",
            "        [9.5189e-03, 3.2758e-02, 9.4243e-04, 1.1692e-03, 1.1388e-02, 2.5443e-03,\n",
            "         4.2829e-04, 1.1248e-03, 2.0060e-03, 2.1087e-03, 1.8339e-04, 1.9745e-04,\n",
            "         4.3042e-04, 7.6671e-04, 1.8294e-03, 6.4446e-03, 6.8458e-03, 2.7827e-02,\n",
            "         1.2771e-02, 2.5510e-03, 1.0835e-01, 3.1654e-04, 2.6026e-03, 5.0050e-03,\n",
            "         2.7631e-03, 3.6426e-02, 2.5431e-02, 9.9621e-03, 1.5313e-01, 3.0600e-02],\n",
            "        [7.1109e-02, 1.0792e-01, 7.6946e-02, 7.9504e-02, 7.3726e-02, 9.2522e-02,\n",
            "         9.9485e-02, 5.1066e-02, 7.0343e-02, 6.9110e-02, 9.5435e-02, 1.1906e-01,\n",
            "         5.2330e-02, 5.9161e-02, 7.1736e-02, 1.0568e-01, 7.8953e-02, 2.4412e-01,\n",
            "         2.8344e-01, 7.5699e-02, 1.0476e-01, 9.4709e-02, 6.1566e-02, 1.8750e-01,\n",
            "         1.4603e-01, 1.1175e-01, 9.6982e-02, 8.8284e-02, 1.7532e-01, 1.1124e-01],\n",
            "        [3.0886e-02, 5.8210e-02, 4.2076e-02, 5.9179e-02, 3.9945e-02, 7.5651e-02,\n",
            "         2.6108e-02, 3.3635e-02, 3.9595e-02, 2.7571e-02, 2.9722e-02, 1.8906e-02,\n",
            "         2.1211e-02, 2.0972e-02, 5.1387e-02, 8.6359e-02, 6.7043e-02, 1.4885e-01,\n",
            "         4.2611e-01, 5.0794e-02, 1.0713e-01, 3.9471e-02, 6.7363e-02, 1.0402e-01,\n",
            "         1.2890e-01, 7.3475e-02, 6.8974e-02, 8.9007e-02, 2.1276e-01, 7.7166e-02],\n",
            "        [1.9352e-02, 3.2832e-02, 2.2817e-02, 2.3654e-02, 2.9568e-02, 2.7984e-02,\n",
            "         1.2997e-02, 2.3370e-02, 2.0232e-02, 1.5525e-02, 1.2391e-02, 7.1846e-03,\n",
            "         1.3051e-02, 1.2515e-02, 2.4743e-02, 3.6985e-02, 3.7372e-02, 5.3321e-02,\n",
            "         9.7227e-02, 2.4360e-02, 7.5249e-02, 1.6076e-02, 3.4276e-02, 3.0015e-02,\n",
            "         3.7258e-02, 5.1153e-02, 4.6318e-02, 5.0378e-02, 2.5408e-01, 5.9666e-02],\n",
            "        [3.5406e-02, 4.5741e-02, 2.7852e-02, 2.3101e-02, 3.5965e-02, 2.7774e-02,\n",
            "         1.8692e-02, 2.7052e-02, 2.2985e-02, 2.2406e-02, 1.5579e-02, 1.1062e-02,\n",
            "         1.5207e-02, 1.7506e-02, 2.5887e-02, 3.2397e-02, 3.9856e-02, 5.9151e-02,\n",
            "         8.5855e-02, 2.5505e-02, 4.1494e-02, 1.7386e-02, 2.7158e-02, 3.3875e-02,\n",
            "         3.1420e-02, 4.9440e-02, 4.5554e-02, 3.6436e-02, 2.2564e-01, 6.7180e-02],\n",
            "        [1.9651e-02, 3.2468e-02, 5.7530e-03, 6.3183e-03, 2.4575e-02, 9.3686e-03,\n",
            "         3.5388e-03, 7.1537e-03, 9.8179e-03, 1.0281e-02, 2.9919e-03, 2.2634e-03,\n",
            "         4.4945e-03, 6.4188e-03, 9.4608e-03, 1.5870e-02, 1.8380e-02, 3.0115e-02,\n",
            "         2.9559e-02, 1.0954e-02, 3.4265e-02, 3.0647e-03, 1.3143e-02, 1.0754e-02,\n",
            "         1.2090e-02, 3.9580e-02, 3.3399e-02, 2.3444e-02, 1.9207e-01, 4.6743e-02],\n",
            "        [2.7711e-02, 4.7021e-02, 1.9745e-02, 2.1155e-02, 2.4440e-02, 2.7639e-02,\n",
            "         1.1288e-02, 1.5422e-02, 1.7194e-02, 1.4182e-02, 1.1894e-02, 9.6293e-03,\n",
            "         9.4575e-03, 9.2989e-03, 2.0643e-02, 3.2757e-02, 3.1847e-02, 7.0406e-02,\n",
            "         1.0832e-01, 2.2111e-02, 3.8231e-02, 1.5592e-02, 2.4205e-02, 3.7909e-02,\n",
            "         3.5127e-02, 6.1004e-02, 3.3937e-02, 3.4415e-02, 2.2053e-01, 6.5814e-02],\n",
            "        [1.8526e-02, 3.0320e-02, 9.2296e-03, 9.8959e-03, 2.2400e-02, 1.5378e-02,\n",
            "         5.3212e-03, 8.8623e-03, 1.2021e-02, 1.0372e-02, 4.3134e-03, 3.6681e-03,\n",
            "         5.6981e-03, 6.9491e-03, 1.2463e-02, 2.1012e-02, 2.2987e-02, 4.1493e-02,\n",
            "         5.1764e-02, 1.3812e-02, 4.4809e-02, 5.1926e-03, 1.7464e-02, 1.7400e-02,\n",
            "         2.1079e-02, 4.0437e-02, 3.7431e-02, 3.2274e-02, 2.0400e-01, 4.2696e-02],\n",
            "        [1.0203e-02, 2.1847e-02, 5.7003e-03, 7.2651e-03, 1.2068e-02, 1.0582e-02,\n",
            "         2.5746e-03, 5.7999e-03, 6.4841e-03, 4.7986e-03, 2.5659e-03, 1.3724e-03,\n",
            "         3.1779e-03, 3.2678e-03, 8.3955e-03, 1.5345e-02, 1.5706e-02, 3.6587e-02,\n",
            "         4.0581e-02, 8.4506e-03, 2.4448e-02, 3.2285e-03, 1.1443e-02, 1.2904e-02,\n",
            "         1.3442e-02, 3.4350e-02, 1.8927e-02, 1.9536e-02, 1.7122e-01, 4.7059e-02],\n",
            "        [2.0667e-02, 4.3070e-02, 6.3496e-03, 6.6401e-03, 2.3884e-02, 1.1211e-02,\n",
            "         2.3789e-03, 6.5125e-03, 9.0790e-03, 7.6245e-03, 2.4007e-03, 1.5386e-03,\n",
            "         3.6926e-03, 4.8432e-03, 9.1482e-03, 1.7712e-02, 2.1001e-02, 4.9149e-02,\n",
            "         5.2724e-02, 1.0585e-02, 3.8848e-02, 2.7994e-03, 1.3600e-02, 1.3661e-02,\n",
            "         1.4428e-02, 5.8820e-02, 3.5711e-02, 2.6954e-02, 1.9607e-01, 6.5991e-02]])]\n",
            "torch.Size([2, 30, 30])\n",
            "cross_attn_avg: tensor([[4.7626e-02, 5.7095e-02, 7.8113e-02, 3.2661e-02, 5.1085e-02, 2.9840e-02,\n",
            "         1.0475e-01, 1.3392e-01, 3.3622e-02, 6.2597e-02, 6.0439e-02, 8.0164e-02,\n",
            "         3.2787e-02, 1.1206e-01, 3.4903e-02, 3.2670e-02, 4.1747e-02, 7.8306e-02,\n",
            "         5.5944e-02, 3.1766e-02, 3.8892e-02, 5.0940e-02, 2.4269e-02, 5.4231e-02,\n",
            "         3.3506e-02, 3.6914e-02, 5.6472e-02, 3.0759e-02, 5.3181e-02, 4.6240e-02],\n",
            "        [7.4224e-03, 6.0954e-02, 5.2046e-03, 7.4171e-03, 1.8652e-02, 8.7197e-03,\n",
            "         5.5178e-03, 4.0633e-03, 9.1721e-03, 1.9826e-02, 6.1828e-03, 1.0666e-02,\n",
            "         1.0080e-02, 1.1628e-02, 6.3549e-03, 5.9459e-02, 8.4127e-03, 5.9631e-02,\n",
            "         4.7687e-02, 8.8493e-03, 3.1227e-02, 6.3240e-03, 6.8177e-03, 1.6165e-02,\n",
            "         1.9989e-02, 1.4947e-02, 1.2600e-02, 8.5421e-03, 7.0694e-02, 1.1077e-02],\n",
            "        [1.0162e-01, 8.5142e-02, 1.8777e-01, 7.4240e-02, 9.6953e-02, 5.6187e-02,\n",
            "         2.1587e-01, 2.1715e-01, 7.4198e-02, 1.2853e-01, 1.9873e-01, 1.3166e-01,\n",
            "         8.8776e-02, 1.5334e-01, 8.4856e-02, 5.1520e-02, 7.0506e-02, 7.4408e-02,\n",
            "         1.1185e-01, 6.3847e-02, 7.1397e-02, 1.2606e-01, 6.8452e-02, 6.4612e-02,\n",
            "         6.3981e-02, 6.3692e-02, 8.6893e-02, 6.9833e-02, 1.1832e-01, 7.2819e-02],\n",
            "        [1.1643e-01, 1.4501e-01, 8.5083e-02, 9.3063e-02, 1.5369e-01, 1.1862e-01,\n",
            "         1.0773e-01, 1.2178e-01, 1.2920e-01, 1.7887e-01, 1.0951e-01, 1.6296e-01,\n",
            "         1.8245e-01, 2.1886e-01, 8.7117e-02, 1.2298e-01, 8.5036e-02, 9.9021e-02,\n",
            "         6.4336e-02, 1.1220e-01, 9.4422e-02, 9.4639e-02, 8.5664e-02, 9.3521e-02,\n",
            "         8.4429e-02, 1.1871e-01, 1.0088e-01, 8.5237e-02, 4.1681e-02, 8.2530e-02],\n",
            "        [2.2859e-02, 3.3139e-02, 3.2788e-02, 1.4050e-02, 8.6672e-02, 1.1014e-02,\n",
            "         5.5661e-02, 7.7901e-02, 1.9049e-02, 7.3299e-02, 5.4372e-02, 4.8653e-02,\n",
            "         3.5406e-02, 7.8682e-02, 1.7059e-02, 5.3483e-02, 1.5055e-02, 2.6617e-02,\n",
            "         5.0371e-03, 1.6024e-02, 2.1713e-02, 2.1087e-02, 1.3140e-02, 1.1922e-02,\n",
            "         1.4312e-02, 1.3522e-02, 1.8073e-02, 1.2991e-02, 6.0177e-03, 1.2651e-02],\n",
            "        [1.9767e-02, 1.1319e-02, 1.2340e-02, 6.6905e-03, 2.9408e-02, 5.9036e-03,\n",
            "         7.2305e-02, 2.7098e-02, 1.0142e-02, 7.0021e-02, 3.3518e-02, 5.8728e-02,\n",
            "         3.2545e-02, 1.7932e-01, 1.0782e-02, 1.0298e-02, 6.1188e-03, 3.3486e-03,\n",
            "         1.0825e-03, 7.4321e-03, 5.8975e-03, 1.3303e-02, 7.8837e-03, 3.6358e-03,\n",
            "         4.6837e-03, 6.1818e-03, 7.0355e-03, 5.3602e-03, 1.9521e-03, 5.9985e-03],\n",
            "        [5.9011e-02, 2.8905e-02, 8.4298e-02, 2.6782e-02, 4.1538e-02, 2.5158e-02,\n",
            "         1.3178e-01, 1.4288e-01, 3.0961e-02, 7.9012e-02, 1.1120e-01, 8.8422e-02,\n",
            "         4.7635e-02, 1.2409e-01, 3.5341e-02, 1.8907e-02, 2.4304e-02, 1.5111e-02,\n",
            "         1.9966e-02, 2.5724e-02, 2.2601e-02, 4.7809e-02, 3.1866e-02, 2.0896e-02,\n",
            "         2.4112e-02, 2.9676e-02, 2.5811e-02, 2.8891e-02, 3.1811e-02, 2.9484e-02],\n",
            "        [1.8745e-02, 1.1737e-02, 2.8752e-02, 1.6762e-02, 1.3749e-02, 1.5441e-02,\n",
            "         4.8982e-02, 7.9916e-02, 1.6129e-02, 2.5876e-02, 4.6715e-02, 8.4954e-02,\n",
            "         4.0908e-02, 6.6073e-02, 1.5052e-02, 1.3060e-02, 1.0962e-02, 9.8644e-03,\n",
            "         1.7656e-02, 1.3956e-02, 9.7610e-03, 2.5483e-02, 1.3301e-02, 1.6283e-02,\n",
            "         1.3786e-02, 1.0549e-02, 1.0201e-02, 1.0599e-02, 1.2939e-02, 1.0942e-02],\n",
            "        [2.9673e-02, 1.8569e-02, 2.9636e-02, 9.7137e-03, 5.8792e-02, 1.0135e-02,\n",
            "         6.9962e-02, 8.0379e-02, 1.7382e-02, 1.1400e-01, 5.1773e-02, 8.2698e-02,\n",
            "         6.0917e-02, 1.2194e-01, 1.3907e-02, 1.1012e-02, 9.3570e-03, 6.3314e-03,\n",
            "         3.1284e-03, 1.1526e-02, 7.6993e-03, 1.7255e-02, 9.6948e-03, 7.1685e-03,\n",
            "         6.7474e-03, 9.4670e-03, 1.1919e-02, 7.7657e-03, 5.7615e-03, 8.7300e-03],\n",
            "        [9.2574e-03, 5.9514e-03, 7.1288e-03, 3.3668e-03, 2.5804e-02, 3.2452e-03,\n",
            "         3.8957e-02, 1.2913e-02, 6.0410e-03, 8.9918e-02, 2.0328e-02, 6.3226e-02,\n",
            "         3.5634e-02, 1.5220e-01, 4.2487e-03, 4.4986e-03, 2.1018e-03, 1.2422e-03,\n",
            "         6.6150e-04, 3.9440e-03, 2.0846e-03, 7.9400e-03, 3.1039e-03, 2.4971e-03,\n",
            "         1.7058e-03, 3.0175e-03, 2.8967e-03, 1.8403e-03, 6.0844e-04, 2.4486e-03],\n",
            "        [1.0819e-02, 8.3790e-03, 2.1801e-02, 1.0510e-02, 2.0960e-02, 9.9154e-03,\n",
            "         6.5771e-02, 2.0860e-02, 1.2621e-02, 4.1867e-02, 4.2745e-02, 7.3563e-02,\n",
            "         6.9024e-02, 9.1970e-02, 1.1248e-02, 1.0472e-02, 6.4083e-03, 5.1569e-03,\n",
            "         9.9332e-03, 1.0487e-02, 7.6244e-03, 2.0482e-02, 1.0620e-02, 8.9367e-03,\n",
            "         8.7392e-03, 6.4113e-03, 7.4509e-03, 7.4834e-03, 6.6196e-03, 6.0590e-03],\n",
            "        [8.6154e-03, 4.1646e-03, 1.4869e-02, 5.1793e-03, 1.1024e-02, 5.9147e-03,\n",
            "         4.1744e-02, 3.9642e-02, 4.8082e-03, 2.8949e-02, 2.5959e-02, 7.1055e-02,\n",
            "         2.0974e-02, 1.0380e-01, 4.0905e-03, 6.4409e-03, 2.6823e-03, 2.4385e-03,\n",
            "         5.8048e-03, 4.9524e-03, 4.0297e-03, 6.9866e-03, 5.2971e-03, 4.6527e-03,\n",
            "         5.8688e-03, 3.7582e-03, 2.7492e-03, 3.9429e-03, 2.8640e-03, 3.0059e-03],\n",
            "        [1.0752e-02, 6.4334e-03, 1.5907e-02, 8.4353e-03, 2.4763e-02, 8.8526e-03,\n",
            "         4.8116e-02, 2.1903e-02, 1.0208e-02, 5.0321e-02, 2.8014e-02, 7.8134e-02,\n",
            "         8.4259e-02, 1.1781e-01, 8.9378e-03, 9.5331e-03, 4.9780e-03, 4.3049e-03,\n",
            "         9.9849e-03, 7.9721e-03, 6.4390e-03, 1.4033e-02, 8.7132e-03, 7.1704e-03,\n",
            "         8.6449e-03, 5.1788e-03, 5.4206e-03, 6.6132e-03, 6.0653e-03, 5.0265e-03],\n",
            "        [1.4437e-02, 9.8852e-03, 1.8015e-02, 8.4547e-03, 2.7444e-02, 9.8529e-03,\n",
            "         6.1721e-02, 2.4220e-02, 1.1497e-02, 8.0619e-02, 3.6573e-02, 8.8601e-02,\n",
            "         9.9031e-02, 1.5421e-01, 7.8558e-03, 1.1305e-02, 3.9967e-03, 2.1787e-03,\n",
            "         9.2794e-03, 8.4485e-03, 5.6883e-03, 1.4772e-02, 9.0917e-03, 6.4908e-03,\n",
            "         8.2226e-03, 6.7643e-03, 4.5550e-03, 5.9541e-03, 5.9592e-03, 5.1899e-03],\n",
            "        [2.2279e-02, 2.3487e-02, 1.8786e-02, 1.4099e-02, 5.0048e-02, 1.4045e-02,\n",
            "         7.0314e-02, 2.0773e-02, 1.8987e-02, 9.7813e-02, 4.8836e-02, 1.1010e-01,\n",
            "         1.0741e-01, 1.5740e-01, 2.0122e-02, 2.0476e-02, 1.7846e-02, 3.1302e-02,\n",
            "         1.6311e-02, 1.5880e-02, 1.6933e-02, 1.8564e-02, 1.4481e-02, 1.7811e-02,\n",
            "         1.5946e-02, 1.5653e-02, 2.1800e-02, 1.4205e-02, 1.2438e-02, 1.6577e-02],\n",
            "        [6.4021e-02, 1.2182e-01, 3.8568e-02, 5.6592e-02, 1.5032e-01, 7.3142e-02,\n",
            "         4.5868e-02, 4.2870e-02, 7.1376e-02, 1.0810e-01, 5.0248e-02, 5.7407e-02,\n",
            "         7.0401e-02, 8.6357e-02, 5.8324e-02, 1.5694e-01, 7.4095e-02, 1.7788e-01,\n",
            "         1.6633e-01, 7.0018e-02, 1.3348e-01, 4.6260e-02, 6.5825e-02, 1.0273e-01,\n",
            "         9.3272e-02, 1.1883e-01, 1.1721e-01, 8.7451e-02, 9.3038e-02, 1.1806e-01],\n",
            "        [3.1048e-02, 4.6012e-02, 2.4059e-02, 2.5020e-02, 4.9694e-02, 3.1855e-02,\n",
            "         1.5968e-02, 2.6515e-02, 2.8881e-02, 2.7969e-02, 1.4195e-02, 1.8201e-02,\n",
            "         2.1846e-02, 2.3266e-02, 2.6915e-02, 9.0010e-02, 3.8618e-02, 5.4585e-02,\n",
            "         9.0162e-02, 2.9885e-02, 6.6740e-02, 1.7903e-02, 3.2902e-02, 3.3848e-02,\n",
            "         3.9356e-02, 4.8148e-02, 5.2368e-02, 4.2605e-02, 1.8293e-01, 5.0078e-02],\n",
            "        [1.4193e-02, 2.7244e-02, 2.2814e-02, 3.0399e-02, 2.2659e-02, 3.7307e-02,\n",
            "         1.0844e-02, 1.7169e-02, 1.8464e-02, 1.4588e-02, 1.1340e-02, 8.8436e-03,\n",
            "         1.6424e-02, 1.2411e-02, 2.3323e-02, 5.8211e-02, 3.1574e-02, 1.1444e-01,\n",
            "         2.2089e-01, 2.4997e-02, 7.8157e-02, 2.1910e-02, 3.7161e-02, 5.3080e-02,\n",
            "         7.2778e-02, 3.9345e-02, 3.9423e-02, 5.1832e-02, 2.1003e-01, 4.1766e-02],\n",
            "        [3.4119e-03, 5.6469e-03, 5.7965e-03, 4.7729e-03, 3.9367e-03, 6.8860e-03,\n",
            "         3.3331e-03, 1.7693e-02, 4.2510e-03, 3.9582e-03, 4.0991e-03, 8.6693e-03,\n",
            "         5.2745e-03, 4.4741e-02, 4.2449e-03, 9.4323e-03, 5.6006e-03, 2.5011e-02,\n",
            "         1.2449e-01, 4.8513e-03, 9.5604e-03, 3.1365e-03, 5.4397e-03, 1.7600e-02,\n",
            "         1.5106e-02, 7.4836e-03, 6.2710e-03, 7.3485e-03, 1.2988e-02, 5.6774e-03],\n",
            "        [4.8765e-03, 1.1111e-02, 9.8255e-03, 1.0873e-02, 4.9127e-03, 1.4946e-02,\n",
            "         5.8948e-03, 1.3002e-02, 5.8140e-03, 9.0132e-03, 6.3284e-03, 1.3834e-02,\n",
            "         1.9450e-02, 7.7956e-02, 6.4265e-03, 1.4459e-02, 1.0635e-02, 2.9199e-02,\n",
            "         1.1091e-01, 7.6048e-03, 1.7253e-02, 8.0574e-03, 1.1264e-02, 2.3405e-02,\n",
            "         2.8347e-02, 1.4724e-02, 1.0956e-02, 1.4967e-02, 7.1874e-02, 1.4894e-02],\n",
            "        [5.2703e-03, 1.6943e-02, 1.7277e-03, 2.1719e-03, 5.9927e-03, 3.7945e-03,\n",
            "         3.4851e-04, 1.2758e-03, 1.3385e-03, 1.1574e-03, 2.6622e-04, 1.7127e-04,\n",
            "         3.5541e-04, 4.3779e-04, 1.3725e-03, 1.7015e-02, 4.2868e-03, 8.8428e-02,\n",
            "         3.2442e-02, 2.0735e-03, 6.1076e-02, 4.8699e-04, 2.6219e-03, 1.3809e-02,\n",
            "         1.2577e-02, 1.9187e-02, 1.3676e-02, 6.8124e-03, 9.9954e-02, 1.6904e-02],\n",
            "        [4.0364e-02, 5.9807e-02, 4.5111e-02, 4.8145e-02, 4.2408e-02, 5.8177e-02,\n",
            "         5.3929e-02, 2.8274e-02, 4.3254e-02, 3.7841e-02, 5.2512e-02, 6.5946e-02,\n",
            "         3.4208e-02, 3.2576e-02, 4.2368e-02, 7.2209e-02, 4.6026e-02, 1.3199e-01,\n",
            "         1.9172e-01, 4.8120e-02, 6.3778e-02, 5.1481e-02, 4.0553e-02, 1.0757e-01,\n",
            "         9.5842e-02, 6.1188e-02, 5.7249e-02, 5.1476e-02, 1.5611e-01, 6.1946e-02],\n",
            "        [3.0077e-02, 3.9015e-02, 4.8873e-02, 3.9618e-02, 2.5739e-02, 5.2943e-02,\n",
            "         2.3099e-02, 3.6771e-02, 2.6863e-02, 1.8492e-02, 2.3944e-02, 1.5327e-02,\n",
            "         1.4967e-02, 1.4683e-02, 3.2007e-02, 5.3225e-02, 4.3205e-02, 8.9215e-02,\n",
            "         2.9872e-01, 3.2851e-02, 6.2369e-02, 2.5595e-02, 4.0698e-02, 7.2260e-02,\n",
            "         8.9056e-02, 4.8936e-02, 4.3507e-02, 5.3890e-02, 2.0935e-01, 4.9961e-02],\n",
            "        [1.1673e-02, 1.8956e-02, 1.5667e-02, 1.7759e-02, 1.6312e-02, 2.2456e-02,\n",
            "         7.7255e-03, 1.4551e-02, 1.2215e-02, 8.6500e-03, 7.5799e-03, 4.3409e-03,\n",
            "         7.6784e-03, 6.8387e-03, 1.4906e-02, 5.5722e-02, 2.2428e-02, 9.6124e-02,\n",
            "         1.0184e-01, 1.5715e-02, 4.6417e-02, 1.0063e-02, 2.2064e-02, 2.5204e-02,\n",
            "         3.6879e-02, 2.9424e-02, 2.6544e-02, 3.1273e-02, 1.6644e-01, 3.5545e-02],\n",
            "        [2.9773e-02, 4.4756e-02, 3.3556e-02, 3.9925e-02, 3.1860e-02, 4.9996e-02,\n",
            "         1.7010e-02, 2.8632e-02, 2.7330e-02, 1.8539e-02, 1.7348e-02, 1.2311e-02,\n",
            "         1.5928e-02, 1.2603e-02, 3.0754e-02, 7.8124e-02, 4.4798e-02, 1.2402e-01,\n",
            "         1.8956e-01, 3.9967e-02, 7.4941e-02, 2.2040e-02, 4.7795e-02, 8.1413e-02,\n",
            "         1.0441e-01, 5.0094e-02, 5.4662e-02, 5.7674e-02, 2.4960e-01, 7.0021e-02],\n",
            "        [9.9804e-03, 1.7178e-02, 3.3591e-03, 3.7088e-03, 1.2458e-02, 6.1710e-03,\n",
            "         1.8255e-03, 3.7798e-03, 5.1049e-03, 5.1897e-03, 1.5692e-03, 1.1462e-03,\n",
            "         2.2796e-03, 3.2216e-03, 4.9673e-03, 9.3356e-03, 9.8295e-03, 2.5132e-02,\n",
            "         8.4379e-02, 5.8501e-03, 1.9879e-02, 1.6610e-03, 7.0915e-03, 9.9984e-03,\n",
            "         1.2641e-02, 2.0517e-02, 1.8185e-02, 1.3116e-02, 1.5814e-01, 2.5479e-02],\n",
            "        [1.4341e-02, 2.7390e-02, 1.0296e-02, 1.2160e-02, 1.4054e-02, 1.6297e-02,\n",
            "         5.9552e-03, 7.8741e-03, 9.7574e-03, 7.6583e-03, 6.2443e-03, 5.3273e-03,\n",
            "         5.2091e-03, 4.8108e-03, 1.1546e-02, 4.7582e-02, 1.8413e-02, 6.8807e-02,\n",
            "         6.9845e-02, 1.3533e-02, 3.8627e-02, 8.4753e-03, 1.3846e-02, 2.6309e-02,\n",
            "         2.6546e-02, 3.2464e-02, 2.1227e-02, 1.9385e-02, 1.6774e-01, 3.5437e-02],\n",
            "        [1.5552e-02, 2.7542e-02, 1.4815e-02, 1.4581e-02, 1.7271e-02, 1.9314e-02,\n",
            "         7.3720e-03, 1.3558e-02, 1.2048e-02, 9.3146e-03, 6.6640e-03, 4.7264e-03,\n",
            "         5.6809e-03, 5.7801e-03, 1.2669e-02, 2.6700e-02, 2.4344e-02, 8.1735e-02,\n",
            "         1.8184e-01, 1.4965e-02, 4.0533e-02, 8.4116e-03, 1.5958e-02, 3.2251e-02,\n",
            "         3.1052e-02, 3.0847e-02, 3.3733e-02, 2.7550e-02, 2.4676e-01, 3.7204e-02],\n",
            "        [5.2970e-03, 1.2061e-02, 3.2012e-03, 5.0276e-03, 6.4820e-03, 7.6424e-03,\n",
            "         1.3326e-03, 3.0507e-03, 3.9577e-03, 2.4727e-03, 1.3312e-03, 8.0086e-04,\n",
            "         1.8189e-03, 1.6559e-03, 4.7132e-03, 2.2435e-02, 9.1616e-03, 5.8351e-02,\n",
            "         6.7162e-02, 5.8350e-03, 2.2851e-02, 1.7992e-03, 6.8957e-03, 1.7966e-02,\n",
            "         1.9515e-02, 1.8068e-02, 1.2036e-02, 1.1542e-02, 1.4437e-01, 2.5033e-02],\n",
            "        [1.0828e-02, 2.5102e-02, 3.9608e-03, 5.7623e-03, 1.3683e-02, 9.1701e-03,\n",
            "         1.5462e-03, 3.5908e-03, 5.9046e-03, 4.3271e-03, 1.5751e-03, 1.1097e-03,\n",
            "         2.2768e-03, 2.5541e-03, 6.1223e-03, 1.9879e-02, 1.3850e-02, 5.8824e-02,\n",
            "         6.2369e-02, 7.9394e-03, 2.8235e-02, 2.3218e-03, 9.2838e-03, 1.8668e-02,\n",
            "         2.2871e-02, 3.2308e-02, 2.2970e-02, 1.8001e-02, 1.6070e-01, 3.8052e-02]])\n",
            "SAVING ATTENTION MATRIX...\n",
            "SAVING ATTENTION MATRIX...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python connectivity_evaluation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roV4yTfXI-_N",
        "outputId": "70dfbc09-6eb9-48ea-b410-7ce672990d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_name: generated, 9ch, enc_layers 5, dec_layers 6, heads 10, huge model, bs 128, initial_downsample_convs 0, cp 15, tp 1, tf 3, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none, time_emb_dim 6\n",
            "LOADING ATTENTION MATRIX...\n",
            "median: 0.0955611914396286\n",
            "median: 0.18893185257911682\n",
            "LOADING PDC...\n",
            "LOADING Ground Truth...\n",
            "cross_attn:  7.25821585103445\n",
            "cross_attn_avg:  6.925796163002088\n",
            "PDC:  4.471683607328457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_eeg_channels_loss.py spacetimeformer eeg_social_memory --d_model 15 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .8 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6"
      ],
      "metadata": {
        "id": "7pliHlifTqS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23405b2-dee8-4b97-9776-d32eba41887f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069  9.43670267  9.59859439\n",
            " 11.99715269 12.86104958 11.94699925 11.85080883 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497  9.70085921  9.86928682 11.93068806\n",
            " 12.90250279 11.89939114 11.79367808 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055  9.63590846  9.12828423 12.05575209 12.47919508\n",
            " 11.63939716 11.96261054 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  8.96765425  8.63643789 11.71539883 11.55649842 11.29742551\n",
            " 11.61590913 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            "  8.95039971  8.51054961 11.94941264 11.70165593 11.36950736 11.78133177\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766   9.80036353\n",
            "  9.69273876 12.86747024 12.61101867 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883   9.61653816  9.90022743\n",
            " 12.25216021 12.08176313 11.82400636 11.50908441 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252  9.48844158  8.64725042 12.11688656\n",
            " 11.52212503 10.798857   11.6021242  11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898  9.30173033  8.69311485 12.25332822 11.84272318\n",
            " 10.68338069 11.66820011 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527  9.2346348  10.23483608 11.84921294 11.57971618 11.10154951\n",
            " 11.74282084 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            "  9.03840838 10.18675135 11.51147703 11.28135554 10.96719716 11.50737969\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.08023829\n",
            "  9.58226092 11.42916339 10.97640557 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.05685314 10.0722999\n",
            " 11.99304668 11.66070749 11.33506908 11.84901884 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 10.47832141 10.2976647  13.27774384\n",
            " 12.96302158 12.20277853 12.83705054 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 10.03977555 10.39463715 13.5985274  13.27036878\n",
            " 12.29980717 13.05854175 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 10.40234949 10.05417797 13.17285163 13.25988943 12.24764307\n",
            " 12.88071884 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 10.15624638  8.96728021 12.20744883 12.9980197  11.51224922 11.94614962\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273  9.75187948\n",
            "  8.8189279  11.80604791 12.70660893 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 10.52146353  9.46570555\n",
            " 12.41179997 13.07154775 11.30195552 11.50207537 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.15869699  9.77417063 12.7285977\n",
            " 12.88490919 11.44445635 11.84196478 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.22632594  8.96505438 11.46346181 11.71290237\n",
            " 10.96461481 10.84702527 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395  9.51773815  9.31238487 12.11335282 11.89242115 11.25800381\n",
            " 11.42653252 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.00240709 10.72385311 12.1556716  12.28089028 11.77264389 11.82849204\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.29551629\n",
            " 10.0615734  10.78465069 11.28879561 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   8.88849634  9.41367867\n",
            " 10.07824619 10.54847444 10.35848873  9.64748684 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904  9.72281672  9.7939879  11.60442643\n",
            " 11.98134946 11.27213917 10.98061171 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963  9.77152655  9.86258568 12.35229027 12.65799275\n",
            " 11.15616714 11.5224307  11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163  9.24617346 10.21107356 11.92073779 12.41134609 10.82055414\n",
            " 11.08829494 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 10.37867082 10.66698994 12.29603346 13.20818118 12.39604745 12.19674886\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  1.74675089e-17 -6.47630098e-18\n",
            "  5.65288557e-18 -1.87627691e-17  2.23339865e-17  1.00382665e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/22/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.21.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/22/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.21.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/22/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.21.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:04<00:00, 13.56it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9904844164848328\n",
            "     test/class_loss        0.1804775595664978\n",
            "   test/forecast_loss       0.20968037843704224\n",
            "        test/loss           0.20969843864440918\n",
            "        test/mae            0.35713139176368713\n",
            "        test/mape           15.206356048583984\n",
            "        test/mse            0.20968037843704224\n",
            "      test/norm_mae         0.35713139176368713\n",
            "      test/norm_mse         0.20968037843704224\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.45778292417526245\n",
            "       test/smape           0.7571079134941101\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/23/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.20.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/23/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.20.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch23_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10389625\n",
            " -0.11045897 -0.12415218 -0.12058142 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.04492073\n",
            " 11.40830655 11.54580482 10.93482488 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ...  9.79579117 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.43920224 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.70064388 -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.48073776 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.56348265  0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.19582588 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -0.68676453 -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.66757082  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.65470285  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  1.29259171  1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.90133569  0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.10073837  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ... -0.32523906 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.67949681  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ... -0.17755435  0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.49728133 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.48447895 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  1.02813193  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.70810622 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.12229146  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.23228592  0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.60375175  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.74273023 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.59859439\n",
            " 11.99715269 12.86104958 11.94699925 11.85080883 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.86928682 11.93068806\n",
            " 12.90250279 11.89939114 11.79367808 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.12828423 12.05575209 12.47919508\n",
            " 11.63939716 11.96261054 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.63643789 11.71539883 11.55649842 11.29742551\n",
            " 11.61590913 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.51054961 11.94941264 11.70165593 11.36950736 11.78133177\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.69273876 12.86747024 12.61101867 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.90022743\n",
            " 12.25216021 12.08176313 11.82400636 11.50908441 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  8.64725042 12.11688656\n",
            " 11.52212503 10.798857   11.6021242  11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  8.69311485 12.25332822 11.84272318\n",
            " 10.68338069 11.66820011 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443 10.23483608 11.84921294 11.57971618 11.10154951\n",
            " 11.74282084 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598 10.18675135 11.51147703 11.28135554 10.96719716 11.50737969\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.58226092 11.42916339 10.97640557 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.0722999\n",
            " 11.99304668 11.66070749 11.33506908 11.84901884 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.2976647  13.27774384\n",
            " 12.96302158 12.20277853 12.83705054 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.39463715 13.5985274  13.27036878\n",
            " 12.29980717 13.05854175 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.05417797 13.17285163 13.25988943 12.24764307\n",
            " 12.88071884 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702  8.96728021 12.20744883 12.9980197  11.51224922 11.94614962\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  8.8189279  11.80604791 12.70660893 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406  9.46570555\n",
            " 12.41179997 13.07154775 11.30195552 11.50207537 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131  9.77417063 12.7285977\n",
            " 12.88490919 11.44445635 11.84196478 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   8.96505438 11.46346181 11.71290237\n",
            " 10.96461481 10.84702527 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.31238487 12.11335282 11.89242115 11.25800381\n",
            " 11.42653252 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.72385311 12.1556716  12.28089028 11.77264389 11.82849204\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            " 10.0615734  10.78465069 11.28879561 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  9.41367867\n",
            " 10.07824619 10.54847444 10.35848873  9.64748684 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.7939879  11.60442643\n",
            " 11.98134946 11.27213917 10.98061171 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.86258568 12.35229027 12.65799275\n",
            " 11.15616714 11.5224307  11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247 10.21107356 11.92073779 12.41134609 10.82055414\n",
            " 11.08829494 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.66698994 12.29603346 13.20818118 12.39604745 12.19674886\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18 -6.47630098e-18\n",
            "  5.65288557e-18 -1.87627691e-17  2.23339865e-17  1.00382665e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/23/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.20.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/23/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.20.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/23/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.20.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 17.30it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9990428686141968\n",
            "     test/class_loss        0.15043139457702637\n",
            "   test/forecast_loss       0.2094263881444931\n",
            "        test/loss           0.2094414085149765\n",
            "        test/mae            0.35768991708755493\n",
            "        test/mape            13.81930160522461\n",
            "        test/mse            0.2094263732433319\n",
            "      test/norm_mae         0.35768991708755493\n",
            "      test/norm_mse         0.2094263732433319\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.45739051699638367\n",
            "       test/smape           0.7566475868225098\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/24/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.19.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/24/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.19.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch24_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.11045897 -0.12415218 -0.12058142 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            " 11.40830655 11.54580482 10.93482488 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ...  9.79579117 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.43920224 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.70064388 -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.48073776 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.56348265  0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.19582588 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -0.68676453 -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.66757082  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.65470285  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  1.29259171  1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.90133569  0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.10073837  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ... -0.32523906 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.67949681  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ... -0.17755435  0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.49728133 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.48447895 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  1.02813193  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.70810622 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.12229146  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.23228592  0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.60375175  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.74273023 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            " 11.99715269 12.86104958 11.94699925 11.85080883 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921 11.93068806\n",
            " 12.90250279 11.89939114 11.79367808 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.63590846 12.05575209 12.47919508\n",
            " 11.63939716 11.96261054 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.96765425 11.71539883 11.55649842 11.29742551\n",
            " 11.61590913 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.95039971 11.94941264 11.70165593 11.36950736 11.78133177\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.80036353 12.86747024 12.61101867 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            " 12.25216021 12.08176313 11.82400636 11.50908441 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158 12.11688656\n",
            " 11.52212503 10.798857   11.6021242  11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  9.30173033 12.25332822 11.84272318\n",
            " 10.68338069 11.66820011 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443  9.2346348  11.84921294 11.57971618 11.10154951\n",
            " 11.74282084 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598  9.03840838 11.51147703 11.28135554 10.96719716 11.50737969\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.08023829 11.42916339 10.97640557 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 11.99304668 11.66070749 11.33506908 11.84901884 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141 13.27774384\n",
            " 12.96302158 12.20277853 12.83705054 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.03977555 13.5985274  13.27036878\n",
            " 12.29980717 13.05854175 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.40234949 13.17285163 13.25988943 12.24764307\n",
            " 12.88071884 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702 10.15624638 12.20744883 12.9980197  11.51224922 11.94614962\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  9.75187948 11.80604791 12.70660893 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            " 12.41179997 13.07154775 11.30195552 11.50207537 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699 12.7285977\n",
            " 12.88490919 11.44445635 11.84196478 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   9.22632594 11.46346181 11.71290237\n",
            " 10.96461481 10.84702527 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.51773815 12.11335282 11.89242115 11.25800381\n",
            " 11.42653252 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.00240709 12.1556716  12.28089028 11.77264389 11.82849204\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            "  9.29551629 10.78465069 11.28879561 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            " 10.07824619 10.54847444 10.35848873  9.64748684 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672 11.60442643\n",
            " 11.98134946 11.27213917 10.98061171 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.77152655 12.35229027 12.65799275\n",
            " 11.15616714 11.5224307  11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247  9.24617346 11.92073779 12.41134609 10.82055414\n",
            " 11.08829494 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.37867082 12.29603346 13.20818118 12.39604745 12.19674886\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            "  5.65288557e-18 -1.87627691e-17  2.23339865e-17  1.00382665e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/24/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.19.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/24/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.19.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/24/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.19.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 17.59it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9934241771697998\n",
            "     test/class_loss        0.20373059809207916\n",
            "   test/forecast_loss       0.1982538253068924\n",
            "        test/loss           0.19827423989772797\n",
            "        test/mae            0.3490199148654938\n",
            "        test/mape            12.4102783203125\n",
            "        test/mse            0.1982538253068924\n",
            "      test/norm_mae         0.3490199148654938\n",
            "      test/norm_mse         0.1982538253068924\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.44496580958366394\n",
            "       test/smape           0.7442391514778137\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/25/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.23.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/25/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.23.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch25_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.12415218 -0.12058142 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.54580482 10.93482488 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ...  9.79579117 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.43920224 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.70064388 -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.48073776 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.56348265  0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.19582588 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -0.68676453 -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.66757082  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.65470285  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  1.29259171  1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.90133569  0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.10073837  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ... -0.32523906 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.67949681  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ... -0.17755435  0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.49728133 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.48447895 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  1.02813193  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.70810622 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.12229146  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.23228592  0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.60375175  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.74273023 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 12.86104958 11.94699925 11.85080883 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921  9.86928682\n",
            " 12.90250279 11.89939114 11.79367808 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.63590846  9.12828423 12.47919508\n",
            " 11.63939716 11.96261054 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.96765425  8.63643789 11.55649842 11.29742551\n",
            " 11.61590913 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.95039971  8.51054961 11.70165593 11.36950736 11.78133177\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.80036353  9.69273876 12.61101867 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.08176313 11.82400636 11.50908441 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158  8.64725042\n",
            " 11.52212503 10.798857   11.6021242  11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  9.30173033  8.69311485 11.84272318\n",
            " 10.68338069 11.66820011 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443  9.2346348  10.23483608 11.57971618 11.10154951\n",
            " 11.74282084 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598  9.03840838 10.18675135 11.28135554 10.96719716 11.50737969\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.08023829  9.58226092 10.97640557 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.66070749 11.33506908 11.84901884 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141 10.2976647\n",
            " 12.96302158 12.20277853 12.83705054 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.03977555 10.39463715 13.27036878\n",
            " 12.29980717 13.05854175 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.40234949 10.05417797 13.25988943 12.24764307\n",
            " 12.88071884 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702 10.15624638  8.96728021 12.9980197  11.51224922 11.94614962\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  9.75187948  8.8189279  12.70660893 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 13.07154775 11.30195552 11.50207537 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699  9.77417063\n",
            " 12.88490919 11.44445635 11.84196478 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   9.22632594  8.96505438 11.71290237\n",
            " 10.96461481 10.84702527 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.51773815  9.31238487 11.89242115 11.25800381\n",
            " 11.42653252 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.00240709 10.72385311 12.28089028 11.77264389 11.82849204\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            "  9.29551629 10.0615734  11.28879561 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.54847444 10.35848873  9.64748684 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672  9.7939879\n",
            " 11.98134946 11.27213917 10.98061171 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.77152655  9.86258568 12.65799275\n",
            " 11.15616714 11.5224307  11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247  9.24617346 10.21107356 12.41134609 10.82055414\n",
            " 11.08829494 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.37867082 10.66698994 13.20818118 12.39604745 12.19674886\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18 -1.87627691e-17  2.23339865e-17  1.00382665e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/25/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.23.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/25/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.23.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/25/hyperscanning_gc_PEBE-STPH_solo_1_epoch=43-val/norm_mse=0.23.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 17.00it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9872321486473083\n",
            "     test/class_loss        0.18403755128383636\n",
            "   test/forecast_loss       0.23059135675430298\n",
            "        test/loss           0.23060978949069977\n",
            "        test/mae            0.37656933069229126\n",
            "        test/mape            9.491512298583984\n",
            "        test/mse            0.23059137165546417\n",
            "      test/norm_mae         0.37656933069229126\n",
            "      test/norm_mse         0.23059137165546417\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.4796140193939209\n",
            "       test/smape           0.7882269620895386\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/26/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.21.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/26/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.21.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch26_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.11045897 -0.12058142 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.40830655 10.93482488 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ...  9.79579117 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.43920224 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.70064388 -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.48073776 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.56348265  0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.19582588 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -0.68676453 -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.66757082  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.65470285  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  1.29259171  1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.90133569  0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.10073837  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ... -0.32523906 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.67949681  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ... -0.17755435  0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.49728133 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.48447895 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  1.02813193  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.70810622 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.12229146  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.23228592  0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.60375175  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.74273023 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 11.99715269 11.94699925 11.85080883 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921  9.86928682\n",
            " 11.93068806 11.89939114 11.79367808 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.63590846  9.12828423 12.05575209\n",
            " 11.63939716 11.96261054 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.96765425  8.63643789 11.71539883 11.29742551\n",
            " 11.61590913 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.95039971  8.51054961 11.94941264 11.36950736 11.78133177\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.80036353  9.69273876 12.86747024 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.25216021 11.82400636 11.50908441 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158  8.64725042\n",
            " 12.11688656 10.798857   11.6021242  11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  9.30173033  8.69311485 12.25332822\n",
            " 10.68338069 11.66820011 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443  9.2346348  10.23483608 11.84921294 11.10154951\n",
            " 11.74282084 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598  9.03840838 10.18675135 11.51147703 10.96719716 11.50737969\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.08023829  9.58226092 11.42916339 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.99304668 11.33506908 11.84901884 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141 10.2976647\n",
            " 13.27774384 12.20277853 12.83705054 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.03977555 10.39463715 13.5985274\n",
            " 12.29980717 13.05854175 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.40234949 10.05417797 13.17285163 12.24764307\n",
            " 12.88071884 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702 10.15624638  8.96728021 12.20744883 11.51224922 11.94614962\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  9.75187948  8.8189279  11.80604791 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 12.41179997 11.30195552 11.50207537 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699  9.77417063\n",
            " 12.7285977  11.44445635 11.84196478 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   9.22632594  8.96505438 11.46346181\n",
            " 10.96461481 10.84702527 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.51773815  9.31238487 12.11335282 11.25800381\n",
            " 11.42653252 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.00240709 10.72385311 12.1556716  11.77264389 11.82849204\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            "  9.29551629 10.0615734  10.78465069 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.07824619 10.35848873  9.64748684 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672  9.7939879\n",
            " 11.60442643 11.27213917 10.98061171 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.77152655  9.86258568 12.35229027\n",
            " 11.15616714 11.5224307  11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247  9.24617346 10.21107356 11.92073779 10.82055414\n",
            " 11.08829494 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.37867082 10.66698994 12.29603346 12.39604745 12.19674886\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18  5.65288557e-18  2.23339865e-17  1.00382665e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/26/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.21.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/26/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.21.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/26/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.21.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 16.63it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9938418865203857\n",
            "     test/class_loss        0.16193322837352753\n",
            "   test/forecast_loss       0.21886782348155975\n",
            "        test/loss           0.21888400614261627\n",
            "        test/mae            0.3648582994937897\n",
            "        test/mape           15.980589866638184\n",
            "        test/mse            0.21886782348155975\n",
            "      test/norm_mae         0.3648582994937897\n",
            "      test/norm_mse         0.21886782348155975\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.4678286015987396\n",
            "       test/smape           0.7682896256446838\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/27/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.25.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/27/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.25.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch27_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.11045897 -0.12415218 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.40830655 11.54580482 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ... 10.26087842 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.59744232 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.5527217  -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.23147781 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.1615049   0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.74449724 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -1.2023078  -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.17450583  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.09585112  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  0.766787    1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.9460234   0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.40943929  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ...  0.08573714 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.77786927  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ...  0.0123093   0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.56042893 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.21520821 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  0.72215923  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.61810945 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.14130052  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.160581    0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.45294958  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.73939524 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.29635174 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.83474816 -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.61165532 -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ... -0.20496925  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.25987497  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.19167734  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.58129006 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ... -0.03511565  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.65108777  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.78385541  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.3806394   0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  1.00538477  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.43376052  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.4290983   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 11.99715269 12.86104958 11.85080883 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921  9.86928682\n",
            " 11.93068806 12.90250279 11.79367808 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.63590846  9.12828423 12.05575209\n",
            " 12.47919508 11.96261054 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.96765425  8.63643789 11.71539883 11.55649842\n",
            " 11.61590913 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.95039971  8.51054961 11.94941264 11.70165593 11.78133177\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.80036353  9.69273876 12.86747024 12.61101867 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.25216021 12.08176313 11.50908441 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158  8.64725042\n",
            " 12.11688656 11.52212503 11.6021242  11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  9.30173033  8.69311485 12.25332822\n",
            " 11.84272318 11.66820011 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443  9.2346348  10.23483608 11.84921294 11.57971618\n",
            " 11.74282084 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598  9.03840838 10.18675135 11.51147703 11.28135554 11.50737969\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.08023829  9.58226092 11.42916339 10.97640557 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.99304668 11.66070749 11.84901884 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141 10.2976647\n",
            " 13.27774384 12.96302158 12.83705054 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.03977555 10.39463715 13.5985274\n",
            " 13.27036878 13.05854175 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.40234949 10.05417797 13.17285163 13.25988943\n",
            " 12.88071884 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702 10.15624638  8.96728021 12.20744883 12.9980197  11.94614962\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  9.75187948  8.8189279  11.80604791 12.70660893 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 12.41179997 13.07154775 11.50207537 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699  9.77417063\n",
            " 12.7285977  12.88490919 11.84196478 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   9.22632594  8.96505438 11.46346181\n",
            " 11.71290237 10.84702527 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.51773815  9.31238487 12.11335282 11.89242115\n",
            " 11.42653252 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.00240709 10.72385311 12.1556716  12.28089028 11.82849204\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            "  9.29551629 10.0615734  10.78465069 11.28879561 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.07824619 10.54847444  9.64748684 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672  9.7939879\n",
            " 11.60442643 11.98134946 10.98061171 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.77152655  9.86258568 12.35229027\n",
            " 12.65799275 11.5224307  11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247  9.24617346 10.21107356 11.92073779 12.41134609\n",
            " 11.08829494 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.37867082 10.66698994 12.29603346 13.20818118 12.19674886\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ... -0.218151   -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.44763307  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.52705662  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  1.19495714  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  1.09950936  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.91867392  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ...  0.02095174  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.00455498 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -0.85426511 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.2107246  -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.65336241 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.15717556  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.45134272  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.12141741 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.29913098  0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.66103776 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -0.81137143 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.08766439 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.22988447  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ... -0.18139108 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.29586833 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.9257072   0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.62225867  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.41044014  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.35805477 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.41981814 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.14574059 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.89982761 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.4393136   0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.39484855  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88377332  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  2.03018623  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.2424118   2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.48525349  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.8019525   0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.3666846   0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 12.40575232 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -5.88878147e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.16312025e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -3.46789226e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  2.67387270e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -7.06521602e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -1.28965035e+00\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  9.85089061e-02\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  1.15567386e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  6.44830515e-01\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06060527e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  4.79132349e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -4.92083718e-02\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  5.78508162e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ...  5.34512759e-02\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -5.86047628e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  2.80621285e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  8.10446680e-01\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  8.19832458e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  3.48629109e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  1.35674236e-02\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -1.54439843e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  9.30916510e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  5.60984611e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  2.47694095e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ...  1.64863677e-01\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -5.26894313e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -3.22733622e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -1.15100852e+00\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ... -4.67792930e-01\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ... -4.12533962e-01\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09850227e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.36933307e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.68376645e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  6.76046962e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  8.88433669e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.52035113e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18  5.65288557e-18 -1.87627691e-17  1.00382665e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/27/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.25.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/27/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.25.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/27/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.25.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 17.27it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9944232702255249\n",
            "     test/class_loss        0.14216694235801697\n",
            "   test/forecast_loss       0.24523372948169708\n",
            "        test/loss            0.245247945189476\n",
            "        test/mae            0.38613131642341614\n",
            "        test/mape            19.6525821685791\n",
            "        test/mse            0.24523374438285828\n",
            "      test/norm_mae         0.38613131642341614\n",
            "      test/norm_mse         0.24523374438285828\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.49521851539611816\n",
            "       test/smape           0.8007398843765259\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/28/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.20.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/28/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.20.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch28_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.11045897 -0.12415218 -0.12058142 -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.40830655 11.54580482 10.93482488 10.93393114]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ... 10.26087842  9.79579117\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.59744232 -0.43920224\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.5527217  -0.70064388\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.23147781 -0.48073776\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.1615049   0.56348265\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.74449724 -0.19582588\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -1.2023078  -0.68676453\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.17450583  0.66757082\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.09585112  0.65470285\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  0.766787    1.29259171\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.9460234   0.90133569\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.40943929  0.10073837\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ...  0.08573714 -0.32523906\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.77786927  0.67949681\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ...  0.0123093  -0.17755435\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.56042893 -0.49728133\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.21520821  0.48447895\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  0.72215923  1.02813193\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.61810945  0.70810622\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.14130052 -0.12229146\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.160581   -0.23228592\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.45294958  0.60375175\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.73939524 -0.74273023\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.29635174 -0.25659488\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.83474816 -0.8560216\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.61165532 -0.6169971\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ... -0.20496925  0.03304608\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.25987497  0.72154617\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.19167734 -0.12445033\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.58129006 -0.65809992\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ... -0.03511565  0.02278944\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.65108777  0.18233828\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.78385541  0.72039167\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.3806394   0.57661115\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  1.00538477  0.63003678\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.43376052  0.19643623\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.4290983   0.2197039\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 11.99715269 12.86104958 11.94699925 11.84452559  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921  9.86928682\n",
            " 11.93068806 12.90250279 11.89939114 11.86000642  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.63590846  9.12828423 12.05575209\n",
            " 12.47919508 11.63939716 12.15679174  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.96765425  8.63643789 11.71539883 11.55649842\n",
            " 11.29742551 11.76704991  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.95039971  8.51054961 11.94941264 11.70165593 11.36950736\n",
            " 11.6031362   1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.80036353  9.69273876 12.86747024 12.61101867 11.95326588 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.25216021 12.08176313 11.82400636 11.46147175  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158  8.64725042\n",
            " 12.11688656 11.52212503 10.798857   11.21125968  1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  9.30173033  8.69311485 12.25332822\n",
            " 11.84272318 10.68338069 11.05561652  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443  9.2346348  10.23483608 11.84921294 11.57971618\n",
            " 11.10154951 11.29401002  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598  9.03840838 10.18675135 11.51147703 11.28135554 10.96719716\n",
            " 11.23066426  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.08023829  9.58226092 11.42916339 10.97640557 10.65151806 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.99304668 11.66070749 11.33506908 11.67320766  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141 10.2976647\n",
            " 13.27774384 12.96302158 12.20277853 12.6330725   1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.03977555 10.39463715 13.5985274\n",
            " 13.27036878 12.29980717 12.82895091  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.40234949 10.05417797 13.17285163 13.25988943\n",
            " 12.24764307 12.26067668  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702 10.15624638  8.96728021 12.20744883 12.9980197  11.51224922\n",
            " 11.28404435  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  9.75187948  8.8189279  11.80604791 12.70660893 10.89735725 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 12.41179997 13.07154775 11.30195552 11.1120345   1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699  9.77417063\n",
            " 12.7285977  12.88490919 11.44445635 11.36618485  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   9.22632594  8.96505438 11.46346181\n",
            " 11.71290237 10.96461481 10.96869271  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.51773815  9.31238487 12.11335282 11.89242115\n",
            " 11.25800381 11.19620962  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.00240709 10.72385311 12.1556716  12.28089028 11.77264389\n",
            " 11.74066985  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            "  9.29551629 10.0615734  10.78465069 11.28879561 11.12590103 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.07824619 10.54847444 10.35848873 10.46887137  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672  9.7939879\n",
            " 11.60442643 11.98134946 11.27213917 11.2538474   1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.77152655  9.86258568 12.35229027\n",
            " 12.65799275 11.15616714 11.19846076  1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247  9.24617346 10.21107356 11.92073779 12.41134609\n",
            " 10.82055414 10.91265827  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.37867082 10.66698994 12.29603346 13.20818118 12.39604745\n",
            " 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ... -0.218151    0.02161577\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.44763307  0.78896111\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.52705662  0.96050075\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  1.19495714  0.60905196\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  1.09950936  0.45105897\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.91867392  1.56687519\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ...  0.02095174 -0.10970938\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.00455498 -1.23362621\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -0.85426511 -1.01951074\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.2107246  -0.04179909\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.65336241 -0.66952232\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.15717556 -0.17448182\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.45134272  0.44463858\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.12141741 -0.28873708\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.29913098  0.1343048\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.66103776 -0.33257207\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -0.81137143 -1.00966246\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.08766439 -0.58464422\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.22988447  0.77176315\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ... -0.18139108  0.06358979\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.29586833 -0.50577786\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.9257072   0.75776961\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.62225867  0.83450282\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.41044014  0.67294156\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.35805477  0.12892167\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.41981814 -0.86400498\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.14574059 -0.58699881\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.89982761 -0.48965943\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.4393136  -0.09062332\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.39484855 -0.06810203\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88377332  1.88417085\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  2.03018623  1.93625584\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.2424118   2.29911966\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.48525349  0.19914422\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.8019525   0.64736523\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.3666846   1.09597421\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 12.40575232 11.47649415\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -5.88878147e-01\n",
            "   -4.06458234e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.16312025e-01\n",
            "   -5.84163741e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -3.46789226e-01\n",
            "   -5.38360195e-01 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  2.67387270e-01\n",
            "    6.98849827e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -7.06521602e-01\n",
            "   -1.27607854e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -1.28965035e+00\n",
            "   -7.53858136e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  9.85089061e-02\n",
            "    5.36514577e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  1.15567386e-01\n",
            "    6.41112167e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  6.44830515e-01\n",
            "    1.19941809e+00  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06060527e+00\n",
            "    1.06568508e+00  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  4.79132349e-01\n",
            "    1.17187053e-01 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -4.92083718e-02\n",
            "   -5.14441704e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  5.78508162e-01\n",
            "    4.84759951e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ...  5.34512759e-02\n",
            "   -1.46884682e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -5.86047628e-01\n",
            "   -4.73243848e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  2.80621285e-01\n",
            "    5.41138810e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  8.10446680e-01\n",
            "    1.08299557e+00  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  8.19832458e-01\n",
            "    9.08773191e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  3.48629109e-01\n",
            "    9.36163606e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  1.35674236e-02\n",
            "    3.07592549e-01  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -1.54439843e-01\n",
            "   -3.33165764e-01 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  9.30916510e-01\n",
            "    6.91903077e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  5.60984611e-01\n",
            "    7.71585906e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  2.47694095e-01\n",
            "    4.92588457e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ...  1.64863677e-01\n",
            "   -6.05193327e-02  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -5.26894313e-01\n",
            "   -9.57202975e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -3.22733622e-01\n",
            "   -8.14025538e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -1.15100852e+00\n",
            "   -6.33162438e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ... -4.67792930e-01\n",
            "    2.19351638e-02  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ... -4.12533962e-01\n",
            "    3.12669666e-02  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09850227e+00\n",
            "    2.09978179e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.36933307e+00\n",
            "    2.29928156e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.68376645e+00\n",
            "    2.98942839e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  6.76046962e-01\n",
            "    3.08556082e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  8.88433669e-01\n",
            "    7.06158272e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.52035113e+00\n",
            "    1.26741646e+00  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18  5.65288557e-18 -1.87627691e-17  2.23339865e-17\n",
            " -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/28/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.20.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/28/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.20.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/28/hyperscanning_gc_PEBE-STPH_solo_1_epoch=41-val/norm_mse=0.20.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 16.88it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9952483177185059\n",
            "     test/class_loss        0.1479043811559677\n",
            "   test/forecast_loss       0.19768743216991425\n",
            "        test/loss           0.19770224392414093\n",
            "        test/mae            0.3478325605392456\n",
            "        test/mape           12.841948509216309\n",
            "        test/mse            0.19768743216991425\n",
            "      test/norm_mae         0.3478325605392456\n",
            "      test/norm_mse         0.19768743216991425\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.44471821188926697\n",
            "       test/smape            0.743289589881897\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/29/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.20.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/29/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.20.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch29_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.11045897 -0.12415218 -0.12058142 -0.0936998 ]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.40830655 11.54580482 10.93482488 11.01690962]\n",
            "\n",
            "(67, 125, 29)\n",
            "(125, 1943)\n",
            "[ 1.99615518  1.97167855  3.62424836 ... 10.26087842  9.79579117\n",
            " 10.18380136]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.59744232 -0.43920224\n",
            "   -0.68986631]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.5527217  -0.70064388\n",
            "   -1.03937019]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.23147781 -0.48073776\n",
            "   -1.13372511]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.1615049   0.56348265\n",
            "    0.44975167]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.74449724 -0.19582588\n",
            "   -0.30646481]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -1.2023078  -0.68676453\n",
            "   -0.63375789]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.17450583  0.66757082\n",
            "    0.61935841]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.09585112  0.65470285\n",
            "    0.5364043 ]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  0.766787    1.29259171\n",
            "    1.10164041]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.9460234   0.90133569\n",
            "    0.72898489]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.40943929  0.10073837\n",
            "    0.0631063 ]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ...  0.08573714 -0.32523906\n",
            "   -0.30807972]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.77786927  0.67949681\n",
            "    0.85791591]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ...  0.0123093  -0.17755435\n",
            "    0.02704969]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.56042893 -0.49728133\n",
            "   -0.50252986]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.21520821  0.48447895\n",
            "   -0.28389499]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  0.72215923  1.02813193\n",
            "    0.18380515]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.61810945  0.70810622\n",
            "   -0.03312987]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.14130052 -0.12229146\n",
            "    0.76095355]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.160581   -0.23228592\n",
            "    0.99846427]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.45294958  0.60375175\n",
            "    1.38002138]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.73939524 -0.74273023\n",
            "   -1.09612502]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.29635174 -0.25659488\n",
            "   -0.35247767]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.83474816 -0.8560216\n",
            "   -1.05955144]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.61165532 -0.6169971\n",
            "   -0.16462438]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ... -0.20496925  0.03304608\n",
            "    0.24483925]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.25987497  0.72154617\n",
            "    0.46768515]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.19167734 -0.12445033\n",
            "    0.30415913]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.58129006 -0.65809992\n",
            "   -0.20511809]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ... -0.03511565  0.02278944\n",
            "    0.38095385]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.65108777  0.18233828\n",
            "    0.91300756]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.78385541  0.72039167\n",
            "    0.89956508]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.3806394   0.57661115\n",
            "    0.37945488]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  1.00538477  0.63003678\n",
            "    0.57172086]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.43376052  0.19643623\n",
            "    0.01978036]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.4290983   0.2197039\n",
            "    0.21807905]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 29)\n",
            "(125, 841)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 11.99715269 12.86104958 11.94699925 11.85080883  1.88184776\n",
            "  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705  6.33410066\n",
            "  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318   6.79552098\n",
            "  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289  6.77824521\n",
            "  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921  9.86928682\n",
            " 11.93068806 12.90250279 11.89939114 11.79367808  2.06883189  1.91661711\n",
            "  3.85233037  4.42292723  4.1374595   6.81035074  6.64573315  7.18731759\n",
            "  6.26918517  6.01881812  6.97541996  7.27111397  6.98355443  7.35192665\n",
            "  7.54824275  3.25604588  2.23328104  6.81108219  6.57736118  6.86432759\n",
            "  9.95974759  9.31834055 10.18899777  9.63590846  9.12828423 12.05575209\n",
            " 12.47919508 11.63939716 11.96261054  2.08443065  1.98596134  3.83962243\n",
            "  4.22279771  3.76795588  6.42752564  6.13132809  6.69774275  5.68827778\n",
            "  6.23725527  6.67127157  6.90169847  6.55130188  7.12823226  7.49348426\n",
            "  3.14803762  2.06132263  6.43459622  6.37124187  5.98529942  9.62134554\n",
            "  8.89269879  9.82472116  8.96765425  8.63643789 11.71539883 11.55649842\n",
            " 11.29742551 11.61590913  2.05714907  2.01596769  3.81920868  4.04543441\n",
            "  3.67401784  6.40404012  5.84459665  6.26471958  5.19789279  6.09455435\n",
            "  6.84464184  6.62593371  6.46554987  8.02217773  7.74906949  3.48674251\n",
            "  2.51977878  6.96327518  7.29676781  6.45964461  9.89411476  9.5076942\n",
            " 10.50630294  8.95039971  8.51054961 11.94941264 11.70165593 11.36950736\n",
            " 11.78133177  1.950334    1.89589981  3.86060154  4.18198789  3.57699067\n",
            "  7.10774686  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541\n",
            "  7.29071108  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978\n",
            "  7.45958959  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541\n",
            "  9.80036353  9.69273876 12.86747024 12.61101867 11.95326588 12.28790978\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.25216021 12.08176313 11.82400636 11.50908441  1.99033952\n",
            "  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896   6.32339266\n",
            "  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582  7.29277914\n",
            "  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263  7.71857688\n",
            "  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158  8.64725042\n",
            " 12.11688656 11.52212503 10.798857   11.6021242   1.80121598  2.07652659\n",
            "  3.7605136   4.24542613  4.06859539  7.65918282  6.69040362  7.39731648\n",
            "  6.49430049  6.3643217   7.48429195  7.52232488  7.37332257  8.0513519\n",
            "  7.98824847  3.57452942  2.41812451  7.88520733  8.11533146  6.89586103\n",
            " 11.1710355  10.47776898 11.03357716  9.30173033  8.69311485 12.25332822\n",
            " 11.84272318 10.68338069 11.66820011  1.82791145  2.09208537  3.50735831\n",
            "  4.15594171  4.09518794  6.79583825  6.56195016  6.97421609  6.66097041\n",
            "  6.61589602  8.10152145  8.22978585  8.02613303  8.7729156   8.75035593\n",
            "  3.67001391  2.3804257   7.02003142  7.17652939  6.85871531 10.67842269\n",
            " 10.13450527 10.45496443  9.2346348  10.23483608 11.84921294 11.57971618\n",
            " 11.10154951 11.74282084  1.93100064  2.02021931  3.74552906  4.01431827\n",
            "  3.97064439  6.29691915  5.96857677  6.26159764  5.99635367  6.45736633\n",
            "  7.47985335  7.62232341  7.55674052  8.52537525  8.6450095   3.47750314\n",
            "  2.26931219  7.60395089  7.21287068  6.67717042 11.34364005  9.72940255\n",
            " 10.02639598  9.03840838 10.18675135 11.51147703 11.28135554 10.96719716\n",
            " 11.50737969  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736\n",
            "  6.45863912  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856\n",
            "  6.95333873  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933\n",
            "  7.53669674  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353\n",
            "  9.08023829  9.58226092 11.42916339 10.97640557 10.65151806 11.19972809\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.99304668 11.66070749 11.33506908 11.84901884  1.84477013\n",
            "  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485  5.98492494\n",
            "  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852  7.75480676\n",
            "  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184  7.80570743\n",
            "  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141 10.2976647\n",
            " 13.27774384 12.96302158 12.20277853 12.83705054  1.78979467  2.17622104\n",
            "  4.0068727   4.63627272  4.03731734  6.53891699  5.97634515  7.03584538\n",
            "  5.99649078  6.13472289  7.61490571  7.5650414   7.68145861  9.10214271\n",
            "  8.94473244  3.75972078  2.62109792  7.95489467  7.2869975   7.16152792\n",
            " 11.67771207 10.99031344 11.01307477 10.03977555 10.39463715 13.5985274\n",
            " 13.27036878 12.29980717 13.05854175  1.80468748  2.1451755   4.0995777\n",
            "  4.64108936  3.72679634  6.67297214  6.51383983  7.45839902  6.05540903\n",
            "  6.45660766  7.91017549  7.81849634  7.67847763  9.25570329  8.85866284\n",
            "  3.61671009  2.40130729  8.13099911  7.48708786  7.43859304 11.75584735\n",
            " 11.03327799 11.31430319 10.40234949 10.05417797 13.17285163 13.25988943\n",
            " 12.24764307 12.88071884  1.90395885  2.12960838  3.95577557  4.49204244\n",
            "  4.14381524  7.24982919  6.50271528  7.36651585  6.342633    6.68792736\n",
            "  7.93570197  7.86236322  7.88793578  9.42768574  9.08173909  3.40585311\n",
            "  2.34139841  8.10039798  8.03019026  7.38740474 10.9645403  10.58078801\n",
            " 11.55560702 10.15624638  8.96728021 12.20744883 12.9980197  11.51224922\n",
            " 11.94614962  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508\n",
            "  6.03673841  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181\n",
            "  7.22216747  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362\n",
            "  7.43157486  7.90466022  7.03058532 10.27483583  9.68283273 11.007388\n",
            "  9.75187948  8.8189279  11.80604791 12.70660893 10.89735725 11.21663075\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 12.41179997 13.07154775 11.30195552 11.50207537  1.74263235\n",
            "  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487  6.68735852\n",
            "  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508  8.08984167\n",
            "  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427   7.78653692\n",
            "  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699  9.77417063\n",
            " 12.7285977  12.88490919 11.44445635 11.84196478  1.82340721  2.08091064\n",
            "  3.89253523  4.76246475  4.43146198  5.89471786  6.11866813  7.6460129\n",
            "  6.50855249  6.13933745  7.48120544  8.0305169   8.01999366  8.51148361\n",
            "  8.75731073  3.63449301  2.29258968  6.75669259  7.47531682  7.08705483\n",
            "  9.56218281  9.24274512  9.9277007   9.22632594  8.96505438 11.46346181\n",
            " 11.71290237 10.96461481 10.84702527  1.85265466  2.04588677  3.76453158\n",
            "  4.17742994  4.22585425  5.89983552  5.50978835  7.02875411  5.97987863\n",
            "  5.82679902  6.70321513  7.12044411  6.91637158  7.64008616  7.76636463\n",
            "  3.62038724  2.39178626  7.52954389  7.48654883  6.90144591 10.44061308\n",
            " 10.13180395 10.29338773  9.51773815  9.31238487 12.11335282 11.89242115\n",
            " 11.25800381 11.42653252  1.77877023  1.90417067  3.78619061  4.28783371\n",
            "  4.01827456  6.26846414  5.89338791  7.14622311  6.11958558  6.41843992\n",
            "  6.8310591   7.06351754  6.70989625  7.65707323  7.59317665  3.56359132\n",
            "  2.34372216  7.16470528  7.40622305  6.9304806  11.30085323 10.12922744\n",
            " 10.47519558 10.00240709 10.72385311 12.1556716  12.28089028 11.77264389\n",
            " 11.82849204  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027\n",
            "  6.78068766  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938\n",
            "  7.32308781  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383\n",
            "  6.77399259  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168\n",
            "  9.29551629 10.0615734  10.78465069 11.28879561 11.12590103 10.51536922\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.07824619 10.54847444 10.35848873  9.64748684  1.83425429\n",
            "  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219  6.01757752\n",
            "  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096  6.59436719\n",
            "  7.01664582  7.28995495  3.83778333  2.51717108  7.992437    7.55642944\n",
            "  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672  9.7939879\n",
            " 11.60442643 11.98134946 11.27213917 10.98061171  1.92155723  1.82960492\n",
            "  3.96447411  4.4602094   3.98225844  5.43861127  6.21333084  7.08304519\n",
            "  5.88378244  6.05876977  6.92338628  7.24032222  6.81278692  7.63072764\n",
            "  7.68090723  3.72236349  2.39430203  7.70540307  6.88671585  6.50260885\n",
            " 11.29525907 10.59755963 10.97248782  9.77152655  9.86258568 12.35229027\n",
            " 12.65799275 11.15616714 11.5224307   1.89743321  1.74721883  4.21425752\n",
            "  4.99200531  4.55385035  6.14514044  6.4456965   7.63585205  6.22354469\n",
            "  6.3173814   7.16293758  7.60438664  7.19649234  7.87391972  8.08140706\n",
            "  3.70057102  2.2049803   6.98808832  6.95730381  6.64506007 10.75521731\n",
            " 10.25717163 10.82306247  9.24617346 10.21107356 11.92073779 12.41134609\n",
            " 10.82055414 11.08829494  1.72028084  1.89636765  4.03164913  4.63303388\n",
            "  4.17626539  6.64726211  6.43809779  7.25837738  5.85767341  6.20732657\n",
            "  6.87061902  7.07470772  7.04337365  7.45866082  7.81708581  3.54552947\n",
            "  2.43026236  7.2873142   8.02621394  7.34897411 10.77711788 10.77382234\n",
            " 11.8624731  10.37867082 10.66698994 12.29603346 13.20818118 12.39604745\n",
            " 12.19674886]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ... -0.218151    0.02161577\n",
            "   -0.04840784]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.44763307  0.78896111\n",
            "    0.4729229 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.52705662  0.96050075\n",
            "    0.48122952]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  1.19495714  0.60905196\n",
            "    0.85280443]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  1.09950936  0.45105897\n",
            "    0.6726741 ]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.91867392  1.56687519\n",
            "    1.30855821]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ...  0.02095174 -0.10970938\n",
            "    0.60726968]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.00455498 -1.23362621\n",
            "   -0.41914534]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -0.85426511 -1.01951074\n",
            "   -0.19201651]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.2107246  -0.04179909\n",
            "   -0.11927994]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.65336241 -0.66952232\n",
            "   -0.34653857]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.15717556 -0.17448182\n",
            "    0.53891529]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.45134272  0.44463858\n",
            "    0.12338367]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.12141741 -0.28873708\n",
            "   -0.67130338]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.29913098  0.1343048\n",
            "    0.0105088 ]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.66103776 -0.33257207\n",
            "   -1.01321802]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -0.81137143 -1.00966246\n",
            "   -1.1797325 ]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.08766439 -0.58464422\n",
            "   -0.66160011]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.22988447  0.77176315\n",
            "    0.17062976]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ... -0.18139108  0.06358979\n",
            "   -0.09552594]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.29586833 -0.50577786\n",
            "   -0.11044955]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.9257072   0.75776961\n",
            "    0.35984422]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.62225867  0.83450282\n",
            "    0.60867949]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.41044014  0.67294156\n",
            "    0.67025622]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.35805477  0.12892167\n",
            "   -0.08342555]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.41981814 -0.86400498\n",
            "   -0.53201642]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.14574059 -0.58699881\n",
            "   -0.09426715]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.89982761 -0.48965943\n",
            "   -0.14109104]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.4393136  -0.09062332\n",
            "    0.46685113]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.39484855 -0.06810203\n",
            "    0.69184938]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88377332  1.88417085\n",
            "    2.38539216]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  2.03018623  1.93625584\n",
            "    2.33908689]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.2424118   2.29911966\n",
            "    2.44034651]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.48525349  0.19914422\n",
            "    0.02612233]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.8019525   0.64736523\n",
            "    0.18000188]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.3666846   1.09597421\n",
            "    0.75413417]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 29)\n",
            "(125, 2784)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 12.40575232 11.47649415\n",
            " 11.51270578]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -5.88878147e-01\n",
            "   -4.06458234e-01 -6.09305839e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.16312025e-01\n",
            "   -5.84163741e-01 -9.14374790e-01]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -3.46789226e-01\n",
            "   -5.38360195e-01 -1.19109670e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  2.67387270e-01\n",
            "    6.98849827e-01  5.52231656e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -7.06521602e-01\n",
            "   -1.27607854e-01 -2.32494763e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -1.28965035e+00\n",
            "   -7.53858136e-01 -6.96881305e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  9.85089061e-02\n",
            "    5.36514577e-01  5.59362087e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  1.15567386e-01\n",
            "    6.41112167e-01  5.67122878e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  6.44830515e-01\n",
            "    1.19941809e+00  9.97165055e-01]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06060527e+00\n",
            "    1.06568508e+00  7.76438608e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  4.79132349e-01\n",
            "    1.17187053e-01  3.57571424e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -4.92083718e-02\n",
            "   -5.14441704e-01 -5.26805145e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  5.78508162e-01\n",
            "    4.84759951e-01  5.93922738e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ...  5.34512759e-02\n",
            "   -1.46884682e-01  1.02356834e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -5.86047628e-01\n",
            "   -4.73243848e-01 -5.28088921e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  2.80621285e-01\n",
            "    5.41138810e-01 -1.21698452e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  8.10446680e-01\n",
            "    1.08299557e+00  3.75752986e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  8.19832458e-01\n",
            "    9.08773191e-01  2.75951906e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  3.48629109e-01\n",
            "    9.36163606e-01  1.98023699e-01]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  1.35674236e-02\n",
            "    3.07592549e-01  3.32496541e-02]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -1.54439843e-01\n",
            "   -3.33165764e-01  1.17418958e-02]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  9.30916510e-01\n",
            "    6.91903077e-01  2.72633239e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  5.60984611e-01\n",
            "    7.71585906e-01  5.60309016e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  2.47694095e-01\n",
            "    4.92588457e-01  5.79955354e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ...  1.64863677e-01\n",
            "   -6.05193327e-02 -1.81864855e-01]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -5.26894313e-01\n",
            "   -9.57202975e-01 -6.06257228e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -3.22733622e-01\n",
            "   -8.14025538e-01 -2.44528277e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -1.15100852e+00\n",
            "   -6.33162438e-01 -2.16975752e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ... -4.67792930e-01\n",
            "    2.19351638e-02  5.61632967e-01]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ... -4.12533962e-01\n",
            "    3.12669666e-02  8.20480561e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09850227e+00\n",
            "    2.09978179e+00  2.54997038e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.36933307e+00\n",
            "    2.29928156e+00  2.69748653e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.68376645e+00\n",
            "    2.98942839e+00  2.92106022e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  6.76046962e-01\n",
            "    3.08556082e-01  1.37910195e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  8.88433669e-01\n",
            "    7.06158272e-01  2.37696664e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.52035113e+00\n",
            "    1.26741646e+00  8.63850083e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 29)\n",
            "(29, 125, 29)\n",
            "(96, 125, 29)\n",
            "yc.shape: (96, 125, 29)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18  5.65288557e-18 -1.87627691e-17  2.23339865e-17\n",
            "  1.00382665e-17]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/29/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.20.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/29/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.20.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/29/hyperscanning_gc_PEBE-STPH_solo_1_epoch=42-val/norm_mse=0.20.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 16.97it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9991171360015869\n",
            "     test/class_loss        0.10921982675790787\n",
            "   test/forecast_loss       0.19876952469348907\n",
            "        test/loss           0.19878041744232178\n",
            "        test/mae            0.3492695391178131\n",
            "        test/mape            16.21294593811035\n",
            "        test/mse            0.19876952469348907\n",
            "      test/norm_mae         0.3492695391178131\n",
            "      test/norm_mse         0.19876952469348907\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.4457748532295227\n",
            "       test/smape           0.7432766556739807\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: hyperscanning_gc_PEBE-STPH_solo_1\n",
            "File list:  ['./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt\n",
            "./data/eeg_hyperscanning/hyperscanning_gc_PEBE-STPH_solo_1/ch30_sub_PEBE-STPH_cond_solo.pkl\n",
            "(96, 125, 30)\n",
            "yc.shape: (96, 125, 30)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.00156618 -0.00029809 -0.02160312  0.00846223 -0.02622011 -0.06639924\n",
            " -0.031008   -0.02252487 -0.03509717 -0.07913491 -0.0623871  -0.05262879\n",
            " -0.05373189 -0.08255141 -0.07263106  0.00116828 -0.02306956 -0.09284131\n",
            " -0.07491422 -0.07531845 -0.08017608 -0.11246881 -0.12396803 -0.10735687\n",
            " -0.10389625 -0.11045897 -0.12415218 -0.12058142 -0.0936998  -0.10724216]\n",
            "std_dev yc: [ 1.98445584  2.08724335  3.98084374  4.62799829  3.99251462  6.82331246\n",
            "  6.40646348  7.36963483  6.12677348  6.41393267  7.33288502  7.36928014\n",
            "  7.2284064   8.13185953  8.02995219  3.53693138  2.35331518  7.06569612\n",
            "  7.36873553  6.75222746 10.00660401  9.69212249 10.40833894  9.37359332\n",
            "  9.04492073 11.40830655 11.54580482 10.93482488 11.01690962 10.93393114]\n",
            "\n",
            "(67, 125, 30)\n",
            "(125, 2010)\n",
            "[ 1.99615518  1.97167855  3.62424836 ...  9.79579117 10.18380136\n",
            "  9.82548272]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[ 0.80345035  1.14351516  1.14754979 ... -0.43920224 -0.68986631\n",
            "   -0.52749841]\n",
            "  [ 1.23869616  0.78954972  1.77618968 ... -0.70064388 -1.03937019\n",
            "   -1.17119664]\n",
            "  [ 0.28688161  0.52677681 -0.24257303 ... -0.48073776 -1.13372511\n",
            "   -0.99915994]\n",
            "  ...\n",
            "  [-1.34876585  0.95308801 -0.72301384 ...  0.56348265  0.44975167\n",
            "    0.65930943]\n",
            "  [-2.24911122  0.84773527 -0.97546226 ... -0.19582588 -0.30646481\n",
            "   -0.22655226]\n",
            "  [-2.80140953 -0.11610288 -1.31646888 ... -0.68676453 -0.63375789\n",
            "   -0.70086535]]\n",
            "\n",
            " [[ 0.42920549 -1.91953494  0.39989383 ...  0.66757082  0.61935841\n",
            "    0.74794459]\n",
            "  [-0.01489738 -1.77661719 -0.04455078 ...  0.65470285  0.5364043\n",
            "    0.77235979]\n",
            "  [-1.01485354 -0.09291645 -0.47317359 ...  1.29259171  1.10164041\n",
            "    1.50488202]\n",
            "  ...\n",
            "  [-0.79837551 -1.26438947 -0.49008422 ...  0.90133569  0.72898489\n",
            "    0.67233484]\n",
            "  [-0.26932623 -0.24723491 -0.49718466 ...  0.10073837  0.0631063\n",
            "   -0.09666265]\n",
            "  [-0.08431578  0.19693118 -0.05340281 ... -0.32523906 -0.30807972\n",
            "   -0.59449282]]\n",
            "\n",
            " [[-0.46562285  0.4783205   0.12713275 ...  0.67949681  0.85791591\n",
            "    0.72778522]\n",
            "  [ 0.52296154  0.37164542 -0.22460294 ... -0.17755435  0.02704969\n",
            "   -0.30359894]\n",
            "  [ 1.70020105  0.01211772  0.5676872  ... -0.49728133 -0.50252986\n",
            "   -0.57614136]\n",
            "  ...\n",
            "  [-1.51756962  0.33951385 -1.43712462 ...  0.48447895 -0.28389499\n",
            "    0.5164231 ]\n",
            "  [-2.21452713  2.55561988 -0.94210593 ...  1.02813193  0.18380515\n",
            "    0.72252663]\n",
            "  [-1.92685899  1.66406773 -0.95193463 ...  0.70810622 -0.03312987\n",
            "    0.16132314]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.3681556  -0.7551306   0.6210811  ... -0.12229146  0.76095355\n",
            "    0.24033179]\n",
            "  [ 0.2326752  -0.2264799  -0.03136328 ... -0.23228592  0.99846427\n",
            "    0.296148  ]\n",
            "  [ 0.24113885  0.06176883  0.08417612 ...  0.60375175  1.38002138\n",
            "    1.01714432]\n",
            "  ...\n",
            "  [ 0.31494825  0.88085085 -0.30136721 ... -0.74273023 -1.09612502\n",
            "   -1.17015642]\n",
            "  [ 0.07605031  0.74629948  0.23772203 ... -0.25659488 -0.35247767\n",
            "   -0.48510791]\n",
            "  [-0.02926054  0.29944394  0.44130618 ... -0.8560216  -1.05955144\n",
            "   -0.97669641]]\n",
            "\n",
            " [[ 1.22488452 -0.40268044 -0.0513374  ... -0.6169971  -0.16462438\n",
            "   -0.48325025]\n",
            "  [-0.17568638  0.32761919 -0.81697816 ...  0.03304608  0.24483925\n",
            "    0.13190509]\n",
            "  [ 0.41933398  0.37925264 -0.37068316 ...  0.72154617  0.46768515\n",
            "    0.80483825]\n",
            "  ...\n",
            "  [ 0.90378175  0.6170323   1.15412391 ... -0.12445033  0.30415913\n",
            "    0.16000753]\n",
            "  [ 0.72940095 -0.15079417 -0.25016528 ... -0.65809992 -0.20511809\n",
            "   -0.31430337]\n",
            "  [-0.1265327  -0.8782117  -1.0900803  ...  0.02278944  0.38095385\n",
            "    0.21421635]]\n",
            "\n",
            " [[-0.61147086 -0.48752088  0.01223407 ...  0.18233828  0.91300756\n",
            "    0.46627347]\n",
            "  [-0.2755587   0.88768104 -0.68587643 ...  0.72039167  0.89956508\n",
            "    1.23359711]\n",
            "  [-0.80513111  0.42283657 -1.95971553 ...  0.57661115  0.37945488\n",
            "    0.80452531]\n",
            "  ...\n",
            "  [-0.09820933 -0.42558025  0.3518697  ...  0.63003678  0.57172086\n",
            "    0.76186755]\n",
            "  [-0.84487373 -0.50313162 -0.44929788 ...  0.19643623  0.01978036\n",
            "    0.16935726]\n",
            "  [-0.3595335  -0.87145917 -0.30254352 ...  0.2197039   0.21807905\n",
            "    0.02397552]]]---------------------\n",
            "\n",
            "\n",
            "(29, 125, 30)\n",
            "(125, 870)\n",
            "[ 1.63749729  2.17991407  3.98292178  4.55207963  4.12817832  6.73994271\n",
            "  6.30087511  6.83267129  5.71406856  5.9160772   6.77355794  6.80099081\n",
            "  6.73321803  7.65132061  7.68576555  3.83114629  2.60028543  7.14345104\n",
            "  7.10395341  6.32603154 10.46605824 10.08473069 10.51785464  9.43670267\n",
            "  9.59859439 11.99715269 12.86104958 11.94699925 11.85080883 11.84452559\n",
            "  1.88184776  1.82590576  3.88665007  4.41783522  3.94396535  6.78279705\n",
            "  6.33410066  6.85799808  5.89872365  5.71133148  6.8000369   6.9180318\n",
            "  6.79552098  7.43447229  7.73579878  3.6668011   2.25979695  6.86669289\n",
            "  6.77824521  6.8794388   9.70358542  9.64561497 10.21483119  9.70085921\n",
            "  9.86928682 11.93068806 12.90250279 11.89939114 11.79367808 11.86000642\n",
            "  2.06883189  1.91661711  3.85233037  4.42292723  4.1374595   6.81035074\n",
            "  6.64573315  7.18731759  6.26918517  6.01881812  6.97541996  7.27111397\n",
            "  6.98355443  7.35192665  7.54824275  3.25604588  2.23328104  6.81108219\n",
            "  6.57736118  6.86432759  9.95974759  9.31834055 10.18899777  9.63590846\n",
            "  9.12828423 12.05575209 12.47919508 11.63939716 11.96261054 12.15679174\n",
            "  2.08443065  1.98596134  3.83962243  4.22279771  3.76795588  6.42752564\n",
            "  6.13132809  6.69774275  5.68827778  6.23725527  6.67127157  6.90169847\n",
            "  6.55130188  7.12823226  7.49348426  3.14803762  2.06132263  6.43459622\n",
            "  6.37124187  5.98529942  9.62134554  8.89269879  9.82472116  8.96765425\n",
            "  8.63643789 11.71539883 11.55649842 11.29742551 11.61590913 11.76704991\n",
            "  2.05714907  2.01596769  3.81920868  4.04543441  3.67401784  6.40404012\n",
            "  5.84459665  6.26471958  5.19789279  6.09455435  6.84464184  6.62593371\n",
            "  6.46554987  8.02217773  7.74906949  3.48674251  2.51977878  6.96327518\n",
            "  7.29676781  6.45964461  9.89411476  9.5076942  10.50630294  8.95039971\n",
            "  8.51054961 11.94941264 11.70165593 11.36950736 11.78133177 11.6031362\n",
            "  1.950334    1.89589981  3.86060154  4.18198789  3.57699067  7.10774686\n",
            "  6.26602632  7.04897561  5.86862842  6.64252393  7.40607541  7.29071108\n",
            "  7.49951035  8.49684817  8.48167707  3.50475392  2.44835978  7.45958959\n",
            "  7.79035475  6.74548177 10.75430724 10.3476766  11.0724541   9.80036353\n",
            "  9.69273876 12.86747024 12.61101867 11.95326588 12.28790978 11.74337612\n",
            "  1.98581383  1.96514427  3.78758666  4.28309154  3.89243725  6.92477553\n",
            "  6.30837793  7.52159143  6.37113137  7.11456193  7.40480307  7.55313911\n",
            "  7.71982316  8.49869156  8.46596635  3.32817687  2.41857809  6.94786121\n",
            "  7.19772113  6.64597071 10.5774181   9.9272883  10.53317855  9.61653816\n",
            "  9.90022743 12.25216021 12.08176313 11.82400636 11.50908441 11.46147175\n",
            "  1.99033952  2.06905127  3.63894923  4.34275904  3.91249519  6.6389896\n",
            "  6.32339266  7.54892827  6.2614098   6.12671809  7.0363462   7.29827582\n",
            "  7.29277914  8.02618571  7.92485093  3.29108353  2.29328472  7.14938263\n",
            "  7.71857688  6.91076451 10.78394997  9.89473252 10.78793142  9.48844158\n",
            "  8.64725042 12.11688656 11.52212503 10.798857   11.6021242  11.21125968\n",
            "  1.80121598  2.07652659  3.7605136   4.24542613  4.06859539  7.65918282\n",
            "  6.69040362  7.39731648  6.49430049  6.3643217   7.48429195  7.52232488\n",
            "  7.37332257  8.0513519   7.98824847  3.57452942  2.41812451  7.88520733\n",
            "  8.11533146  6.89586103 11.1710355  10.47776898 11.03357716  9.30173033\n",
            "  8.69311485 12.25332822 11.84272318 10.68338069 11.66820011 11.05561652\n",
            "  1.82791145  2.09208537  3.50735831  4.15594171  4.09518794  6.79583825\n",
            "  6.56195016  6.97421609  6.66097041  6.61589602  8.10152145  8.22978585\n",
            "  8.02613303  8.7729156   8.75035593  3.67001391  2.3804257   7.02003142\n",
            "  7.17652939  6.85871531 10.67842269 10.13450527 10.45496443  9.2346348\n",
            " 10.23483608 11.84921294 11.57971618 11.10154951 11.74282084 11.29401002\n",
            "  1.93100064  2.02021931  3.74552906  4.01431827  3.97064439  6.29691915\n",
            "  5.96857677  6.26159764  5.99635367  6.45736633  7.47985335  7.62232341\n",
            "  7.55674052  8.52537525  8.6450095   3.47750314  2.26931219  7.60395089\n",
            "  7.21287068  6.67717042 11.34364005  9.72940255 10.02639598  9.03840838\n",
            " 10.18675135 11.51147703 11.28135554 10.96719716 11.50737969 11.23066426\n",
            "  2.02273483  1.94521449  3.85337991  3.9995029   3.91197736  6.45863912\n",
            "  5.99389828  6.19910846  5.45535851  6.28579277  7.38453856  6.95333873\n",
            "  6.9871443   8.30922629  8.26094131  3.5156659   2.34922933  7.53669674\n",
            "  7.12525832  6.6795391  10.91918669  9.75232803  9.45033353  9.08023829\n",
            "  9.58226092 11.42916339 10.97640557 10.65151806 11.19972809 11.16063367\n",
            "  1.87332923  2.03938332  4.08618222  4.33486489  4.12871243  6.68807307\n",
            "  6.44520604  6.69495186  6.19505884  6.60538035  7.8342273   7.34055916\n",
            "  7.58044089  8.91464856  8.74211581  3.56482173  2.51660882  7.39223594\n",
            "  7.23982875  7.19699208 11.4462333  10.17581993 10.48447683 10.05685314\n",
            " 10.0722999  11.99304668 11.66070749 11.33506908 11.84901884 11.67320766\n",
            "  1.84477013  2.22284795  4.35790202  4.36446437  4.15522218  6.80672485\n",
            "  5.98492494  6.78828853  6.25274911  6.34289713  7.63881018  7.54062852\n",
            "  7.75480676  9.13061808  8.99421994  3.68782743  2.69030252  8.00277184\n",
            "  7.80570743  7.64970801 11.66616226 11.01346305 11.40696035 10.47832141\n",
            " 10.2976647  13.27774384 12.96302158 12.20277853 12.83705054 12.6330725\n",
            "  1.78979467  2.17622104  4.0068727   4.63627272  4.03731734  6.53891699\n",
            "  5.97634515  7.03584538  5.99649078  6.13472289  7.61490571  7.5650414\n",
            "  7.68145861  9.10214271  8.94473244  3.75972078  2.62109792  7.95489467\n",
            "  7.2869975   7.16152792 11.67771207 10.99031344 11.01307477 10.03977555\n",
            " 10.39463715 13.5985274  13.27036878 12.29980717 13.05854175 12.82895091\n",
            "  1.80468748  2.1451755   4.0995777   4.64108936  3.72679634  6.67297214\n",
            "  6.51383983  7.45839902  6.05540903  6.45660766  7.91017549  7.81849634\n",
            "  7.67847763  9.25570329  8.85866284  3.61671009  2.40130729  8.13099911\n",
            "  7.48708786  7.43859304 11.75584735 11.03327799 11.31430319 10.40234949\n",
            " 10.05417797 13.17285163 13.25988943 12.24764307 12.88071884 12.26067668\n",
            "  1.90395885  2.12960838  3.95577557  4.49204244  4.14381524  7.24982919\n",
            "  6.50271528  7.36651585  6.342633    6.68792736  7.93570197  7.86236322\n",
            "  7.88793578  9.42768574  9.08173909  3.40585311  2.34139841  8.10039798\n",
            "  8.03019026  7.38740474 10.9645403  10.58078801 11.55560702 10.15624638\n",
            "  8.96728021 12.20744883 12.9980197  11.51224922 11.94614962 11.28404435\n",
            "  1.83003168  2.06499481  3.65896155  4.52782616  4.13830508  6.03673841\n",
            "  5.81881013  7.1585711   6.12759683  6.61312419  7.28334181  7.22216747\n",
            "  7.3845138   8.4625487   8.34909441  3.33040041  2.43952362  7.43157486\n",
            "  7.90466022  7.03058532 10.27483583  9.68283273 11.007388    9.75187948\n",
            "  8.8189279  11.80604791 12.70660893 10.89735725 11.21663075 10.63696357\n",
            "  1.79275141  1.78688213  3.89667765  4.63911609  3.77218567  6.41184254\n",
            "  6.17016867  7.32682598  5.85074808  6.46879177  7.47219931  7.41820586\n",
            "  7.39934157  8.32359213  8.44539895  3.7019145   2.49032681  7.43522766\n",
            "  8.14080947  7.35388699 10.11509294 10.03732746 11.43903406 10.52146353\n",
            "  9.46570555 12.41179997 13.07154775 11.30195552 11.50207537 11.1120345\n",
            "  1.74263235  2.03934787  4.09826602  5.11214444  4.30394001  6.62897487\n",
            "  6.68735852  7.87723699  6.54565697  5.93787362  7.9671577   8.20988508\n",
            "  8.08984167  8.90245998  8.95740618  3.82820808  2.47354365  7.5996427\n",
            "  7.78653692  7.21437229 10.22894679 10.14022935 10.79343131 10.15869699\n",
            "  9.77417063 12.7285977  12.88490919 11.44445635 11.84196478 11.36618485\n",
            "  1.82340721  2.08091064  3.89253523  4.76246475  4.43146198  5.89471786\n",
            "  6.11866813  7.6460129   6.50855249  6.13933745  7.48120544  8.0305169\n",
            "  8.01999366  8.51148361  8.75731073  3.63449301  2.29258968  6.75669259\n",
            "  7.47531682  7.08705483  9.56218281  9.24274512  9.9277007   9.22632594\n",
            "  8.96505438 11.46346181 11.71290237 10.96461481 10.84702527 10.96869271\n",
            "  1.85265466  2.04588677  3.76453158  4.17742994  4.22585425  5.89983552\n",
            "  5.50978835  7.02875411  5.97987863  5.82679902  6.70321513  7.12044411\n",
            "  6.91637158  7.64008616  7.76636463  3.62038724  2.39178626  7.52954389\n",
            "  7.48654883  6.90144591 10.44061308 10.13180395 10.29338773  9.51773815\n",
            "  9.31238487 12.11335282 11.89242115 11.25800381 11.42653252 11.19620962\n",
            "  1.77877023  1.90417067  3.78619061  4.28783371  4.01827456  6.26846414\n",
            "  5.89338791  7.14622311  6.11958558  6.41843992  6.8310591   7.06351754\n",
            "  6.70989625  7.65707323  7.59317665  3.56359132  2.34372216  7.16470528\n",
            "  7.40622305  6.9304806  11.30085323 10.12922744 10.47519558 10.00240709\n",
            " 10.72385311 12.1556716  12.28089028 11.77264389 11.82849204 11.74066985\n",
            "  1.84972155  2.05378291  3.6519845   4.76588686  4.3069027   6.78068766\n",
            "  6.27379331  7.45430979  6.34278343  6.3773157   7.15537938  7.32308781\n",
            "  7.05289155  8.26897033  8.21158882  3.29156185  2.26540383  6.77399259\n",
            "  6.82865924  6.62601899  9.3412566   9.20525525  9.71183168  9.29551629\n",
            " 10.0615734  10.78465069 11.28879561 11.12590103 10.51536922 11.09080376\n",
            "  1.9277153   1.99935477  3.84057869  4.9114598   4.02295482  6.27216876\n",
            "  6.1108172   7.17122055  5.9933363   6.13796481  6.67652218  6.90512267\n",
            "  6.72238779  7.52800774  7.71882504  3.49340597  2.22667436  7.29838457\n",
            "  7.35900686  6.11964774  9.08661481  9.0432498   9.79014488  8.88849634\n",
            "  9.41367867 10.07824619 10.54847444 10.35848873  9.64748684 10.46887137\n",
            "  1.83425429  1.72374111  3.74760072  4.60437118  3.71016176  6.04766219\n",
            "  6.01757752  6.95730276  5.82742202  6.2279241   6.44626211  6.79570096\n",
            "  6.59436719  7.01664582  7.28995495  3.83778333  2.51717108  7.992437\n",
            "  7.55642944  6.6049084  11.02795572 10.54490904 10.77917896  9.72281672\n",
            "  9.7939879  11.60442643 11.98134946 11.27213917 10.98061171 11.2538474\n",
            "  1.92155723  1.82960492  3.96447411  4.4602094   3.98225844  5.43861127\n",
            "  6.21333084  7.08304519  5.88378244  6.05876977  6.92338628  7.24032222\n",
            "  6.81278692  7.63072764  7.68090723  3.72236349  2.39430203  7.70540307\n",
            "  6.88671585  6.50260885 11.29525907 10.59755963 10.97248782  9.77152655\n",
            "  9.86258568 12.35229027 12.65799275 11.15616714 11.5224307  11.19846076\n",
            "  1.89743321  1.74721883  4.21425752  4.99200531  4.55385035  6.14514044\n",
            "  6.4456965   7.63585205  6.22354469  6.3173814   7.16293758  7.60438664\n",
            "  7.19649234  7.87391972  8.08140706  3.70057102  2.2049803   6.98808832\n",
            "  6.95730381  6.64506007 10.75521731 10.25717163 10.82306247  9.24617346\n",
            " 10.21107356 11.92073779 12.41134609 10.82055414 11.08829494 10.91265827\n",
            "  1.72028084  1.89636765  4.03164913  4.63303388  4.17626539  6.64726211\n",
            "  6.43809779  7.25837738  5.85767341  6.20732657  6.87061902  7.07470772\n",
            "  7.04337365  7.45866082  7.81708581  3.54552947  2.43026236  7.2873142\n",
            "  8.02621394  7.34897411 10.77711788 10.77382234 11.8624731  10.37867082\n",
            " 10.66698994 12.29603346 13.20818118 12.39604745 12.19674886 12.40762083]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.20105115 -0.58037833  0.28598359 ...  0.02161577 -0.04840784\n",
            "    0.1008345 ]\n",
            "  [-0.31840518  0.21529499  0.39089251 ...  0.78896111  0.4729229\n",
            "    0.4733517 ]\n",
            "  [-0.1317455  -0.73128518  0.58327193 ...  0.96050075  0.48122952\n",
            "    0.6992402 ]\n",
            "  ...\n",
            "  [ 2.31051553  0.1478731   1.2677325  ...  0.60905196  0.85280443\n",
            "    0.5790526 ]\n",
            "  [ 1.57906025  0.08505366  0.5647718  ...  0.45105897  0.6726741\n",
            "    0.27471561]\n",
            "  [ 1.15909272 -0.03629758  0.29787782 ...  1.56687519  1.30855821\n",
            "    1.40064889]]\n",
            "\n",
            " [[-0.60095912 -0.78518848 -0.35157977 ... -0.10970938  0.60726968\n",
            "    0.35907877]\n",
            "  [-1.66258375 -1.15334371 -0.78976418 ... -1.23362621 -0.41914534\n",
            "   -1.00933577]\n",
            "  [-0.42556483 -1.29509609  0.00905861 ... -1.01951074 -0.19201651\n",
            "   -0.54135919]\n",
            "  ...\n",
            "  [ 0.76053168 -0.4546655   0.4551215  ... -0.04179909 -0.11927994\n",
            "   -0.18640866]\n",
            "  [ 0.29993129 -1.49573869  0.29414515 ... -0.66952232 -0.34653857\n",
            "   -0.73408612]\n",
            "  [-0.33998426 -2.35034977  0.22804436 ... -0.17448182  0.53891529\n",
            "    0.01410638]]\n",
            "\n",
            " [[ 1.26761682 -1.29561557  1.03178836 ...  0.44463858  0.12338367\n",
            "    0.56718713]\n",
            "  [ 0.96837504 -0.51715883  0.87277759 ... -0.28873708 -0.67130338\n",
            "   -0.53016069]\n",
            "  [ 0.58573531  0.09029032 -0.01708041 ...  0.1343048   0.0105088\n",
            "    0.11029977]\n",
            "  ...\n",
            "  [-0.06614683  0.68270066 -0.06680567 ... -0.33257207 -1.01321802\n",
            "   -0.59771934]\n",
            "  [ 0.57861222  0.3445865  -0.08828142 ... -1.00966246 -1.1797325\n",
            "   -1.14352712]\n",
            "  [ 1.32201982  0.03638328  0.29517212 ... -0.58464422 -0.66160011\n",
            "   -0.75567676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.40515179  0.5546163   0.13871976 ...  0.77176315  0.17062976\n",
            "    0.83762112]\n",
            "  [ 0.35512842 -0.82644591  1.08020003 ...  0.06358979 -0.09552594\n",
            "    0.20824824]\n",
            "  [ 0.71999521 -1.96642265  1.56213831 ... -0.50577786 -0.11044955\n",
            "   -0.52666847]\n",
            "  ...\n",
            "  [-2.28986311  0.88554461 -1.88792361 ...  0.75776961  0.35984422\n",
            "    0.61230443]\n",
            "  [-1.88297269  2.96058586 -1.66283991 ...  0.83450282  0.60867949\n",
            "    0.88841944]\n",
            "  [-1.42798066  0.78558171 -0.17979155 ...  0.67294156  0.67025622\n",
            "    0.70928764]]\n",
            "\n",
            " [[-1.14679257  0.33477184 -0.13874689 ...  0.12892167 -0.08342555\n",
            "    0.15006372]\n",
            "  [ 0.1167141  -1.1687454   0.72849069 ... -0.86400498 -0.53201642\n",
            "   -0.87423596]\n",
            "  [ 0.05762519 -1.36576972  0.7178405  ... -0.58699881 -0.09426715\n",
            "   -0.52870691]\n",
            "  ...\n",
            "  [-0.44083764 -0.13541105  0.73839861 ... -0.48965943 -0.14109104\n",
            "   -0.20601538]\n",
            "  [-1.00728344 -0.96159224 -1.08776337 ... -0.09062332  0.46685113\n",
            "   -0.09033548]\n",
            "  [-0.66242565 -0.92930346 -1.82317449 ... -0.06810203  0.69184938\n",
            "    0.23569701]]\n",
            "\n",
            " [[ 0.31422733 -2.11941756  0.79546681 ...  1.88417085  2.38539216\n",
            "    1.97604261]\n",
            "  [-0.52745263 -1.63620634  0.28982242 ...  1.93625584  2.33908689\n",
            "    1.88006504]\n",
            "  [-0.50899362 -0.42132701 -0.87363092 ...  2.29911966  2.44034651\n",
            "    2.08684302]\n",
            "  ...\n",
            "  [ 2.01129444 -1.1946223   0.49779683 ...  0.19914422  0.02612233\n",
            "    0.02441748]\n",
            "  [ 0.96634589 -0.65105774 -0.52781782 ...  0.64736523  0.18000188\n",
            "    0.3879767 ]\n",
            "  [ 0.28328717 -0.89692346 -0.72813537 ...  1.09597421  0.75413417\n",
            "    0.74173134]]]---------------------\n",
            "\n",
            "\n",
            "(96, 125, 30)\n",
            "(125, 2880)\n",
            "[ 2.09260572  2.28706165  3.92336892 ... 11.47649415 11.51270578\n",
            " 11.36185229]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 7.40771147e-01  9.55330201e-01  1.03334481e+00 ... -4.06458234e-01\n",
            "   -6.09305839e-01 -4.58209777e-01]\n",
            "  [ 1.16147799e+00  6.61071705e-01  1.51577999e+00 ... -5.84163741e-01\n",
            "   -9.14374790e-01 -1.01317301e+00]\n",
            "  [ 1.41277760e-01  5.20019686e-01 -4.40316082e-01 ... -5.38360195e-01\n",
            "   -1.19109670e+00 -1.04962594e+00]\n",
            "  ...\n",
            "  [-1.34511134e+00  1.00958576e+00 -7.23400401e-01 ...  6.98849827e-01\n",
            "    5.52231656e-01  7.84185233e-01]\n",
            "  [-2.24822182e+00  7.87994211e-01 -1.01140424e+00 ... -1.27607854e-01\n",
            "   -2.32494763e-01 -1.31695662e-01]\n",
            "  [-2.81829049e+00 -2.67550001e-01 -1.40790531e+00 ... -7.53858136e-01\n",
            "   -6.96881305e-01 -7.85385582e-01]]\n",
            "\n",
            " [[ 7.16089950e-01 -2.17587036e+00  5.25639348e-01 ...  5.36514577e-01\n",
            "    5.59362087e-01  5.72518333e-01]\n",
            "  [ 1.52402985e-01 -1.91439967e+00  1.16399463e-02 ...  6.41112167e-01\n",
            "    5.67122878e-01  6.96132833e-01]\n",
            "  [-1.10348504e+00 -2.54803761e-02 -4.85848203e-01 ...  1.19941809e+00\n",
            "    9.97165055e-01  1.40762169e+00]\n",
            "  ...\n",
            "  [-7.56368690e-01 -1.23061955e+00 -3.65815330e-01 ...  1.06568508e+00\n",
            "    7.76438608e-01  8.07114093e-01]\n",
            "  [-2.22858358e-01 -3.06390328e-01 -2.76368232e-01 ...  1.17187053e-01\n",
            "    3.57571424e-02 -7.52245529e-02]\n",
            "  [-5.53191782e-02  6.21052830e-02  2.44366548e-01 ... -5.14441704e-01\n",
            "   -5.26805145e-01 -7.67737323e-01]]\n",
            "\n",
            " [[-5.24516160e-01  2.36191799e-01  4.87198595e-01 ...  4.84759951e-01\n",
            "    5.93922738e-01  5.13930409e-01]\n",
            "  [ 6.89000258e-01  2.17305222e-01  1.30275225e-01 ... -1.46884682e-01\n",
            "    1.02356834e-01 -2.65441286e-01]\n",
            "  [ 1.64161496e+00 -2.09696593e-01  7.87558823e-01 ... -4.73243848e-01\n",
            "   -5.28088921e-01 -5.73114615e-01]\n",
            "  ...\n",
            "  [-1.67229628e+00  1.64098065e-01 -1.40966387e+00 ...  5.41138810e-01\n",
            "   -1.21698452e-01  6.22805955e-01]\n",
            "  [-2.08669817e+00  2.21324673e+00 -8.73735544e-01 ...  1.08299557e+00\n",
            "    3.75752986e-01  8.04992837e-01]\n",
            "  [-1.86554998e+00  1.69698357e+00 -1.01784727e+00 ...  9.08773191e-01\n",
            "    2.75951906e-01  4.23477255e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-5.63959406e-01  4.60221308e-01 -7.41317495e-02 ...  9.36163606e-01\n",
            "    1.98023699e-01  1.03983159e+00]\n",
            "  [ 3.07128513e-01 -8.21970917e-01  9.62230589e-01 ...  3.07592549e-01\n",
            "    3.32496541e-02  4.84685277e-01]\n",
            "  [ 6.80159712e-01 -1.93903584e+00  1.40406843e+00 ... -3.33165764e-01\n",
            "    1.17418958e-02 -3.76467285e-01]\n",
            "  ...\n",
            "  [-2.02257558e+00  8.52323010e-01 -1.81720331e+00 ...  6.91903077e-01\n",
            "    2.72633239e-01  4.89739859e-01]\n",
            "  [-1.50454204e+00  3.10927666e+00 -1.21493132e+00 ...  7.71585906e-01\n",
            "    5.60309016e-01  8.45739070e-01]\n",
            "  [-1.22757309e+00  7.63947356e-01  9.24245262e-02 ...  4.92588457e-01\n",
            "    5.79955354e-01  5.99676268e-01]]\n",
            "\n",
            " [[-1.20064524e+00  3.86789217e-01 -2.07519678e-02 ... -6.05193327e-02\n",
            "   -1.81864855e-01  4.26639739e-03]\n",
            "  [ 2.75299261e-02 -9.55558511e-01  7.77222335e-01 ... -9.57202975e-01\n",
            "   -6.06257228e-01 -9.66561050e-01]\n",
            "  [-8.80898992e-04 -1.24694721e+00  8.26233067e-01 ... -8.14025538e-01\n",
            "   -2.44528277e-01 -7.62877466e-01]\n",
            "  ...\n",
            "  [-4.49315615e-01 -1.51051347e-01  6.28217821e-01 ... -6.33162438e-01\n",
            "   -2.16975752e-01 -3.22046416e-01]\n",
            "  [-8.07052516e-01 -1.01403321e+00 -9.27912877e-01 ...  2.19351638e-02\n",
            "    5.61632967e-01  1.11428216e-02]\n",
            "  [-5.09146126e-01 -1.16583850e+00 -1.60317809e+00 ...  3.12669666e-02\n",
            "    8.20480561e-01  3.37439776e-01]]\n",
            "\n",
            " [[ 2.46185638e-01 -2.36860159e+00  7.04412254e-01 ...  2.09978179e+00\n",
            "    2.54997038e+00  2.09119024e+00]\n",
            "  [-4.14265570e-01 -1.51595503e+00  1.97778580e-01 ...  2.29928156e+00\n",
            "    2.69748653e+00  2.12730235e+00]\n",
            "  [-1.94909337e-01 -4.04298989e-01 -5.96143749e-01 ...  2.98942839e+00\n",
            "    2.92106022e+00  2.63202344e+00]\n",
            "  ...\n",
            "  [ 2.15992386e+00 -9.67199395e-01  6.15544306e-01 ...  3.08556082e-01\n",
            "    1.37910195e-01  1.40362237e-01]\n",
            "  [ 1.05665032e+00 -6.39079107e-01 -5.70275881e-01 ...  7.06158272e-01\n",
            "    2.37696664e-01  4.69180830e-01]\n",
            "  [ 3.80435409e-01 -9.42417130e-01 -7.61612049e-01 ...  1.26741646e+00\n",
            "    8.63850083e-01  8.81715583e-01]]]---------------------\n",
            "\n",
            "\n",
            "(67, 125, 30)\n",
            "(29, 125, 30)\n",
            "(96, 125, 30)\n",
            "yc.shape: (96, 125, 30)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.22969791e-18  9.02056208e-18  7.23495338e-18  2.22044605e-17\n",
            "  1.26380388e-17  1.98452366e-17 -1.89478063e-17 -3.70074342e-20\n",
            "  3.57121740e-18  1.79301018e-17  2.96522066e-17 -2.14643118e-18\n",
            "  2.01505479e-17 -4.57273108e-17  5.05706588e-17 -6.25425637e-18\n",
            " -2.71727085e-17  9.43689571e-18  1.43588845e-17 -1.70974346e-17\n",
            " -7.58652400e-19 -1.36187358e-17  2.22044605e-18  1.74675089e-17\n",
            " -6.47630098e-18  5.65288557e-18 -1.87627691e-17  2.23339865e-17\n",
            "  1.00382665e-17 -2.03540888e-18]\n",
            "std_dev yc: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "context_points 10\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [102, 106, 17, 105, 52, 44, 43, 33, 49, 99, 103, 112, 26, 0, 71, 20, 29, 15, 46, 8, 37, 72, 77, 85, 32, 65, 39, 56, 60, 91, 70, 22, 110, 92, 93, 109, 111, 6, 97, 81, 48, 3, 13, 98, 51, 104, 114, 59, 88, 58, 76, 55, 23, 64, 25, 61, 36, 94, 38, 101, 40, 68, 1, 9, 5, 73, 18, 12, 7, 54, 79, 21, 47, 28, 83, 34, 2, 31, 24, 42, 80, 95, 62, 14, 78, 41, 108, 45, 82, 84, 63, 57, 100, 74, 53, 75, 113, 27, 30, 66, 35, 67, 90, 50, 87, 69, 89, 10, 107, 4, 19, 16, 96, 86, 11]\n",
            "self.series.num_trials(split): 67\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 125\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -10\n",
            "self._slice_start_points: [42, 109, 111, 114, 68, 52, 58, 85, 72, 3, 7, 48, 54, 26, 56, 88, 28, 33, 4, 5, 46, 112, 93, 65, 76, 30, 97, 22, 99, 82, 75, 21, 69, 9, 8, 102, 18, 37, 90, 84, 11, 34, 55, 94, 96, 29, 49, 45, 31, 41, 104, 89, 47, 61, 36, 79, 20, 64, 101, 43, 13, 62, 63, 74, 73, 19, 71, 44, 66, 17, 32, 14, 57, 80, 2, 108, 50, 60, 87, 23, 35, 38, 25, 83, 77, 15, 106, 12, 78, 67, 98, 95, 91, 100, 27, 0, 40, 113, 86, 51, 1, 16, 92, 10, 81, 70, 24, 53, 110, 105, 103, 6, 59, 107, 39]\n",
            "self.series.num_trials(split): 67\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/hyperscanning_gc_PEBE-STPH_solo_1/checkpoints/30/hyperscanning_gc_PEBE-STPH_solo_1_epoch=44-val/norm_mse=0.23.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "Testing DataLoader 0: 100% 61/61 [00:03<00:00, 16.68it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9755223989486694\n",
            "     test/class_loss        0.3072736859321594\n",
            "   test/forecast_loss       0.2271495759487152\n",
            "        test/loss           0.2271803468465805\n",
            "        test/mae            0.37223997712135315\n",
            "        test/mape           16.454917907714844\n",
            "        test/mse            0.2271495759487152\n",
            "      test/norm_mae         0.37223997712135315\n",
            "      test/norm_mse         0.2271495759487152\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.4759294390678406\n",
            "       test/smape            0.779064416885376\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python granger_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ut4m1-Mxn_E",
        "outputId": "c4801bde-263b-481f-90ff-af4866664b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_name: generated_granger_causality_2, 4ch, enc_layers 2, dec_layers 2, heads 2, small model, bs 256, initial_downsample_convs 0, cp 15, tp 1, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none\n",
            "./plots_checkpoints_logs/generated_granger_causality_2, 4ch, enc_layers 2, dec_layers 2, heads 2, small model, bs 256, initial_downsample_convs 0, cp 15, tp 1, loss mse, global_self_attn full, global_cross_attn full, local_self_attn none, local_cross_attn none/plots/0/test/channels_loss.pkl\n",
            "Traceback (most recent call last):\n",
            "  File \"granger_test.py\", line 42, in <module>\n",
            "    channels_loss = torch.stack(new_list)\n",
            "RuntimeError: stack expects each tensor to be equal size, but got [256, 3, 3] at entry 0 and [146, 3, 3] at entry 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python train_eeg_granger_causality.py spacetimeformer eeg_social_memory --d_model 15 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --batch_size 128 --class_loss_imp 0.0001 --warmup_steps 850 --decay_factor .5 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 1 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6\n",
        "!python train_eeg_granger_causality.py spacetimeformer eeg_social_memory --d_model 15 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --batch_size 64 --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .65 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gfvtzZ8fbQ",
        "outputId": "59689f66-9f08-4754-a682-0b96813b3885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.run_name: generated_gc_SNR_standardized_1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca_maurici\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221016_175349-zwet2snq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtoasty-valley-745\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/zwet2snq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "./data/eeg_hyperscanning/eeg_generated/ch0_sub_PEBE-STPH_cond_solo.pkl\n",
            "Traceback (most recent call last):\n",
            "  File \"train_eeg_granger_causality.py\", line 577, in <module>\n",
            "    main(args)\n",
            "  File \"train_eeg_granger_causality.py\", line 486, in main\n",
            "    ) = create_dset(args, channel)\n",
            "  File \"train_eeg_granger_causality.py\", line 349, in create_dset\n",
            "    data_path=data_path\n",
            "  File \"/content/drive/MyDrive/Documenti/Scuola/Università/Sapienza/Tesi/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/data/eeg_dataset.py\", line 38, in __init__\n",
            "    assert os.path.exists(self.data_path)\n",
            "AssertionError\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1019, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1653, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1611, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_manager.py\", line 148, in _teardown\n",
            "    result = self._service.join()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/service/service.py\", line 129, in join\n",
            "    ret = self._internal_proc.wait()\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1032, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtoasty-valley-745\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/zwet2snq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221016_175349-zwet2snq/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SNR\n",
        "#!python train_eeg_granger_causality_SNR.py spacetimeformer eeg_social_memory --d_model 10 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --batch_size 64 --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .8 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6\n",
        "!python train_eeg_granger_causality_SNR.py spacetimeformer eeg_social_memory --d_model 15 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 5e-4 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --batch_size 64 --class_loss_imp 0.0001 --warmup_steps 1000 --decay_factor .5 --wandb --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6"
      ],
      "metadata": {
        "id": "rRQtSKbXbx1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f59b9a-61d7-4adf-b4ff-d8705d59442d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.296, v_num=w0i1]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.32it/s, loss=0.299, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.90it/s, loss=0.299, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.12it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.56it/s, loss=0.296, v_num=w0i1]\n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.293, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.293, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.31it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.37it/s, loss=0.305, v_num=w0i1]\n",
            "Epoch 43:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.298, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.298, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 37.44it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 20.97it/s, loss=0.297, v_num=w0i1]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.299, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.299, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.01it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.311, v_num=w0i1]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.292, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.91it/s, loss=0.292, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.42it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.283, v_num=w0i1]\n",
            "Epoch 46:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.286, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.90it/s, loss=0.286, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.14it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.53it/s, loss=0.288, v_num=w0i1]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.282, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.282, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.72it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.301, v_num=w0i1]\n",
            "Epoch 48:  69% 80/116 [00:04<00:02, 17.73it/s, loss=0.286, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.286, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.84it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 20.94it/s, loss=0.284, v_num=w0i1]\n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.292, v_num=w0i1]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.60it/s, loss=0.292, v_num=w0i1]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.81it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.19it/s, loss=0.288, v_num=w0i1]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.15it/s, loss=0.288, v_num=w0i1]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.288, v_num=w0i1]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇███████▄▄▄▄▃▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▁▁▂▄▅▆▆▇▇▇▇▇██████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▆▆▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇▅▄▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▂▁▂▁▁▁▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇▅▄▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▂▁▂▁▁▁▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae ██▆▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▂▂▁▁▂▂█▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇▅▄▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▂▁▂▁▁▁▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae ██▆▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇▅▄▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▂▁▂▁▁▁▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▆▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▁▂▁▁▂▂▁▂▂▁▂▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▆█▇▆▄▃▃▂▂▂▃▂▂▂▂▂▂▃▁▂▂▁▂▂▂▂▂▂▂▂▃▂▁▂▂▁▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▁▁▃▄▅▆▆▇▇▇▇▇██████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▆▅▅▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▆▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▆▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▇▆▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape █▂▁▃▅▇▅▅▄▄▃▄▃▃▃▃▄▃▄▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▆▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▇▆▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▆▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▇▆▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▇██▆▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.16422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.20592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.20594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.33432\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 1.39957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.20592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.33432\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.20592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.49321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 0.78295\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.18344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.29323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.29325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.40805\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.9943\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.29323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.40805\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.29323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.53201\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 0.80559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mvibrant-waterfall-812\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/2fb9w0i1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_045359-2fb9w0i1/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_045848-37fm1uh5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrisp-sky-813\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/37fm1uh5\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-7.56431954e-18 -1.81484457e-17  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-7.56431954e-18 -1.81484457e-17  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.17it/s, loss=1.1, v_num=1uh5] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.72it/s, loss=1.1, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.02it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.37it/s, loss=1.13, v_num=1uh5]\n",
            "Epoch 1:  69% 80/116 [00:04<00:01, 18.11it/s, loss=1.06, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.74it/s, loss=1.06, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.74it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.37it/s, loss=1.04, v_num=1uh5]\n",
            "Epoch 2:  69% 80/116 [00:04<00:01, 18.25it/s, loss=1.04, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.85it/s, loss=1.04, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.16it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.53it/s, loss=1.01, v_num=1uh5]\n",
            "Epoch 3:  69% 80/116 [00:04<00:01, 18.12it/s, loss=1.07, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.55it/s, loss=1.07, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.28it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.18it/s, loss=1.07, v_num=1uh5]\n",
            "Epoch 4:  69% 80/116 [00:04<00:01, 18.01it/s, loss=1.02, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.59it/s, loss=1.02, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.57it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.13it/s, loss=1.02, v_num=1uh5]\n",
            "Epoch 5:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.933, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.51it/s, loss=0.933, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.51it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.12it/s, loss=0.916, v_num=1uh5]\n",
            "Epoch 6:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.765, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.67it/s, loss=0.765, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.92it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.25it/s, loss=0.754, v_num=1uh5]\n",
            "Epoch 7:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.725, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.725, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.14it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.698, v_num=1uh5]\n",
            "Epoch 8:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.688, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.688, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.05it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.69, v_num=1uh5] \n",
            "Epoch 9:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.675, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.675, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.81it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.05it/s, loss=0.683, v_num=1uh5]\n",
            "Epoch 10:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.692, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.692, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.00it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.694, v_num=1uh5]\n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.716, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.69it/s, loss=0.716, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.62it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.741, v_num=1uh5]\n",
            "Epoch 12:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.7, v_num=1uh5] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.7, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.00it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.692, v_num=1uh5]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.676, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.676, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.60it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.658, v_num=1uh5]\n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.731, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.56it/s, loss=0.731, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.98it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.01it/s, loss=0.73, v_num=1uh5] \n",
            "Epoch 15:  69% 80/116 [00:05<00:02, 13.51it/s, loss=0.675, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:06<00:01, 15.17it/s, loss=0.675, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.91it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:06<00:00, 16.66it/s, loss=0.665, v_num=1uh5]\n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.688, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.688, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.21it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.668, v_num=1uh5]\n",
            "Epoch 17:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.674, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.68it/s, loss=0.674, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.90it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.665, v_num=1uh5]\n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.687, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.87it/s, loss=0.687, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.53it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.7, v_num=1uh5]  \n",
            "Epoch 19:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.715, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.715, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.14it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.71, v_num=1uh5] \n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.711, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.67it/s, loss=0.711, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.32it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.686, v_num=1uh5]\n",
            "Epoch 21:  69% 80/116 [00:04<00:01, 18.45it/s, loss=0.684, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:04<00:00, 20.07it/s, loss=0.684, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.98it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.68it/s, loss=0.698, v_num=1uh5]\n",
            "Epoch 22:  69% 80/116 [00:04<00:01, 18.36it/s, loss=0.702, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.702, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.50it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.722, v_num=1uh5]\n",
            "Epoch 23:  69% 80/116 [00:04<00:01, 18.48it/s, loss=0.703, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:04<00:00, 20.08it/s, loss=0.703, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.36it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.72it/s, loss=0.719, v_num=1uh5]\n",
            "Epoch 24:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.717, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.717, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.28it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.701, v_num=1uh5]\n",
            "Epoch 25:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.688, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.688, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 36.39it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.01it/s, loss=0.714, v_num=1uh5]\n",
            "Epoch 26:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.711, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.69it/s, loss=0.711, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.27it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 21.35it/s, loss=0.731, v_num=1uh5]\n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.681, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.681, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.87it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.30it/s, loss=0.679, v_num=1uh5]\n",
            "Epoch 28:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.677, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.677, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.63it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.703, v_num=1uh5]\n",
            "Epoch 29:  69% 80/116 [00:04<00:01, 18.15it/s, loss=0.672, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.672, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.40it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.44it/s, loss=0.679, v_num=1uh5]\n",
            "Epoch 30:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.677, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.88it/s, loss=0.677, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.06it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.51it/s, loss=0.69, v_num=1uh5] \n",
            "Epoch 31:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.711, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.83it/s, loss=0.711, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.00it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.704, v_num=1uh5]\n",
            "Epoch 32:  69% 80/116 [00:04<00:01, 18.30it/s, loss=0.695, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.695, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.45it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 21.57it/s, loss=0.679, v_num=1uh5]\n",
            "Epoch 33:  69% 80/116 [00:04<00:01, 18.43it/s, loss=0.675, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.675, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.84it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.56it/s, loss=0.678, v_num=1uh5]\n",
            "Epoch 34:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.696, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:04<00:00, 20.00it/s, loss=0.696, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.67it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 21.65it/s, loss=0.786, v_num=1uh5]\n",
            "Epoch 35:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.699, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.699, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.70it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.678, v_num=1uh5]\n",
            "Epoch 36:  69% 80/116 [00:04<00:01, 18.47it/s, loss=0.679, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:04<00:00, 20.03it/s, loss=0.679, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.53it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 21.72it/s, loss=0.681, v_num=1uh5]\n",
            "Epoch 37:  69% 80/116 [00:04<00:02, 17.94it/s, loss=0.67, v_num=1uh5] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.61it/s, loss=0.67, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.33it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 21.20it/s, loss=0.673, v_num=1uh5]\n",
            "Epoch 38:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.673, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.673, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.42it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.37it/s, loss=0.679, v_num=1uh5]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.661, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.661, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 44.13it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.653, v_num=1uh5]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.684, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.68it/s, loss=0.684, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.54it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.693, v_num=1uh5]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.40it/s, loss=0.669, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.88it/s, loss=0.669, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.00it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.719, v_num=1uh5]\n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.711, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.711, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.81it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.726, v_num=1uh5]\n",
            "Epoch 43:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.691, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.691, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.25it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.677, v_num=1uh5]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.7, v_num=1uh5]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.98it/s, loss=0.7, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.45it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.63it/s, loss=0.706, v_num=1uh5]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.684, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.684, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.16it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.61it/s, loss=0.686, v_num=1uh5]\n",
            "Epoch 46:  69% 80/116 [00:05<00:02, 13.67it/s, loss=0.7, v_num=1uh5]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:06<00:01, 15.35it/s, loss=0.7, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.47it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:06<00:00, 16.86it/s, loss=0.691, v_num=1uh5]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.692, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.692, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.66it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.694, v_num=1uh5]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.06it/s, loss=0.682, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.682, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.57it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.69, v_num=1uh5] \n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.49it/s, loss=0.702, v_num=1uh5]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 20.00it/s, loss=0.702, v_num=1uh5]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.34it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.63it/s, loss=0.745, v_num=1uh5]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.745, v_num=1uh5]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.54it/s, loss=0.745, v_num=1uh5]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇████▅▅▅▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▃▄▆▇▇▇██████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▇▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss ▅▄▅▃▅▃▂▂▃▂▂▁▂▂▁▂▂▃▃▃▃▁▂▂▂▁▃▂▂▂▂▁▃▁▂▂▂▂▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▅▄▅▃▅▃▂▂▃▂▂▁▂▂▁▂▂▃▃▃▃▁▂▂▂▁▃▂▂▂▂▁▃▁▂▂▂▂▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae ▆▄▅▄▅▃▂▂▂▂▂▁▁▂▂▂▃▃▃▃▃▁▂▂▃▁▃▂▃▂▁▁▃▁▂▂▂▃▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▁▁▁▁█▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse ▅▄▅▃▅▃▂▂▃▂▂▁▂▂▁▂▂▃▃▃▃▁▂▂▂▁▃▂▂▂▂▁▃▁▂▂▂▂▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae ▆▄▅▄▅▃▂▂▂▂▂▁▁▂▂▂▃▃▃▃▃▁▂▂▃▁▃▂▃▂▁▁▃▁▂▂▂▃▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse ▅▄▅▃▅▃▂▂▃▂▂▁▂▂▁▂▂▃▃▃▃▁▂▂▂▁▃▂▂▂▂▁▃▁▂▂▂▂▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▆▆▆▄▄▄▃▃▂▂▂▃▂▂▂▁▃▃▃▂▂▃▃▃▄▃▃▁▂▁▂▂▂▃▃▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▆▅▅▇█▃▂▃▂▃▂▃▂▂▂▁▂▂▃▃▃▃▁▂▃▃▃▁▃▂▁▁▂▁▃▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▂▃▄▆▆▇▇██████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▆▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss ███▇▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss ███▇▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae ███▇▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▄▄▃▂▁▆▇▆█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse ███▇▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae ███▇▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse ███▇▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse ███▇▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▇▇▇██▁▁▁▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.12163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 1.53518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 1.53519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 1.00176\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 2.80524\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 1.53518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 1.00176\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 1.53518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.94599\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.29458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.11896\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.71694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.71696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.66732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.62185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.71694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.66732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.71694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.83962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.27579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcrisp-sky-813\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/37fm1uh5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_045848-37fm1uh5/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_050332-xg3wkkpt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-universe-814\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/xg3wkkpt\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-2.83624975e-17 -1.81484457e-17  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-2.83624975e-17 -1.81484457e-17  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.22it/s, loss=1.02, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.79it/s, loss=1.02, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.30it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.44it/s, loss=1.03, v_num=kkpt]\n",
            "Epoch 1:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.963, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.58it/s, loss=0.963, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.26it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.18it/s, loss=0.965, v_num=kkpt]\n",
            "Epoch 2:  69% 80/116 [00:04<00:02, 17.82it/s, loss=0.999, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.40it/s, loss=0.999, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.50it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 20.99it/s, loss=0.974, v_num=kkpt]\n",
            "Epoch 3:  69% 80/116 [00:04<00:02, 17.71it/s, loss=0.939, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.25it/s, loss=0.939, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.97it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 20.77it/s, loss=0.905, v_num=kkpt]\n",
            "Epoch 4:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.864, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.864, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.41it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.887, v_num=kkpt]\n",
            "Epoch 5:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.771, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.771, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.38it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.786, v_num=kkpt]\n",
            "Epoch 6:  69% 80/116 [00:04<00:02, 17.88it/s, loss=0.75, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.53it/s, loss=0.75, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.78it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.15it/s, loss=0.777, v_num=kkpt]\n",
            "Epoch 7:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.74, v_num=kkpt] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.51it/s, loss=0.74, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.69it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.711, v_num=kkpt]\n",
            "Epoch 8:  69% 80/116 [00:04<00:02, 17.99it/s, loss=0.762, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.69it/s, loss=0.762, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.83it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.758, v_num=kkpt]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.731, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.731, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.86it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.754, v_num=kkpt]\n",
            "Epoch 10:  69% 80/116 [00:04<00:01, 18.45it/s, loss=0.692, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:04<00:00, 20.02it/s, loss=0.692, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.39it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.69it/s, loss=0.682, v_num=kkpt]\n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.36it/s, loss=0.699, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:04<00:00, 20.01it/s, loss=0.699, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.01it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.57it/s, loss=0.69, v_num=kkpt] \n",
            "Epoch 12:  69% 80/116 [00:04<00:02, 17.36it/s, loss=0.688, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 18.97it/s, loss=0.688, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.35it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 20.64it/s, loss=0.695, v_num=kkpt]\n",
            "Epoch 13:  69% 80/116 [00:04<00:02, 17.28it/s, loss=0.692, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 18.93it/s, loss=0.692, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.31it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 20.54it/s, loss=0.694, v_num=kkpt]\n",
            "Epoch 14:  69% 80/116 [00:04<00:02, 17.37it/s, loss=0.674, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 18.96it/s, loss=0.674, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.50it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 20.58it/s, loss=0.68, v_num=kkpt] \n",
            "Epoch 15:  69% 80/116 [00:04<00:02, 17.39it/s, loss=0.669, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 18.14it/s, loss=0.669, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.89it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 19.76it/s, loss=0.662, v_num=kkpt]\n",
            "Epoch 16:  69% 80/116 [00:04<00:02, 17.30it/s, loss=0.689, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 18.06it/s, loss=0.689, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.82it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 19.66it/s, loss=0.682, v_num=kkpt]\n",
            "Epoch 17:  69% 80/116 [00:04<00:02, 17.56it/s, loss=0.66, v_num=kkpt] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 18.17it/s, loss=0.66, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.21it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 19.68it/s, loss=0.647, v_num=kkpt]\n",
            "Epoch 18:  69% 80/116 [00:04<00:02, 17.76it/s, loss=0.633, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.41it/s, loss=0.633, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.92it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.05it/s, loss=0.614, v_num=kkpt]\n",
            "Epoch 19:  69% 80/116 [00:04<00:01, 18.38it/s, loss=0.651, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.651, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.58it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.68, v_num=kkpt] \n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.66, v_num=kkpt] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.66, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.39it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.38it/s, loss=0.65, v_num=kkpt]\n",
            "Epoch 21:  69% 80/116 [00:04<00:02, 17.97it/s, loss=0.658, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 19.59it/s, loss=0.658, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.31it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.25it/s, loss=0.662, v_num=kkpt]\n",
            "Epoch 22:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.685, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.88it/s, loss=0.685, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.23it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.673, v_num=kkpt]\n",
            "Epoch 23:  69% 80/116 [00:04<00:01, 18.41it/s, loss=0.66, v_num=kkpt] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.66, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.87it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.66, v_num=kkpt]\n",
            "Epoch 24:  69% 80/116 [00:04<00:02, 17.49it/s, loss=0.677, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.10it/s, loss=0.677, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.99it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 20.76it/s, loss=0.669, v_num=kkpt]\n",
            "Epoch 25:  69% 80/116 [00:05<00:02, 14.74it/s, loss=0.624, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:06<00:01, 15.50it/s, loss=0.624, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 26.31it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:06<00:00, 16.91it/s, loss=0.612, v_num=kkpt]\n",
            "Epoch 26:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.641, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.641, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.29it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.658, v_num=kkpt]\n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.636, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.91it/s, loss=0.636, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.20it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.65, v_num=kkpt] \n",
            "Epoch 28:  69% 80/116 [00:04<00:01, 18.41it/s, loss=0.651, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.96it/s, loss=0.651, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.41it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.61it/s, loss=0.656, v_num=kkpt]\n",
            "Epoch 29:  69% 80/116 [00:04<00:02, 17.71it/s, loss=0.629, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.37it/s, loss=0.629, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.23it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.04it/s, loss=0.618, v_num=kkpt]\n",
            "Epoch 30:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.644, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.644, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.50it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.27it/s, loss=0.635, v_num=kkpt]\n",
            "Epoch 31:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.631, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.631, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.42it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 21.42it/s, loss=0.647, v_num=kkpt]\n",
            "Epoch 32:  69% 80/116 [00:04<00:01, 18.32it/s, loss=0.673, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.673, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.97it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.699, v_num=kkpt]\n",
            "Epoch 33:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.637, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.637, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.87it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.631, v_num=kkpt]\n",
            "Epoch 34:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.669, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.54it/s, loss=0.669, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.49it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.687, v_num=kkpt]\n",
            "Epoch 35:  69% 80/116 [00:04<00:02, 17.91it/s, loss=0.661, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.42it/s, loss=0.661, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.81it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.08it/s, loss=0.662, v_num=kkpt]\n",
            "Epoch 36:  69% 80/116 [00:04<00:01, 18.04it/s, loss=0.638, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.638, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.59it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.641, v_num=kkpt]\n",
            "Epoch 37:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.647, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.61it/s, loss=0.647, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.14it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 21.27it/s, loss=0.639, v_num=kkpt]\n",
            "Epoch 38:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.655, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.94it/s, loss=0.655, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 44.20it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.53it/s, loss=0.668, v_num=kkpt]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.32it/s, loss=0.631, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.631, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.28it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.56it/s, loss=0.634, v_num=kkpt]\n",
            "Epoch 40:  69% 80/116 [00:04<00:02, 17.72it/s, loss=0.645, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.36it/s, loss=0.645, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.21it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 20.98it/s, loss=0.635, v_num=kkpt]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.635, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.635, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.36it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.642, v_num=kkpt]\n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.672, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.672, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.12it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.672, v_num=kkpt]\n",
            "Epoch 43:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.635, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.635, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.04it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.652, v_num=kkpt]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.657, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.83it/s, loss=0.657, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.26it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.65, v_num=kkpt] \n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.617, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.617, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.73it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.633, v_num=kkpt]\n",
            "Epoch 46:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.632, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.60it/s, loss=0.632, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.73it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.22it/s, loss=0.632, v_num=kkpt]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.626, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.75it/s, loss=0.626, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.30it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.626, v_num=kkpt]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.653, v_num=kkpt]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.653, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.09it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.44it/s, loss=0.641, v_num=kkpt]\n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.65, v_num=kkpt] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.65, v_num=kkpt]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.28it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.637, v_num=kkpt]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.35it/s, loss=0.637, v_num=kkpt]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.637, v_num=kkpt]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇████▄▄▄▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▃▆██████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▇▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇██▆▆▇▅▅▅▄▄▄▅▅▅▅▅▄▄▅▅▄▅▅▄▄▄▄▄▄▆▄▅▃▄▅▅▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇██▆▆▇▅▅▅▄▄▄▅▅▅▅▅▄▄▅▅▄▅▅▄▄▄▄▄▄▆▄▅▃▄▅▅▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇██▆▆▇▅▅▅▄▄▄▅▄▅▄▄▄▄▅▄▄▅▅▄▄▃▄▃▄▆▄▅▃▄▅▄▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▁▁▁▁▂▁▁▁▅▁█▂▁▁▁▁▂▂▁▁▁▃▂▂▆▂▁▁▁▂▁▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇██▆▆▇▅▅▅▄▄▄▅▅▅▅▅▄▄▅▅▄▅▅▄▄▄▄▄▄▆▄▅▃▄▅▅▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇██▆▆▇▅▅▅▄▄▄▅▄▅▄▄▄▄▅▄▄▅▅▄▄▃▄▃▄▆▄▅▃▄▅▄▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇██▆▆▇▅▅▅▄▄▄▅▅▅▅▅▄▄▅▅▄▅▅▄▄▄▄▄▄▆▄▅▃▄▅▅▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse ██▇▇▅▆▄▄▄▂▂▂▄▂▁▂▂▃▂▂▃▃▃▂▃▄▂▁▃▂▃▃▃▄▁▂▃▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▆▇█▆▄▅▄▄▃▂▂▃▂▂▂▁▁▂▁▂▂▁▃▁▂▃▃▁▁▁▂▂▂▁▁▂▂▁▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▂▃▆██████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss ██▇▇▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss ██▇▇▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae ███▇▃▃▃▃▃▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▃▁▁▂▅▄▄▄█▆▇▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse ██▇▇▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae ███▇▃▃▃▃▃▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse ██▇▇▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse ██▇▇▃▃▃▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▆██▇▄▄▄▃▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.09263\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.27193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.27194\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.45409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 0.86526\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.27193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.45409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.27193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.79986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.40516\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.09251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.66103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.66104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.61761\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.17861\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.66103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.61761\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.66103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.79786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.20521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfearless-universe-814\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/xg3wkkpt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_050332-xg3wkkpt/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_050818-2w0g2n0h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-thunder-815\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/2w0g2n0h\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-2.83624975e-17 -7.56431954e-18  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-2.83624975e-17 -7.56431954e-18  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.51it/s, loss=1.16, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:04<00:00, 20.15it/s, loss=1.16, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.20it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.77it/s, loss=1.14, v_num=2n0h]\n",
            "Epoch 1:  69% 80/116 [00:04<00:02, 17.62it/s, loss=1.05, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.20it/s, loss=1.05, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.64it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 20.85it/s, loss=1.02, v_num=2n0h]\n",
            "Epoch 2:  69% 80/116 [00:04<00:02, 17.78it/s, loss=1.05, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.41it/s, loss=1.05, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.96it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.04it/s, loss=1.04, v_num=2n0h]\n",
            "Epoch 3:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.891, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.89it/s, loss=0.891, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.98it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.858, v_num=2n0h]\n",
            "Epoch 4:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.768, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.28it/s, loss=0.768, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 31.47it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 20.22it/s, loss=0.775, v_num=2n0h]\n",
            "Epoch 5:  69% 80/116 [00:05<00:02, 14.25it/s, loss=0.754, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:06<00:01, 15.85it/s, loss=0.754, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.55it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:06<00:00, 17.25it/s, loss=0.746, v_num=2n0h]\n",
            "Epoch 6:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.671, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.671, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.81it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.67, v_num=2n0h] \n",
            "Epoch 7:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.595, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.595, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.94it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.624, v_num=2n0h]\n",
            "Epoch 8:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.58, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.58, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.11it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.567, v_num=2n0h]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.577, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.577, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.68it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.586, v_num=2n0h]\n",
            "Epoch 10:  69% 80/116 [00:04<00:02, 17.83it/s, loss=0.561, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.45it/s, loss=0.561, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.87it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.09it/s, loss=0.558, v_num=2n0h]\n",
            "Epoch 11:  69% 80/116 [00:04<00:02, 17.85it/s, loss=0.54, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.49it/s, loss=0.54, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.51it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.12it/s, loss=0.58, v_num=2n0h]\n",
            "Epoch 12:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.542, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.542, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.39it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.519, v_num=2n0h]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.532, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.62it/s, loss=0.532, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.44it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.547, v_num=2n0h]\n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.48it/s, loss=0.54, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:04<00:00, 20.09it/s, loss=0.54, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.18it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.68it/s, loss=0.532, v_num=2n0h]\n",
            "Epoch 15:  69% 80/116 [00:04<00:01, 18.49it/s, loss=0.529, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.99it/s, loss=0.529, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.61it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 21.61it/s, loss=0.529, v_num=2n0h]\n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.525, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.525, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.39it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.35it/s, loss=0.532, v_num=2n0h]\n",
            "Epoch 17:  69% 80/116 [00:04<00:01, 18.04it/s, loss=0.548, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.68it/s, loss=0.548, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.70it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.54, v_num=2n0h] \n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.505, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.97it/s, loss=0.505, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.20it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.517, v_num=2n0h]\n",
            "Epoch 19:  69% 80/116 [00:04<00:01, 18.29it/s, loss=0.531, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.91it/s, loss=0.531, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.25it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.61it/s, loss=0.533, v_num=2n0h]\n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.517, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.517, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.94it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.44it/s, loss=0.532, v_num=2n0h]\n",
            "Epoch 21:  69% 80/116 [00:04<00:01, 18.38it/s, loss=0.517, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 20.00it/s, loss=0.517, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.16it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.65it/s, loss=0.523, v_num=2n0h]\n",
            "Epoch 22:  69% 80/116 [00:04<00:01, 18.04it/s, loss=0.514, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.51it/s, loss=0.514, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.65it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.10it/s, loss=0.516, v_num=2n0h]\n",
            "Epoch 23:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.517, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.517, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.07it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.512, v_num=2n0h]\n",
            "Epoch 24:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.512, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.86it/s, loss=0.512, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.21it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.501, v_num=2n0h]\n",
            "Epoch 25:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.497, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.497, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.00it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.62it/s, loss=0.496, v_num=2n0h]\n",
            "Epoch 26:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.503, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.73it/s, loss=0.503, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.59it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.52, v_num=2n0h] \n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.515, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.75it/s, loss=0.515, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.03it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.54, v_num=2n0h] \n",
            "Epoch 28:  69% 80/116 [00:04<00:02, 17.97it/s, loss=0.512, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.55it/s, loss=0.512, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.44it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.19it/s, loss=0.513, v_num=2n0h]\n",
            "Epoch 29:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.53, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.91it/s, loss=0.53, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.72it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.57it/s, loss=0.523, v_num=2n0h]\n",
            "Epoch 30:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.511, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.511, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.60it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.522, v_num=2n0h]\n",
            "Epoch 31:  69% 80/116 [00:04<00:01, 18.30it/s, loss=0.51, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.51, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.77it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.5, v_num=2n0h] \n",
            "Epoch 32:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.509, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.509, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.40it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.529, v_num=2n0h]\n",
            "Epoch 33:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.514, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.51it/s, loss=0.514, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.32it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.04it/s, loss=0.506, v_num=2n0h]\n",
            "Epoch 34:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.495, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.495, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.06it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.542, v_num=2n0h]\n",
            "Epoch 35:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.511, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.511, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.04it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.35it/s, loss=0.543, v_num=2n0h]\n",
            "Epoch 36:  69% 80/116 [00:05<00:02, 13.82it/s, loss=0.54, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:06<00:01, 15.53it/s, loss=0.54, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.64it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:06<00:00, 17.10it/s, loss=0.543, v_num=2n0h]\n",
            "Epoch 37:  69% 80/116 [00:04<00:02, 17.86it/s, loss=0.503, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.49it/s, loss=0.503, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.13it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 21.12it/s, loss=0.509, v_num=2n0h]\n",
            "Epoch 38:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.524, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.92it/s, loss=0.524, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.29it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.56it/s, loss=0.543, v_num=2n0h]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.5, v_num=2n0h]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.5, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.84it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.491, v_num=2n0h]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.32it/s, loss=0.524, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.97it/s, loss=0.524, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.24it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.61it/s, loss=0.532, v_num=2n0h]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.52, v_num=2n0h] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.52, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.55it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.51, v_num=2n0h]\n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.496, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.496, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.61it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.511, v_num=2n0h]\n",
            "Epoch 43:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.548, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.92it/s, loss=0.548, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.78it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.57it/s, loss=0.54, v_num=2n0h] \n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.62it/s, loss=0.512, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:04<00:00, 20.09it/s, loss=0.512, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.34it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.70it/s, loss=0.526, v_num=2n0h]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.51it/s, loss=0.537, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:04<00:00, 20.08it/s, loss=0.537, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.86it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.70it/s, loss=0.538, v_num=2n0h]\n",
            "Epoch 46:  69% 80/116 [00:04<00:01, 18.14it/s, loss=0.519, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.519, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.84it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.506, v_num=2n0h]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.53it/s, loss=0.515, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:04<00:00, 20.16it/s, loss=0.515, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.63it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.78it/s, loss=0.521, v_num=2n0h]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.506, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.73it/s, loss=0.506, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.04it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.55, v_num=2n0h] \n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.51it/s, loss=0.512, v_num=2n0h]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:04<00:00, 20.04it/s, loss=0.512, v_num=2n0h]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.75it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.73it/s, loss=0.533, v_num=2n0h]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.71it/s, loss=0.533, v_num=2n0h]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.68it/s, loss=0.533, v_num=2n0h]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇████▅▅▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▄▅▆▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▆▆▅▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▆▇▄▃▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▁▂▁▂▂▂▂▃▁▂▂▂▂▂▂▂▁▂▂▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▆▇▄▃▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▁▂▁▂▂▂▂▃▁▂▂▂▂▂▂▂▁▂▂▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇▇▄▃▃▃▂▂▂▂▃▃▃▃▂▂▂▃▃▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▃▂▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁█▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▆▇▄▃▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▁▂▁▂▂▂▂▃▁▂▂▂▂▂▂▂▁▂▂▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇▇▄▃▃▃▂▂▂▂▃▃▃▃▂▂▂▃▃▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▃▂▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▆▇▄▃▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▁▂▁▂▂▂▂▃▁▂▂▂▂▂▂▂▁▂▂▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▇▆▅▃▃▄▃▂▄▄▂▂▂▂▂▃▂▃▂▂▂▃▃▂▂▂▂▂▃▂▄▃▂▃▁▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▇██▆▅▃▂▂▃▂▂▃▂▂▂▂▁▂▂▃▂▃▂▂▂▂▂▂▂▂▃▂▃▂▂▂▁▃▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▃▅▅▆▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▆▅▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▇▆▅▃▃▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▇▆▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▇▇▆▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▇▂▁▄▄▄▇▇▇██▇▅▇▆▅▅▅▆▆▅▆▆▆▆▅▆▆▆▆▆▆▅▆▆▅▆▅▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▇▆▅▃▃▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▇▇▆▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▇▆▅▃▃▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▇▇▅▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ███▆▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.13747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.97177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.97178\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.84406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 1.63437\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.97177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.84406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.97177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.92332\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.34105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99653\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.13937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.5307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.53072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.5549\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.71352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.5307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.5549\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.5307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.71625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.02775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdandy-thunder-815\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/2w0g2n0h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_050818-2w0g2n0h/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_051302-3kmquf4q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlilac-mountain-816\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/3kmquf4q\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-2.83624975e-17 -7.56431954e-18 -1.81484457e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-2.83624975e-17 -7.56431954e-18 -1.81484457e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.52it/s, loss=1.46, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:04<00:00, 20.09it/s, loss=1.46, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.48it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.76it/s, loss=1.42, v_num=uf4q]\n",
            "Epoch 1:  69% 80/116 [00:04<00:01, 18.18it/s, loss=1.08, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.82it/s, loss=1.08, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.78it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.45it/s, loss=1.05, v_num=uf4q]\n",
            "Epoch 2:  69% 80/116 [00:04<00:01, 18.20it/s, loss=1.04, v_num=uf4q] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.73it/s, loss=1.04, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.41it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.36it/s, loss=1.05, v_num=uf4q]\n",
            "Epoch 3:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.964, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.87it/s, loss=0.964, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.16it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.951, v_num=uf4q]\n",
            "Epoch 4:  69% 80/116 [00:04<00:02, 17.77it/s, loss=0.881, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.36it/s, loss=0.881, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.99it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.851, v_num=uf4q]\n",
            "Epoch 5:  69% 80/116 [00:04<00:02, 17.88it/s, loss=0.763, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.55it/s, loss=0.763, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.30it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.16it/s, loss=0.759, v_num=uf4q]\n",
            "Epoch 6:  69% 80/116 [00:04<00:01, 18.00it/s, loss=0.621, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.62it/s, loss=0.621, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.98it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.635, v_num=uf4q]\n",
            "Epoch 7:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.612, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.612, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.50it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.596, v_num=uf4q]\n",
            "Epoch 8:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.595, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.76it/s, loss=0.595, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.20it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 21.37it/s, loss=0.593, v_num=uf4q]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.29it/s, loss=0.591, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.90it/s, loss=0.591, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.83it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.587, v_num=uf4q]\n",
            "Epoch 10:  69% 80/116 [00:04<00:02, 17.55it/s, loss=0.567, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.14it/s, loss=0.567, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.20it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 20.76it/s, loss=0.571, v_num=uf4q]\n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.586, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.586, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.93it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.583, v_num=uf4q]\n",
            "Epoch 12:  69% 80/116 [00:04<00:02, 17.97it/s, loss=0.612, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.56it/s, loss=0.612, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.42it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.22it/s, loss=0.6, v_num=uf4q]  \n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.545, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.545, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.77it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.541, v_num=uf4q]\n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.551, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.551, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.68it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.575, v_num=uf4q]\n",
            "Epoch 15:  69% 80/116 [00:04<00:02, 17.99it/s, loss=0.553, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 18.66it/s, loss=0.553, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 28.22it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 19.70it/s, loss=0.562, v_num=uf4q]\n",
            "Epoch 16:  69% 80/116 [00:05<00:02, 14.63it/s, loss=0.564, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:06<00:00, 16.34it/s, loss=0.564, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.94it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:06<00:00, 17.88it/s, loss=0.563, v_num=uf4q]\n",
            "Epoch 17:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.583, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.75it/s, loss=0.583, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.82it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.42it/s, loss=0.595, v_num=uf4q]\n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.41it/s, loss=0.558, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.98it/s, loss=0.558, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.16it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.62it/s, loss=0.55, v_num=uf4q] \n",
            "Epoch 19:  69% 80/116 [00:04<00:02, 17.92it/s, loss=0.55, v_num=uf4q] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.46it/s, loss=0.55, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.47it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.04it/s, loss=0.542, v_num=uf4q]\n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.546, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.60it/s, loss=0.546, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.35it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.14it/s, loss=0.569, v_num=uf4q]\n",
            "Epoch 21:  69% 80/116 [00:04<00:02, 16.82it/s, loss=0.544, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 18.46it/s, loss=0.544, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.66it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 20.09it/s, loss=0.525, v_num=uf4q]\n",
            "Epoch 22:  69% 80/116 [00:04<00:02, 17.46it/s, loss=0.561, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.12it/s, loss=0.561, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.33it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 20.76it/s, loss=0.583, v_num=uf4q]\n",
            "Epoch 23:  69% 80/116 [00:04<00:02, 17.58it/s, loss=0.566, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.20it/s, loss=0.566, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.50it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 20.86it/s, loss=0.567, v_num=uf4q]\n",
            "Epoch 24:  69% 80/116 [00:04<00:02, 17.38it/s, loss=0.548, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.06it/s, loss=0.548, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.22it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 20.59it/s, loss=0.554, v_num=uf4q]\n",
            "Epoch 25:  69% 80/116 [00:04<00:02, 17.27it/s, loss=0.542, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 18.79it/s, loss=0.542, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.51it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 20.35it/s, loss=0.549, v_num=uf4q]\n",
            "Epoch 26:  69% 80/116 [00:04<00:02, 16.97it/s, loss=0.575, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 18.59it/s, loss=0.575, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.63it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 19.42it/s, loss=0.57, v_num=uf4q] \n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.41it/s, loss=0.54, v_num=uf4q] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.96it/s, loss=0.54, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.66it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.60it/s, loss=0.567, v_num=uf4q]\n",
            "Epoch 28:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.534, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.76it/s, loss=0.534, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.59it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.42it/s, loss=0.55, v_num=uf4q] \n",
            "Epoch 29:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.547, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.88it/s, loss=0.547, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.08it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.51it/s, loss=0.559, v_num=uf4q]\n",
            "Epoch 30:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.545, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.545, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.83it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.555, v_num=uf4q]\n",
            "Epoch 31:  69% 80/116 [00:04<00:02, 17.94it/s, loss=0.542, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.15it/s, loss=0.542, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 36.15it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 20.69it/s, loss=0.555, v_num=uf4q]\n",
            "Epoch 32:  69% 80/116 [00:04<00:02, 16.63it/s, loss=0.542, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 18.21it/s, loss=0.542, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.24it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 19.80it/s, loss=0.545, v_num=uf4q]\n",
            "Epoch 33:  69% 80/116 [00:04<00:02, 17.65it/s, loss=0.545, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.19it/s, loss=0.545, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.62it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 20.79it/s, loss=0.559, v_num=uf4q]\n",
            "Epoch 34:  69% 80/116 [00:04<00:02, 17.41it/s, loss=0.534, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.03it/s, loss=0.534, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.65it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 20.68it/s, loss=0.55, v_num=uf4q] \n",
            "Epoch 35:  69% 80/116 [00:04<00:02, 17.80it/s, loss=0.546, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.29it/s, loss=0.546, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.69it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 20.82it/s, loss=0.561, v_num=uf4q]\n",
            "Epoch 36:  69% 80/116 [00:04<00:02, 17.64it/s, loss=0.527, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.12it/s, loss=0.527, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.97it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 20.75it/s, loss=0.558, v_num=uf4q]\n",
            "Epoch 37:  69% 80/116 [00:04<00:02, 17.00it/s, loss=0.557, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 18.47it/s, loss=0.557, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.69it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 19.97it/s, loss=0.551, v_num=uf4q]\n",
            "Epoch 38:  69% 80/116 [00:04<00:02, 16.26it/s, loss=0.545, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 17.64it/s, loss=0.545, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 36.67it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:06<00:00, 19.08it/s, loss=0.538, v_num=uf4q]\n",
            "Epoch 39:  69% 80/116 [00:04<00:02, 17.63it/s, loss=0.541, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.22it/s, loss=0.541, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.67it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 20.87it/s, loss=0.546, v_num=uf4q]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.554, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.83it/s, loss=0.554, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.76it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.563, v_num=uf4q]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.547, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.68it/s, loss=0.547, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.23it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.37it/s, loss=0.568, v_num=uf4q]\n",
            "Epoch 42:  69% 80/116 [00:04<00:02, 17.84it/s, loss=0.53, v_num=uf4q] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.40it/s, loss=0.53, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.78it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.03it/s, loss=0.528, v_num=uf4q]\n",
            "Epoch 43:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.546, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.78it/s, loss=0.546, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.84it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.552, v_num=uf4q]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.37it/s, loss=0.549, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.89it/s, loss=0.549, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.30it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.53it/s, loss=0.549, v_num=uf4q]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.06it/s, loss=0.55, v_num=uf4q] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.55, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.69it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.18it/s, loss=0.57, v_num=uf4q]\n",
            "Epoch 46:  69% 80/116 [00:05<00:02, 15.47it/s, loss=0.544, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:06<00:00, 16.30it/s, loss=0.544, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 28.43it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:06<00:00, 17.25it/s, loss=0.539, v_num=uf4q]\n",
            "Epoch 47:  69% 80/116 [00:04<00:02, 17.58it/s, loss=0.557, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.16it/s, loss=0.557, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.15it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 20.74it/s, loss=0.582, v_num=uf4q]\n",
            "Epoch 48:  69% 80/116 [00:04<00:02, 17.94it/s, loss=0.55, v_num=uf4q] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.55, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.60it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.19it/s, loss=0.535, v_num=uf4q]\n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.15it/s, loss=0.541, v_num=uf4q]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.76it/s, loss=0.541, v_num=uf4q]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.93it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.562, v_num=uf4q]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.37it/s, loss=0.562, v_num=uf4q]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.562, v_num=uf4q]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇█████████▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▄▇▇█████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▇▆▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇▅▄▄▂▂▂▁▂▂▃▂▂▂▂▁▂▃▁▂▂▁▂▂▂▃▂▁▂▂▁▂▂▃▂▁▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇▅▄▄▂▂▂▁▂▂▃▂▂▂▂▁▂▃▁▂▂▁▂▂▂▃▂▁▂▂▁▂▂▃▂▁▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae ██▅▅▅▃▃▃▁▂▂▄▂▃▂▂▂▂▃▁▂▃▁▃▂▂▃▂▁▂▂▁▂▂▃▂▁▂▂▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▂▁▁▁▂▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇▅▄▄▂▂▂▁▂▂▃▂▂▂▂▁▂▃▁▂▂▁▂▂▂▃▂▁▂▂▁▂▂▃▂▁▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae ██▅▅▅▃▃▃▁▂▂▄▂▃▂▂▂▂▃▁▂▃▁▃▂▂▃▂▁▂▂▁▂▂▃▂▁▂▂▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇▅▄▄▂▂▂▁▂▂▃▂▂▂▂▁▂▃▁▂▂▁▂▂▂▃▂▁▂▂▁▂▂▃▂▁▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▆▆▅▅▃▃▃▂▃▂▃▂▃▃▂▂▂▂▂▂▃▂▃▃▂▃▂▁▂▃▁▂▂▃▂▁▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▅█▆▆▇▄▃▃▁▃▃▂▁▃▂▂▂▂▂▁▃▃▁▄▃▃▃▂▂▁▂▁▃▂▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▂▅▇▇█████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ███▇▆▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▅▅▄▃▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▅▅▄▃▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▆▆▅▄▄▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape █▂▁▂▃▄▃▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▅▅▄▃▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▆▆▅▄▄▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▅▅▄▃▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▆▅▅▃▃▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▅█▇▆▄▃▂▂▂▁▂▂▁▁▂▂▁▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.04828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.97411\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.97411\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.86778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 0.72218\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.97411\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.86778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.97411\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.69389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.1786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.04836\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.56792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.56793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.56755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.0932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.56792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.56755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.56792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.73772\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.18578\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlilac-mountain-816\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/3kmquf4q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_051302-3kmquf4q/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_051751-19dovh8p\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33musual-capybara-817\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/19dovh8p\u001b[0m\n",
            "(30, 250, 4)\n",
            "yc.shape: (30, 250, 4)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-2.83624975e-17 -7.56431954e-18 -1.81484457e-17  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1. 1.]\n",
            "\n",
            "(21, 250, 4)\n",
            "(9, 250, 4)\n",
            "(30, 250, 4)\n",
            "yc.shape: (30, 250, 4)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-2.83624975e-17 -7.56431954e-18 -1.81484457e-17  1.71270405e-17]\n",
            "std_dev yc: [1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.059     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:02, 17.99it/s, loss=1.22, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.54it/s, loss=1.22, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.42it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.13it/s, loss=1.22, v_num=vh8p]\n",
            "Epoch 1:  69% 80/116 [00:04<00:02, 17.89it/s, loss=1.03, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.50it/s, loss=1.03, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.21it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.00it/s, loss=1.06, v_num=vh8p]\n",
            "Epoch 2:  69% 80/116 [00:04<00:02, 17.88it/s, loss=1.01, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.52it/s, loss=1.01, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.34it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.12it/s, loss=1, v_num=vh8p]   \n",
            "Epoch 3:  69% 80/116 [00:04<00:02, 18.00it/s, loss=0.974, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.53it/s, loss=0.974, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.41it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.17it/s, loss=0.968, v_num=vh8p]\n",
            "Epoch 4:  69% 80/116 [00:04<00:02, 17.96it/s, loss=0.946, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.946, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.84it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.933, v_num=vh8p]\n",
            "Epoch 5:  69% 80/116 [00:04<00:01, 18.15it/s, loss=0.864, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.864, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.09it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.35it/s, loss=0.884, v_num=vh8p]\n",
            "Epoch 6:  69% 80/116 [00:04<00:02, 17.73it/s, loss=0.731, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.32it/s, loss=0.731, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.83it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 20.94it/s, loss=0.714, v_num=vh8p]\n",
            "Epoch 7:  69% 80/116 [00:04<00:01, 18.37it/s, loss=0.68, v_num=vh8p] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.92it/s, loss=0.68, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.95it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.667, v_num=vh8p]\n",
            "Epoch 8:  69% 80/116 [00:04<00:02, 17.70it/s, loss=0.606, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.21it/s, loss=0.606, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.10it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 20.83it/s, loss=0.615, v_num=vh8p]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.32it/s, loss=0.583, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.583, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.49it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.54it/s, loss=0.609, v_num=vh8p]\n",
            "Epoch 10:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.569, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.76it/s, loss=0.569, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.94it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.578, v_num=vh8p]\n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.05it/s, loss=0.583, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.65it/s, loss=0.583, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.58it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.24it/s, loss=0.597, v_num=vh8p]\n",
            "Epoch 12:  69% 80/116 [00:04<00:01, 18.12it/s, loss=0.576, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.576, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.31it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.595, v_num=vh8p]\n",
            "Epoch 13:  69% 80/116 [00:04<00:02, 17.62it/s, loss=0.557, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.19it/s, loss=0.557, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.76it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 20.78it/s, loss=0.57, v_num=vh8p] \n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.546, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.83it/s, loss=0.546, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.93it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.535, v_num=vh8p]\n",
            "Epoch 15:  69% 80/116 [00:04<00:02, 17.70it/s, loss=0.556, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.20it/s, loss=0.556, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.84it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 20.75it/s, loss=0.559, v_num=vh8p]\n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.557, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.557, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.17it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.51it/s, loss=0.555, v_num=vh8p]\n",
            "Epoch 17:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.536, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.536, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.12it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.553, v_num=vh8p]\n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.29it/s, loss=0.535, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.90it/s, loss=0.535, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.84it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.55it/s, loss=0.541, v_num=vh8p]\n",
            "Epoch 19:  69% 80/116 [00:04<00:02, 17.68it/s, loss=0.549, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.32it/s, loss=0.549, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.92it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 20.88it/s, loss=0.555, v_num=vh8p]\n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.41it/s, loss=0.544, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.99it/s, loss=0.544, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.25it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.64it/s, loss=0.55, v_num=vh8p] \n",
            "Epoch 21:  69% 80/116 [00:04<00:01, 18.29it/s, loss=0.565, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.565, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.65it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.60it/s, loss=0.571, v_num=vh8p]\n",
            "Epoch 22:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.564, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.564, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.65it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.51it/s, loss=0.555, v_num=vh8p]\n",
            "Epoch 23:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.55, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:04<00:00, 20.01it/s, loss=0.55, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.46it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.67it/s, loss=0.564, v_num=vh8p]\n",
            "Epoch 24:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.535, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.56it/s, loss=0.535, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.34it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.22it/s, loss=0.531, v_num=vh8p]\n",
            "Epoch 25:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.54, v_num=vh8p] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.69it/s, loss=0.54, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.13it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.536, v_num=vh8p]\n",
            "Epoch 26:  69% 80/116 [00:05<00:02, 14.53it/s, loss=0.554, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:06<00:01, 15.25it/s, loss=0.554, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 26.46it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:06<00:00, 16.67it/s, loss=0.547, v_num=vh8p]\n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.541, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.60it/s, loss=0.541, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.87it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.536, v_num=vh8p]\n",
            "Epoch 28:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.539, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.91it/s, loss=0.539, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.17it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.55it/s, loss=0.53, v_num=vh8p] \n",
            "Epoch 29:  69% 80/116 [00:04<00:01, 18.30it/s, loss=0.548, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.548, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.38it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.566, v_num=vh8p]\n",
            "Epoch 30:  69% 80/116 [00:04<00:02, 17.85it/s, loss=0.543, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.44it/s, loss=0.543, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.56it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.03it/s, loss=0.56, v_num=vh8p] \n",
            "Epoch 31:  69% 80/116 [00:04<00:01, 18.40it/s, loss=0.55, v_num=vh8p] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.94it/s, loss=0.55, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.42it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 21.60it/s, loss=0.588, v_num=vh8p]\n",
            "Epoch 32:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.557, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.97it/s, loss=0.557, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.33it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 21.64it/s, loss=0.574, v_num=vh8p]\n",
            "Epoch 33:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.547, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.547, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.24it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.56, v_num=vh8p] \n",
            "Epoch 34:  69% 80/116 [00:04<00:01, 18.36it/s, loss=0.544, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.97it/s, loss=0.544, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.32it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 21.63it/s, loss=0.591, v_num=vh8p]\n",
            "Epoch 35:  69% 80/116 [00:04<00:02, 17.88it/s, loss=0.523, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.523, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.30it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.14it/s, loss=0.509, v_num=vh8p]\n",
            "Epoch 36:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.532, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.532, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.15it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 21.60it/s, loss=0.543, v_num=vh8p]\n",
            "Epoch 37:  69% 80/116 [00:04<00:01, 18.15it/s, loss=0.53, v_num=vh8p] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.53, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.01it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 21.17it/s, loss=0.528, v_num=vh8p]\n",
            "Epoch 38:  69% 80/116 [00:04<00:02, 17.93it/s, loss=0.579, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.42it/s, loss=0.579, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.42it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.05it/s, loss=0.591, v_num=vh8p]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.533, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.533, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.65it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.17it/s, loss=0.529, v_num=vh8p]\n",
            "Epoch 40:  69% 80/116 [00:04<00:02, 17.82it/s, loss=0.561, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.45it/s, loss=0.561, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.93it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.10it/s, loss=0.561, v_num=vh8p]\n",
            "Epoch 41:  69% 80/116 [00:04<00:02, 17.85it/s, loss=0.531, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.48it/s, loss=0.531, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.00it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.09it/s, loss=0.54, v_num=vh8p] \n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.37it/s, loss=0.561, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.86it/s, loss=0.561, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.58it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.555, v_num=vh8p]\n",
            "Epoch 43:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.543, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.543, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.43it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.543, v_num=vh8p]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.42it/s, loss=0.542, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:04<00:00, 20.06it/s, loss=0.542, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.19it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.69it/s, loss=0.528, v_num=vh8p]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.30it/s, loss=0.547, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.86it/s, loss=0.547, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.62it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.539, v_num=vh8p]\n",
            "Epoch 46:  69% 80/116 [00:04<00:02, 17.96it/s, loss=0.553, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.36it/s, loss=0.553, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.61it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.01it/s, loss=0.558, v_num=vh8p]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.538, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:04<00:00, 20.03it/s, loss=0.538, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.81it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.67it/s, loss=0.548, v_num=vh8p]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.40it/s, loss=0.534, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.534, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.62it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.542, v_num=vh8p]\n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.33it/s, loss=0.543, v_num=vh8p]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.68it/s, loss=0.543, v_num=vh8p]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.85it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.542, v_num=vh8p]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.542, v_num=vh8p]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.26it/s, loss=0.542, v_num=vh8p]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇████▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▄▄▅▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▇▆▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇▇▇▆▄▄▂▂▂▂▂▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▃▂▂▂▂▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇▇▇▆▄▄▂▂▂▂▂▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▃▂▂▂▂▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇▇▇▆▄▄▂▂▂▂▂▃▂▁▃▂▂▂▂▂▁▁▂▂▁▁▁▂▂▁▂▂▂▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▂▁▂▁▄▃▁▁▁▂▁▁▁▁▁▁▂▁▁█▁▁▂▁▁▁▁▁▂▁▁▂▂▁▂▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇▇▇▆▄▄▂▂▂▂▂▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▃▂▂▂▂▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇▇▇▆▄▄▂▂▂▂▂▃▂▁▃▂▂▂▂▂▁▁▂▂▁▁▁▂▂▁▂▂▂▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇▇▇▆▄▄▂▂▂▂▂▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▃▂▂▂▂▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▇▆▅▄▄▃▂▂▂▂▄▂▂▂▃▂▂▂▁▂▁▂▁▁▂▂▂▂▂▃▃▂▂▃▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▆███▇▅▄▂▂▂▂▂▃▃▁▃▃▃▂▃▁▃▂▃▁▂▂▂▂▂▂▂▂▃▃▂▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▂▄▅▅▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▆▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▇▆▆▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▇▆▆▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▇▇▆▅▃▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape █▂▁▁▄▅▆▆▆▆▆▆▆▆▅▆▆▅▆▅▅▆▅▆▆▆▅▅▅▆▅▆▅▅▅▅▅▅▅▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▇▆▆▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▇▇▆▅▃▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▇▆▆▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▇▆▆▅▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▇███▆▄▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.12234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.6161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.61612\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.57996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 0.66554\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.6161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.57996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.6161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.77677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.06993\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.12742\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.56242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.56243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.57677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.47871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.56242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.57677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.56242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.7365\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 1.1346\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33musual-capybara-817\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/19dovh8p\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_051751-19dovh8p/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_052235-3c8lyywx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-sunset-818\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/3c8lyywx\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-8.98688531e-17 -1.14871076e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-8.98688531e-17 -1.14871076e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.02it/s, loss=1.15, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.54it/s, loss=1.15, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.48it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.06it/s, loss=1.11, v_num=yywx]\n",
            "Epoch 1:  69% 80/116 [00:04<00:02, 17.90it/s, loss=1.03, v_num=yywx] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.53it/s, loss=1.03, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.27it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.18it/s, loss=1.01, v_num=yywx]\n",
            "Epoch 2:  69% 80/116 [00:04<00:01, 18.47it/s, loss=0.787, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:04<00:00, 20.04it/s, loss=0.787, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.71it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.69it/s, loss=0.793, v_num=yywx]\n",
            "Epoch 3:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.566, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.566, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.78it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.20it/s, loss=0.553, v_num=yywx]\n",
            "Epoch 4:  69% 80/116 [00:04<00:02, 17.67it/s, loss=0.499, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.27it/s, loss=0.499, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.78it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 20.80it/s, loss=0.527, v_num=yywx]\n",
            "Epoch 5:  69% 80/116 [00:04<00:02, 17.97it/s, loss=0.441, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.58it/s, loss=0.441, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.07it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.16it/s, loss=0.438, v_num=yywx]\n",
            "Epoch 6:  69% 80/116 [00:05<00:02, 13.78it/s, loss=0.372, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:06<00:01, 14.83it/s, loss=0.372, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 32.34it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:07<00:00, 16.34it/s, loss=0.371, v_num=yywx]\n",
            "Epoch 7:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.33, v_num=yywx] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.48it/s, loss=0.33, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.42it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.08it/s, loss=0.343, v_num=yywx]\n",
            "Epoch 8:  69% 80/116 [00:04<00:02, 17.77it/s, loss=0.301, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.18it/s, loss=0.301, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.15it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 20.69it/s, loss=0.305, v_num=yywx]\n",
            "Epoch 9:  69% 80/116 [00:04<00:02, 17.90it/s, loss=0.277, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.52it/s, loss=0.277, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.56it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.18it/s, loss=0.269, v_num=yywx]\n",
            "Epoch 10:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.257, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.65it/s, loss=0.257, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.77it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.30it/s, loss=0.273, v_num=yywx]\n",
            "Epoch 11:  69% 80/116 [00:04<00:02, 17.62it/s, loss=0.262, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.24it/s, loss=0.262, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.69it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 20.81it/s, loss=0.274, v_num=yywx]\n",
            "Epoch 12:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.246, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.246, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.22it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.252, v_num=yywx]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.43it/s, loss=0.246, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:04<00:00, 20.05it/s, loss=0.246, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.52it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.71it/s, loss=0.242, v_num=yywx]\n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.23it/s, loss=0.223, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.83it/s, loss=0.223, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.14it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.235, v_num=yywx]\n",
            "Epoch 15:  69% 80/116 [00:04<00:02, 17.79it/s, loss=0.233, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.43it/s, loss=0.233, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.85it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 21.06it/s, loss=0.231, v_num=yywx]\n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.232, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.232, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.16it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.235, v_num=yywx]\n",
            "Epoch 17:  69% 80/116 [00:04<00:02, 17.49it/s, loss=0.218, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.11it/s, loss=0.218, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.21it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 20.71it/s, loss=0.215, v_num=yywx]\n",
            "Epoch 18:  69% 80/116 [00:04<00:02, 17.87it/s, loss=0.215, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.48it/s, loss=0.215, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.10it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.06it/s, loss=0.219, v_num=yywx]\n",
            "Epoch 19:  69% 80/116 [00:04<00:02, 17.86it/s, loss=0.216, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.22it/s, loss=0.216, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.35it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 20.81it/s, loss=0.23, v_num=yywx] \n",
            "Epoch 20:  69% 80/116 [00:04<00:02, 17.86it/s, loss=0.203, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.203, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.09it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 20.97it/s, loss=0.262, v_num=yywx]\n",
            "Epoch 21:  69% 80/116 [00:04<00:02, 17.77it/s, loss=0.211, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.211, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.95it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 20.96it/s, loss=0.215, v_num=yywx]\n",
            "Epoch 22:  69% 80/116 [00:04<00:02, 17.57it/s, loss=0.206, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.14it/s, loss=0.206, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.23it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 20.77it/s, loss=0.22, v_num=yywx] \n",
            "Epoch 23:  69% 80/116 [00:04<00:02, 17.87it/s, loss=0.202, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.43it/s, loss=0.202, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.54it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.218, v_num=yywx]\n",
            "Epoch 24:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.2, v_num=yywx]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.39it/s, loss=0.2, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.37it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.01it/s, loss=0.217, v_num=yywx]\n",
            "Epoch 25:  69% 80/116 [00:04<00:02, 17.14it/s, loss=0.203, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 18.71it/s, loss=0.203, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.02it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 20.35it/s, loss=0.22, v_num=yywx] \n",
            "Epoch 26:  69% 80/116 [00:04<00:02, 17.13it/s, loss=0.201, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 18.75it/s, loss=0.201, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.79it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 20.33it/s, loss=0.206, v_num=yywx]\n",
            "Epoch 27:  69% 80/116 [00:04<00:02, 17.34it/s, loss=0.196, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 18.95it/s, loss=0.196, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.82it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 20.61it/s, loss=0.191, v_num=yywx]\n",
            "Epoch 28:  69% 80/116 [00:04<00:02, 16.65it/s, loss=0.202, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 18.31it/s, loss=0.202, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.78it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 19.91it/s, loss=0.208, v_num=yywx]\n",
            "Epoch 29:  69% 80/116 [00:04<00:02, 17.25it/s, loss=0.194, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 18.84it/s, loss=0.194, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.94it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 20.42it/s, loss=0.193, v_num=yywx]\n",
            "Epoch 30:  69% 80/116 [00:04<00:02, 16.72it/s, loss=0.197, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 17.56it/s, loss=0.197, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 27.20it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:06<00:00, 19.13it/s, loss=0.207, v_num=yywx]\n",
            "Epoch 31:  69% 80/116 [00:04<00:02, 16.94it/s, loss=0.209, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 17.72it/s, loss=0.209, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 27.52it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:06<00:00, 19.30it/s, loss=0.206, v_num=yywx]\n",
            "Epoch 32:  69% 80/116 [00:04<00:02, 17.13it/s, loss=0.194, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 17.95it/s, loss=0.194, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 28.06it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 19.56it/s, loss=0.205, v_num=yywx]\n",
            "Epoch 33:  69% 80/116 [00:04<00:02, 17.25it/s, loss=0.2, v_num=yywx]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 18.07it/s, loss=0.2, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 28.24it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 19.69it/s, loss=0.205, v_num=yywx]\n",
            "Epoch 34:  69% 80/116 [00:04<00:02, 17.07it/s, loss=0.198, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 17.96it/s, loss=0.198, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 28.69it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 19.54it/s, loss=0.212, v_num=yywx]\n",
            "Epoch 35:  69% 80/116 [00:04<00:02, 17.53it/s, loss=0.199, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 18.28it/s, loss=0.199, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 27.41it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 19.88it/s, loss=0.202, v_num=yywx]\n",
            "Epoch 36:  69% 80/116 [00:04<00:02, 17.48it/s, loss=0.196, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 18.37it/s, loss=0.196, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 29.15it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 20.00it/s, loss=0.194, v_num=yywx]\n",
            "Epoch 37:  69% 80/116 [00:05<00:02, 13.40it/s, loss=0.195, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:06<00:01, 15.04it/s, loss=0.195, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.08it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:07<00:00, 16.43it/s, loss=0.201, v_num=yywx]\n",
            "Epoch 38:  69% 80/116 [00:04<00:02, 17.92it/s, loss=0.198, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.61it/s, loss=0.198, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.55it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.26it/s, loss=0.2, v_num=yywx]  \n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.39it/s, loss=0.199, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:04<00:00, 20.03it/s, loss=0.199, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.80it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.68it/s, loss=0.215, v_num=yywx]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.199, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.199, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.76it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.211, v_num=yywx]\n",
            "Epoch 41:  69% 80/116 [00:04<00:02, 17.15it/s, loss=0.194, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 18.67it/s, loss=0.194, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.10it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 20.20it/s, loss=0.235, v_num=yywx]\n",
            "Epoch 42:  69% 80/116 [00:04<00:02, 18.00it/s, loss=0.19, v_num=yywx] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.19, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.68it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.242, v_num=yywx]\n",
            "Epoch 43:  69% 80/116 [00:04<00:02, 17.83it/s, loss=0.202, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.49it/s, loss=0.202, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.08it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.12it/s, loss=0.203, v_num=yywx]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.05it/s, loss=0.193, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.193, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.86it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.14it/s, loss=0.211, v_num=yywx]\n",
            "Epoch 45:  69% 80/116 [00:04<00:02, 17.70it/s, loss=0.194, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.27it/s, loss=0.194, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.63it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 20.81it/s, loss=0.213, v_num=yywx]\n",
            "Epoch 46:  69% 80/116 [00:04<00:02, 17.42it/s, loss=0.202, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 18.98it/s, loss=0.202, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.83it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 20.57it/s, loss=0.214, v_num=yywx]\n",
            "Epoch 47:  69% 80/116 [00:04<00:02, 17.90it/s, loss=0.199, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.56it/s, loss=0.199, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.74it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.19it/s, loss=0.217, v_num=yywx]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.199, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.199, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.05it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.38it/s, loss=0.197, v_num=yywx]\n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.197, v_num=yywx]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.197, v_num=yywx]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.60it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.205, v_num=yywx]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.205, v_num=yywx]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.205, v_num=yywx]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇██████▅▅▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▂▄▆▇▇█▇▇███████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▆▄▃▂▃▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▆▄▃▂▃▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇▅▄▃▄▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▂▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▇▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▆▄▃▂▃▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇▅▄▃▄▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▂▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▆▄▃▂▃▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▆▄▃▄▃▃▁▂▂▂▁▂▁▂▂▂▁▁▂▁▂▁▁▁▂▁▂▁▁▁▂▂▁▂▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▇█▆▄▃▄▃▃▂▂▂▂▂▂▂▃▂▂▂▂▃▁▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▂▃▅▇▇▇▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▅▅▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▇▆▄▃▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▇▆▄▃▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▇▆▅▃▃▃▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape █▁▇▆▅▄▇▅▇▆▅▅▅▅▅▆▅▇▆▄▄▄▄▃▅▃▄▃▄▃▅▄▄▅▅▄▄▃▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▇▆▄▃▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▇▆▅▃▃▃▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▇▆▄▃▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▇▆▅▃▃▃▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▇█▅▄▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.08413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.32937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.32938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.44153\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 0.5375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.32937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.44153\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.32937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.53585\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 0.63532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.08108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.19184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.19185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.34541\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 1.65277\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.19184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.34541\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.19184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.43937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 0.72251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mroyal-sunset-818\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/3c8lyywx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_052235-3c8lyywx/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_052727-13yi760g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisunderstood-thunder-819\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/13yi760g\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 5.44749431e-18 -1.14871076e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 5.44749431e-18 -1.14871076e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:02, 17.96it/s, loss=1.24, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.48it/s, loss=1.24, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.55it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.02it/s, loss=1.24, v_num=760g]\n",
            "Epoch 1:  69% 80/116 [00:04<00:01, 18.11it/s, loss=1.03, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.63it/s, loss=1.03, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.98it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.28it/s, loss=1.03, v_num=760g]\n",
            "Epoch 2:  69% 80/116 [00:04<00:02, 17.36it/s, loss=1.01, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 18.88it/s, loss=1.01, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.43it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 20.46it/s, loss=1.03, v_num=760g]\n",
            "Epoch 3:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.753, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.43it/s, loss=0.753, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.95it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.07it/s, loss=0.741, v_num=760g]\n",
            "Epoch 4:  69% 80/116 [00:04<00:01, 18.03it/s, loss=0.463, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.463, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.04it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.15it/s, loss=0.473, v_num=760g]\n",
            "Epoch 5:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.338, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.56it/s, loss=0.338, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.29it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.344, v_num=760g]\n",
            "Epoch 6:  69% 80/116 [00:04<00:02, 17.87it/s, loss=0.308, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.40it/s, loss=0.308, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.71it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 20.91it/s, loss=0.32, v_num=760g] \n",
            "Epoch 7:  69% 80/116 [00:04<00:02, 17.93it/s, loss=0.283, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.42it/s, loss=0.283, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.33it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.06it/s, loss=0.278, v_num=760g]\n",
            "Epoch 8:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.27, v_num=760g] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.38it/s, loss=0.27, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.00it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 20.98it/s, loss=0.271, v_num=760g]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.00it/s, loss=0.248, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.248, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.02it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.243, v_num=760g]\n",
            "Epoch 10:  69% 80/116 [00:04<00:02, 17.53it/s, loss=0.237, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.15it/s, loss=0.237, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.93it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 20.68it/s, loss=0.235, v_num=760g]\n",
            "Epoch 11:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.227, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.46it/s, loss=0.227, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.97it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.09it/s, loss=0.239, v_num=760g]\n",
            "Epoch 12:  69% 80/116 [00:04<00:01, 18.04it/s, loss=0.212, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.212, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.04it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.24it/s, loss=0.216, v_num=760g]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.212, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.59it/s, loss=0.212, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.73it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.215, v_num=760g]\n",
            "Epoch 14:  69% 80/116 [00:04<00:02, 17.90it/s, loss=0.204, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.40it/s, loss=0.204, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.67it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.02it/s, loss=0.212, v_num=760g]\n",
            "Epoch 15:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.212, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.212, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.11it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.209, v_num=760g]\n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.209, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.209, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.12it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.222, v_num=760g]\n",
            "Epoch 17:  69% 80/116 [00:05<00:02, 13.52it/s, loss=0.205, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:06<00:01, 15.27it/s, loss=0.205, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 44.20it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:06<00:00, 16.75it/s, loss=0.224, v_num=760g]\n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.2, v_num=760g]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.86it/s, loss=0.2, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.61it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.54it/s, loss=0.232, v_num=760g]\n",
            "Epoch 19:  69% 80/116 [00:04<00:02, 17.95it/s, loss=0.199, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.58it/s, loss=0.199, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 44.05it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.16it/s, loss=0.201, v_num=760g]\n",
            "Epoch 20:  69% 80/116 [00:04<00:02, 17.45it/s, loss=0.193, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.00it/s, loss=0.193, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.88it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 20.66it/s, loss=0.215, v_num=760g]\n",
            "Epoch 21:  69% 80/116 [00:04<00:02, 17.39it/s, loss=0.191, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 18.97it/s, loss=0.191, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.47it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 20.52it/s, loss=0.208, v_num=760g]\n",
            "Epoch 22:  69% 80/116 [00:04<00:02, 17.82it/s, loss=0.199, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.199, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.01it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 20.94it/s, loss=0.196, v_num=760g]\n",
            "Epoch 23:  69% 80/116 [00:04<00:02, 17.68it/s, loss=0.193, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.30it/s, loss=0.193, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.09it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 20.88it/s, loss=0.206, v_num=760g]\n",
            "Epoch 24:  69% 80/116 [00:04<00:01, 18.14it/s, loss=0.194, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.62it/s, loss=0.194, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.86it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.211, v_num=760g]\n",
            "Epoch 25:  69% 80/116 [00:04<00:02, 17.85it/s, loss=0.186, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.43it/s, loss=0.186, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.03it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.05it/s, loss=0.196, v_num=760g]\n",
            "Epoch 26:  69% 80/116 [00:04<00:02, 17.87it/s, loss=0.195, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.53it/s, loss=0.195, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.08it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 21.15it/s, loss=0.21, v_num=760g] \n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.198, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.67it/s, loss=0.198, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.73it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.27it/s, loss=0.2, v_num=760g]  \n",
            "Epoch 28:  69% 80/116 [00:04<00:01, 18.14it/s, loss=0.191, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.78it/s, loss=0.191, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.28it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.42it/s, loss=0.191, v_num=760g]\n",
            "Epoch 29:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.196, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.196, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.37it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.192, v_num=760g]\n",
            "Epoch 30:  69% 80/116 [00:04<00:01, 18.15it/s, loss=0.19, v_num=760g] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.75it/s, loss=0.19, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.51it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.38it/s, loss=0.185, v_num=760g]\n",
            "Epoch 31:  69% 80/116 [00:04<00:02, 17.64it/s, loss=0.194, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.24it/s, loss=0.194, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.58it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 20.84it/s, loss=0.193, v_num=760g]\n",
            "Epoch 32:  69% 80/116 [00:04<00:01, 18.05it/s, loss=0.194, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.194, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.72it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 21.26it/s, loss=0.192, v_num=760g]\n",
            "Epoch 33:  69% 80/116 [00:04<00:01, 18.20it/s, loss=0.193, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.193, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.23it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.208, v_num=760g]\n",
            "Epoch 34:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.194, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.194, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.20it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.189, v_num=760g]\n",
            "Epoch 35:  69% 80/116 [00:04<00:01, 18.39it/s, loss=0.189, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.94it/s, loss=0.189, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.03it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.191, v_num=760g]\n",
            "Epoch 36:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.192, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.55it/s, loss=0.192, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.99it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 21.20it/s, loss=0.193, v_num=760g]\n",
            "Epoch 37:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.19, v_num=760g] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.19, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.06it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.205, v_num=760g]\n",
            "Epoch 38:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.189, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.43it/s, loss=0.189, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.09it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.05it/s, loss=0.187, v_num=760g]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.15it/s, loss=0.199, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.199, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.30it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.202, v_num=760g]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.30it/s, loss=0.194, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.194, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.22it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.59it/s, loss=0.193, v_num=760g]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.33it/s, loss=0.189, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.87it/s, loss=0.189, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.37it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.56it/s, loss=0.202, v_num=760g]\n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.193, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.77it/s, loss=0.193, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.73it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.44it/s, loss=0.2, v_num=760g]  \n",
            "Epoch 43:  69% 80/116 [00:04<00:02, 17.78it/s, loss=0.194, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.44it/s, loss=0.194, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.97it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.06it/s, loss=0.221, v_num=760g]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.196, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.196, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.77it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.44it/s, loss=0.199, v_num=760g]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.187, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.81it/s, loss=0.187, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.03it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.192, v_num=760g]\n",
            "Epoch 46:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.192, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.78it/s, loss=0.192, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.16it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.213, v_num=760g]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.192, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.192, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.11it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.20it/s, loss=0.195, v_num=760g]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.191, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.191, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.09it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.39it/s, loss=0.201, v_num=760g]\n",
            "Epoch 49:  69% 80/116 [00:05<00:02, 13.85it/s, loss=0.195, v_num=760g]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:06<00:01, 15.23it/s, loss=0.195, v_num=760g]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.73it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:06<00:00, 16.79it/s, loss=0.199, v_num=760g]\n",
            "Epoch 49: 100% 116/116 [00:06<00:00, 16.76it/s, loss=0.199, v_num=760g]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:06<00:00, 16.60it/s, loss=0.199, v_num=760g]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇██▅▅▅▅▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▁▂▄▆▇▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▆▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▆▇▃▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▆▇▃▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▆▇▄▃▃▃▂▁▂▁▂▂▂▁▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▂▁▁▁▁▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▂▂▂▄▃▂█▁▁▁▂▂▂▁▁▂▂▁▁▃▃▃▃▃▁▂▁▂▁▂▁▁▄▁▂▂▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▆▇▃▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▆▇▄▃▃▃▂▁▂▁▂▂▂▁▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▂▁▁▁▁▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▆▇▃▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▆▄▃▂▃▂▁▂▁▂▂▂▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▆██▄▃▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▂▁▂▂▂▂▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▁▂▅▆▇█████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▅▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▇▆▅▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▇▆▅▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▇▇▅▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape █▂▁▂▄▅▆▆▇▅▅▆▅▅▅▅▅▅▅▅▄▅▄▄▄▅▅▄▄▄▅▅▅▄▄▅▄▅▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▇▆▅▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▇▇▅▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▇▆▅▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▇▇▅▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▇██▅▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 0.98611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.14314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.25666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.25668\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.40024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 1.58147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.25666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.40024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.25666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.67043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 0.95329\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.14477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.18459\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.1846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.33913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.65896\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.18459\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.33913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.18459\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.42471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 0.70733\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmisunderstood-thunder-819\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/13yi760g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_052727-13yi760g/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_053215-360vvmb8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomic-oath-820\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/360vvmb8\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 5.44749431e-18 -8.98688531e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 5.44749431e-18 -8.98688531e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.17it/s, loss=1.11, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.81it/s, loss=1.11, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.25it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.37it/s, loss=1.09, v_num=vmb8]\n",
            "Epoch 1:  69% 80/116 [00:04<00:02, 17.68it/s, loss=1.06, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.26it/s, loss=1.06, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.77it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 20.91it/s, loss=1.04, v_num=vmb8]\n",
            "Epoch 2:  69% 80/116 [00:04<00:02, 17.69it/s, loss=0.992, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.37it/s, loss=0.992, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.20it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 20.95it/s, loss=0.971, v_num=vmb8]\n",
            "Epoch 3:  69% 80/116 [00:04<00:01, 18.09it/s, loss=0.668, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.58it/s, loss=0.668, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.51it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.22it/s, loss=0.666, v_num=vmb8]\n",
            "Epoch 4:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.54, v_num=vmb8] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.54, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.22it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.566, v_num=vmb8]\n",
            "Epoch 5:  69% 80/116 [00:04<00:01, 18.31it/s, loss=0.432, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.75it/s, loss=0.432, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.04it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.38it/s, loss=0.462, v_num=vmb8]\n",
            "Epoch 6:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.393, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.393, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.50it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.4, v_num=vmb8]  \n",
            "Epoch 7:  69% 80/116 [00:04<00:02, 17.96it/s, loss=0.373, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.48it/s, loss=0.373, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.97it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.10it/s, loss=0.37, v_num=vmb8] \n",
            "Epoch 8:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.384, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.384, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.61it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.381, v_num=vmb8]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.23it/s, loss=0.361, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.89it/s, loss=0.361, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.52it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.373, v_num=vmb8]\n",
            "Epoch 10:  69% 80/116 [00:04<00:01, 18.12it/s, loss=0.371, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.371, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.30it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.38, v_num=vmb8] \n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.355, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.355, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.93it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.27it/s, loss=0.392, v_num=vmb8]\n",
            "Epoch 12:  69% 80/116 [00:04<00:02, 17.90it/s, loss=0.345, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.48it/s, loss=0.345, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.02it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.02it/s, loss=0.339, v_num=vmb8]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.333, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.75it/s, loss=0.333, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.16it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.336, v_num=vmb8]\n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.338, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.338, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.42it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.41it/s, loss=0.337, v_num=vmb8]\n",
            "Epoch 15:  69% 80/116 [00:04<00:01, 18.13it/s, loss=0.343, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.343, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.14it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.351, v_num=vmb8]\n",
            "Epoch 16:  69% 80/116 [00:04<00:02, 17.95it/s, loss=0.324, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.41it/s, loss=0.324, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.11it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.04it/s, loss=0.366, v_num=vmb8]\n",
            "Epoch 17:  69% 80/116 [00:04<00:01, 18.35it/s, loss=0.319, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:04<00:00, 20.02it/s, loss=0.319, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.75it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.61it/s, loss=0.316, v_num=vmb8]\n",
            "Epoch 18:  69% 80/116 [00:04<00:02, 17.69it/s, loss=0.322, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.322, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.62it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.33, v_num=vmb8] \n",
            "Epoch 19:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.318, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.318, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.56it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.326, v_num=vmb8]\n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.314, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.87it/s, loss=0.314, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.53it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.308, v_num=vmb8]\n",
            "Epoch 21:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.316, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 19.76it/s, loss=0.316, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.53it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.304, v_num=vmb8]\n",
            "Epoch 22:  69% 80/116 [00:04<00:02, 17.85it/s, loss=0.315, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.52it/s, loss=0.315, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.37it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.323, v_num=vmb8]\n",
            "Epoch 23:  69% 80/116 [00:04<00:02, 17.77it/s, loss=0.308, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.37it/s, loss=0.308, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.43it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.306, v_num=vmb8]\n",
            "Epoch 24:  69% 80/116 [00:04<00:01, 18.12it/s, loss=0.315, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.67it/s, loss=0.315, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.83it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.317, v_num=vmb8]\n",
            "Epoch 25:  69% 80/116 [00:04<00:02, 17.96it/s, loss=0.312, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.54it/s, loss=0.312, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.82it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.315, v_num=vmb8]\n",
            "Epoch 26:  69% 80/116 [00:04<00:01, 18.03it/s, loss=0.307, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.307, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.29it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.311, v_num=vmb8]\n",
            "Epoch 27:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.304, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.304, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.98it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 21.33it/s, loss=0.305, v_num=vmb8]\n",
            "Epoch 28:  69% 80/116 [00:04<00:01, 18.00it/s, loss=0.315, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.61it/s, loss=0.315, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.97it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.26it/s, loss=0.32, v_num=vmb8] \n",
            "Epoch 29:  69% 80/116 [00:04<00:02, 16.08it/s, loss=0.304, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 16.79it/s, loss=0.304, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 29.27it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:06<00:00, 17.93it/s, loss=0.308, v_num=vmb8]\n",
            "Epoch 30:  69% 80/116 [00:04<00:02, 16.01it/s, loss=0.313, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 17.68it/s, loss=0.313, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.16it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:06<00:00, 19.24it/s, loss=0.311, v_num=vmb8]\n",
            "Epoch 31:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.304, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.66it/s, loss=0.304, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.90it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 21.27it/s, loss=0.319, v_num=vmb8]\n",
            "Epoch 32:  69% 80/116 [00:04<00:02, 17.98it/s, loss=0.311, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.311, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.56it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 21.19it/s, loss=0.327, v_num=vmb8]\n",
            "Epoch 33:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.315, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.92it/s, loss=0.315, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.62it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.53it/s, loss=0.329, v_num=vmb8]\n",
            "Epoch 34:  69% 80/116 [00:04<00:02, 17.61it/s, loss=0.313, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.23it/s, loss=0.313, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.91it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 20.86it/s, loss=0.314, v_num=vmb8]\n",
            "Epoch 35:  69% 80/116 [00:04<00:01, 18.34it/s, loss=0.299, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.97it/s, loss=0.299, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.93it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.62it/s, loss=0.302, v_num=vmb8]\n",
            "Epoch 36:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.304, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.89it/s, loss=0.304, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.84it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 21.57it/s, loss=0.311, v_num=vmb8]\n",
            "Epoch 37:  69% 80/116 [00:04<00:01, 18.27it/s, loss=0.308, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.95it/s, loss=0.308, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.61it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 21.58it/s, loss=0.312, v_num=vmb8]\n",
            "Epoch 38:  69% 80/116 [00:04<00:01, 18.36it/s, loss=0.302, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.99it/s, loss=0.302, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.04it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 21.64it/s, loss=0.301, v_num=vmb8]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.40it/s, loss=0.289, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.99it/s, loss=0.289, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.84it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.64it/s, loss=0.28, v_num=vmb8] \n",
            "Epoch 40:  69% 80/116 [00:04<00:02, 17.90it/s, loss=0.312, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.48it/s, loss=0.312, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.76it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.07it/s, loss=0.308, v_num=vmb8]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.46it/s, loss=0.303, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:04<00:00, 20.03it/s, loss=0.303, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.70it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.68it/s, loss=0.312, v_num=vmb8]\n",
            "Epoch 42:  69% 80/116 [00:04<00:02, 17.90it/s, loss=0.303, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.303, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.61it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.304, v_num=vmb8]\n",
            "Epoch 43:  69% 80/116 [00:04<00:02, 17.24it/s, loss=0.303, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 18.88it/s, loss=0.303, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.39it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 20.52it/s, loss=0.304, v_num=vmb8]\n",
            "Epoch 44:  69% 80/116 [00:04<00:02, 17.23it/s, loss=0.306, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 18.86it/s, loss=0.306, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.86it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 20.41it/s, loss=0.308, v_num=vmb8]\n",
            "Epoch 45:  69% 80/116 [00:04<00:02, 17.05it/s, loss=0.302, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 18.58it/s, loss=0.302, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.56it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 20.17it/s, loss=0.311, v_num=vmb8]\n",
            "Epoch 46:  69% 80/116 [00:04<00:02, 17.49it/s, loss=0.296, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.06it/s, loss=0.296, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.80it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 20.66it/s, loss=0.314, v_num=vmb8]\n",
            "Epoch 47:  69% 80/116 [00:04<00:02, 17.44it/s, loss=0.308, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.12it/s, loss=0.308, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.98it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 20.75it/s, loss=0.324, v_num=vmb8]\n",
            "Epoch 48:  69% 80/116 [00:04<00:02, 17.36it/s, loss=0.286, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.04it/s, loss=0.286, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.89it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 20.67it/s, loss=0.285, v_num=vmb8]\n",
            "Epoch 49:  69% 80/116 [00:04<00:02, 17.29it/s, loss=0.302, v_num=vmb8]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 18.91it/s, loss=0.302, v_num=vmb8]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.19it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 20.47it/s, loss=0.297, v_num=vmb8]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 20.46it/s, loss=0.297, v_num=vmb8]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 20.43it/s, loss=0.297, v_num=vmb8]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇███▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▆███████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▆▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss ██▆▄▃▂▂▂▁▂▂▂▁▂▂▁▂▃▁▁▂▁▁▁▁▁▁▂▁▂▁▂▁▂▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ██▆▄▃▂▂▂▁▂▂▂▁▂▂▁▂▃▁▁▂▁▁▁▁▁▁▂▁▂▁▂▁▂▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae ██▇▅▃▂▂▂▂▂▂▂▂▂▂▁▂▃▂▂▂▁▁▂▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▁▂▁▁▁▁▁▁▆█▁▁▁▁▁▂▁▁▂▁▁▁▁▃▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse ██▆▄▃▂▂▂▁▂▂▂▁▂▂▁▂▃▁▁▂▁▁▁▁▁▁▂▁▂▁▂▁▂▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae ██▇▅▃▂▂▂▂▂▂▂▂▂▂▁▂▃▂▂▂▁▁▂▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse ██▆▄▃▂▂▂▁▂▂▂▁▂▂▁▂▃▁▁▂▁▁▁▁▁▁▂▁▂▁▂▁▂▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse ██▇▅▃▃▃▃▂▂▂▂▁▂▂▂▃▃▂▂▂▂▂▁▁▁▃▃▂▂▂▂▁▂▁▂▂▂▂▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ███▅▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▃▂▂▂▁▂▂▂▂▂▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▃▇███████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss ██▇▅▃▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss ██▇▅▃▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae ██▇▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▃▁▁██▆▆▆▇▆▆▅█▆▆▇▆▇▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse ██▇▅▃▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae ██▇▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse ██▇▅▃▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse ██▇▅▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ███▅▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.12775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.28686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.28687\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.49785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 2.16104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.28686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.49785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.28686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.72139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.17898\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.12923\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.30689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.3069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.42916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.78041\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.30689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.42916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.30689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.54557\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 0.83136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcomic-oath-820\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/360vvmb8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_053215-360vvmb8/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_053701-1g1s8xoa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-monkey-821\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/1g1s8xoa\u001b[0m\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 5.44749431e-18 -8.98688531e-17 -1.14871076e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "(21, 250, 3)\n",
            "(9, 250, 3)\n",
            "(30, 250, 3)\n",
            "yc.shape: (30, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 5.44749431e-18 -8.98688531e-17 -1.14871076e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.058     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:01, 18.42it/s, loss=1, v_num=8xoa]   \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:04<00:00, 20.04it/s, loss=1, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.11it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.68it/s, loss=1.01, v_num=8xoa]\n",
            "Epoch 1:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.973, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.973, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.30it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.942, v_num=8xoa]\n",
            "Epoch 2:  69% 80/116 [00:04<00:01, 18.33it/s, loss=0.81, v_num=8xoa] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.87it/s, loss=0.81, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.82it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.56it/s, loss=0.796, v_num=8xoa]\n",
            "Epoch 3:  69% 80/116 [00:04<00:01, 18.45it/s, loss=0.611, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.88it/s, loss=0.611, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.88it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 21.48it/s, loss=0.623, v_num=8xoa]\n",
            "Epoch 4:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.511, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.78it/s, loss=0.511, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.45it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 21.24it/s, loss=0.507, v_num=8xoa]\n",
            "Epoch 5:  69% 80/116 [00:04<00:02, 17.88it/s, loss=0.456, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.52it/s, loss=0.456, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.01it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.20it/s, loss=0.471, v_num=8xoa]\n",
            "Epoch 6:  69% 80/116 [00:04<00:01, 18.26it/s, loss=0.369, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.89it/s, loss=0.369, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.43it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.383, v_num=8xoa]\n",
            "Epoch 7:  69% 80/116 [00:04<00:02, 17.89it/s, loss=0.355, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.47it/s, loss=0.355, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.22it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.08it/s, loss=0.346, v_num=8xoa]\n",
            "Epoch 8:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.348, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.67it/s, loss=0.348, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.93it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.348, v_num=8xoa]\n",
            "Epoch 9:  69% 80/116 [00:04<00:02, 17.82it/s, loss=0.353, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.353, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.66it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 20.95it/s, loss=0.351, v_num=8xoa]\n",
            "Epoch 10:  69% 80/116 [00:05<00:02, 14.25it/s, loss=0.355, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:06<00:01, 15.21it/s, loss=0.355, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 30.76it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:06<00:00, 16.71it/s, loss=0.356, v_num=8xoa]\n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.23it/s, loss=0.34, v_num=8xoa] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.34, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.27it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.346, v_num=8xoa]\n",
            "Epoch 12:  69% 80/116 [00:04<00:01, 18.00it/s, loss=0.332, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.332, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.96it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.351, v_num=8xoa]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.31, v_num=8xoa] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.86it/s, loss=0.31, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.66it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.309, v_num=8xoa]\n",
            "Epoch 14:  69% 80/116 [00:04<00:01, 18.22it/s, loss=0.325, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.325, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.89it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.334, v_num=8xoa]\n",
            "Epoch 15:  69% 80/116 [00:04<00:01, 18.06it/s, loss=0.318, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.68it/s, loss=0.318, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.05it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 21.30it/s, loss=0.32, v_num=8xoa] \n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.327, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.327, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.10it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.343, v_num=8xoa]\n",
            "Epoch 17:  69% 80/116 [00:04<00:02, 17.78it/s, loss=0.32, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.36it/s, loss=0.32, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.65it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.02it/s, loss=0.331, v_num=8xoa]\n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.309, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.86it/s, loss=0.309, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.75it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.49it/s, loss=0.315, v_num=8xoa]\n",
            "Epoch 19:  69% 80/116 [00:04<00:01, 18.39it/s, loss=0.316, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.85it/s, loss=0.316, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 38.99it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.316, v_num=8xoa]\n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.306, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.306, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.53it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.26it/s, loss=0.331, v_num=8xoa]\n",
            "Epoch 21:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.318, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 19.53it/s, loss=0.318, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.86it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.334, v_num=8xoa]\n",
            "Epoch 22:  69% 80/116 [00:04<00:02, 17.86it/s, loss=0.306, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.46it/s, loss=0.306, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.35it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.311, v_num=8xoa]\n",
            "Epoch 23:  69% 80/116 [00:04<00:02, 17.91it/s, loss=0.321, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.53it/s, loss=0.321, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.11it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.339, v_num=8xoa]\n",
            "Epoch 24:  69% 80/116 [00:04<00:02, 17.87it/s, loss=0.309, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:05<00:00, 19.38it/s, loss=0.309, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.45it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.317, v_num=8xoa]\n",
            "Epoch 25:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.303, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.65it/s, loss=0.303, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.99it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.14it/s, loss=0.311, v_num=8xoa]\n",
            "Epoch 26:  69% 80/116 [00:04<00:02, 17.83it/s, loss=0.314, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.41it/s, loss=0.314, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.35it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.311, v_num=8xoa]\n",
            "Epoch 27:  69% 80/116 [00:04<00:02, 17.71it/s, loss=0.308, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.27it/s, loss=0.308, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.75it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 20.92it/s, loss=0.315, v_num=8xoa]\n",
            "Epoch 28:  69% 80/116 [00:04<00:02, 17.97it/s, loss=0.31, v_num=8xoa] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.31, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.58it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 21.24it/s, loss=0.298, v_num=8xoa]\n",
            "Epoch 29:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.317, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 19.65it/s, loss=0.317, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.06it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.318, v_num=8xoa]\n",
            "Epoch 30:  69% 80/116 [00:04<00:01, 18.07it/s, loss=0.306, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.62it/s, loss=0.306, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.02it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 21.27it/s, loss=0.303, v_num=8xoa]\n",
            "Epoch 31:  69% 80/116 [00:04<00:01, 18.04it/s, loss=0.304, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 19.60it/s, loss=0.304, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.44it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.332, v_num=8xoa]\n",
            "Epoch 32:  69% 80/116 [00:04<00:02, 17.81it/s, loss=0.32, v_num=8xoa] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 19.39it/s, loss=0.32, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.48it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 20.88it/s, loss=0.346, v_num=8xoa]\n",
            "Epoch 33:  69% 80/116 [00:04<00:02, 17.98it/s, loss=0.314, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.314, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.83it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 21.22it/s, loss=0.304, v_num=8xoa]\n",
            "Epoch 34:  69% 80/116 [00:04<00:01, 18.38it/s, loss=0.308, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:04<00:00, 20.00it/s, loss=0.308, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.94it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 21.64it/s, loss=0.299, v_num=8xoa]\n",
            "Epoch 35:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.318, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.318, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 44.02it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 21.37it/s, loss=0.323, v_num=8xoa]\n",
            "Epoch 36:  69% 80/116 [00:04<00:01, 18.14it/s, loss=0.308, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.308, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.23it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 21.31it/s, loss=0.327, v_num=8xoa]\n",
            "Epoch 37:  69% 80/116 [00:04<00:02, 17.67it/s, loss=0.307, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.29it/s, loss=0.307, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.18it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 20.93it/s, loss=0.335, v_num=8xoa]\n",
            "Epoch 38:  69% 80/116 [00:04<00:02, 17.61it/s, loss=0.317, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.30it/s, loss=0.317, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.64it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 20.90it/s, loss=0.317, v_num=8xoa]\n",
            "Epoch 39:  69% 80/116 [00:04<00:01, 18.28it/s, loss=0.299, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.80it/s, loss=0.299, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.61it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.312, v_num=8xoa]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.19it/s, loss=0.301, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.83it/s, loss=0.301, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.43it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.46it/s, loss=0.302, v_num=8xoa]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.14it/s, loss=0.312, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.312, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.08it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.356, v_num=8xoa]\n",
            "Epoch 42:  69% 80/116 [00:04<00:02, 17.88it/s, loss=0.314, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 18.23it/s, loss=0.314, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 29.39it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:06<00:00, 19.31it/s, loss=0.308, v_num=8xoa]\n",
            "Epoch 43:  69% 80/116 [00:05<00:02, 14.84it/s, loss=0.311, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:06<00:00, 16.36it/s, loss=0.311, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.72it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:06<00:00, 17.86it/s, loss=0.315, v_num=8xoa]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.314, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.314, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.87it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.42it/s, loss=0.34, v_num=8xoa] \n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.18it/s, loss=0.307, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.78it/s, loss=0.307, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.91it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.45it/s, loss=0.305, v_num=8xoa]\n",
            "Epoch 46:  69% 80/116 [00:04<00:01, 18.12it/s, loss=0.305, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.69it/s, loss=0.305, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.22it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.25it/s, loss=0.324, v_num=8xoa]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.25it/s, loss=0.305, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.84it/s, loss=0.305, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.87it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.32, v_num=8xoa] \n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.09it/s, loss=0.306, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.306, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.32it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.30it/s, loss=0.329, v_num=8xoa]\n",
            "Epoch 49:  69% 80/116 [00:04<00:02, 17.69it/s, loss=0.305, v_num=8xoa]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.22it/s, loss=0.305, v_num=8xoa]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.57it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 20.85it/s, loss=0.293, v_num=8xoa]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 20.81it/s, loss=0.293, v_num=8xoa]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 20.78it/s, loss=0.293, v_num=8xoa]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇█████▅▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▄▆▇▇▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▆▅▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇▆▅▄▃▂▃▃▃▃▃▃▃▃▂▃▂▂▃▂▃▃▃▂▂▃▂▂▂▃▃▂▂▂▃▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇▆▅▄▃▂▃▃▃▃▃▃▃▃▂▃▂▂▃▂▃▃▃▂▂▃▂▂▂▃▃▂▂▂▃▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇▆▅▄▃▃▃▃▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▂▃▂▃▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇▆▅▄▃▂▃▃▃▃▃▃▃▃▂▃▂▂▃▂▃▃▃▂▂▃▂▂▂▃▃▂▂▂▃▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇▆▅▄▃▃▃▃▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▂▃▂▃▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇▆▅▄▃▂▃▃▃▃▃▃▃▃▂▃▂▂▃▂▃▃▃▂▂▃▂▂▂▃▃▂▂▂▃▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse ██▆▄▄▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▂▁▂▃▁▂▂▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ██▄▃▃▂▁▁▂▂▁▁▁▂▂▁▂▁▁▂▂▂▁▂▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▃▆▇▇▇████████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss ██▆▄▂▂▁▁▁▁▁▁▁▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss ██▆▄▂▂▁▁▁▁▁▁▁▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae ██▆▄▃▂▂▂▁▁▂▂▁▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape ▁▁▂▃▃▄▄▄▄▄▄▄▄█▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse ██▆▄▂▂▁▁▁▁▁▁▁▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae ██▆▄▃▂▂▂▁▁▂▂▁▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse ██▆▄▂▂▁▁▁▁▁▁▁▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse ██▆▄▃▂▁▂▁▁▂▁▁▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ██▅▃▂▂▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.09266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.12718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.12719\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.28987\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 1.09385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.12718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.28987\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.12718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.52939\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 0.8078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 0.99995\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.09107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.30986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.30986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.42634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.81471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.30986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.42634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.30986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.56162\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 0.83626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrose-monkey-821\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/1g1s8xoa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_053701-1g1s8xoa/logs\u001b[0m\n",
            "args.run_name: generated_gc_SNR_standardized_3\n",
            "warning: CRLF will be replaced by LF in spacetimeformer/spacetimeformer/spacetimeformer_model/nn/model.py.\n",
            "The file will have its original line endings in your working directory.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/.shortcut-targets-by-id/1LjEIbosqEOG_S80QdYcVNBS0kSCwr2zq/EEG-connectivity-estimate-with-transformers/spacetimeformer/spacetimeformer/wandb/run-20221017_054148-3ugsvijn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclean-surf-822\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/3ugsvijn\u001b[0m\n",
            "(30, 250, 4)\n",
            "yc.shape: (30, 250, 4)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 5.44749431e-18 -8.98688531e-17 -1.14871076e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1. 1.]\n",
            "\n",
            "(21, 250, 4)\n",
            "(9, 250, 4)\n",
            "(30, 250, 4)\n",
            "yc.shape: (30, 250, 4)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 5.44749431e-18 -8.98688531e-17 -1.14871076e-17 -2.16567505e-17]\n",
            "std_dev yc: [1. 1. 1. 1.]\n",
            "\n",
            "config.context_points 6\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=15, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=15, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 15\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 21\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type            | Params\n",
            "----------------------------------------------------\n",
            "0 | spacetimeformer | Spacetimeformer | 14.6 K\n",
            "----------------------------------------------------\n",
            "14.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "14.6 K    Total params\n",
            "0.059     Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 9\n",
            "OVERFIT:  False True\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [108, 17, 217, 178, 227, 180, 56, 138, 109, 147, 58, 64, 232, 118, 151, 170, 223, 99, 119, 196, 90, 112, 60, 22, 226, 207, 182, 68, 130, 51, 208, 16, 228, 168, 181, 44, 82, 230, 188, 59, 98, 209, 79, 216, 211, 100, 198, 136, 134, 43, 40, 171, 36, 83, 127, 120, 6, 159, 191, 203, 161, 113, 146, 73, 13, 139, 124, 229, 26, 32, 97, 131, 15, 104, 186, 67, 233, 107, 125, 141, 24, 55, 116, 91, 148, 239, 47, 105, 242, 153, 129, 152, 62, 57, 37, 189, 195, 220, 143, 1, 49, 7, 28, 175, 132, 157, 110, 8, 11, 81, 190, 224, 72, 187, 21, 19, 114, 128, 162, 42, 154, 194, 234, 63, 89, 14, 144, 31, 87, 212, 243, 123, 215, 27, 237, 76, 84, 103, 219, 18, 201, 238, 78, 12, 61, 122, 197, 75, 241, 140, 213, 92, 169, 149, 200, 41, 199, 184, 9, 158, 177, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 163, 117, 164, 126, 206, 222, 179, 192, 145, 93, 236, 142, 66, 174, 52, 231, 10, 4, 218, 39, 205, 88, 165, 221, 133, 167, 34, 65, 29, 202, 204, 115, 70, 5, 20, 160, 101, 121, 225, 46, 176, 71, 77, 50, 166, 155, 30, 48, 25, 156, 135, 172, 210, 235, 193, 183, 240, 54, 0, 80, 106, 173, 102, 3, 33, 185, 214]\n",
            "self.series.num_trials(split): 21\n",
            "Epoch 0:  69% 80/116 [00:04<00:02, 17.88it/s, loss=1.58, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 100/116 [00:05<00:00, 19.44it/s, loss=1.58, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.91it/s]\u001b[A\n",
            "Epoch 0: 100% 116/116 [00:05<00:00, 21.01it/s, loss=1.57, v_num=vijn]\n",
            "Epoch 1:  69% 80/116 [00:04<00:02, 17.91it/s, loss=1.15, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  86% 100/116 [00:05<00:00, 19.50it/s, loss=1.15, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.41it/s]\u001b[A\n",
            "Epoch 1: 100% 116/116 [00:05<00:00, 21.10it/s, loss=1.19, v_num=vijn]\n",
            "Epoch 2:  69% 80/116 [00:04<00:02, 17.80it/s, loss=1.06, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  86% 100/116 [00:05<00:00, 19.49it/s, loss=1.06, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.84it/s]\u001b[A\n",
            "Epoch 2: 100% 116/116 [00:05<00:00, 21.10it/s, loss=1.05, v_num=vijn]\n",
            "Epoch 3:  69% 80/116 [00:04<00:02, 17.59it/s, loss=0.956, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  86% 100/116 [00:05<00:00, 19.19it/s, loss=0.956, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.30it/s]\u001b[A\n",
            "Epoch 3: 100% 116/116 [00:05<00:00, 20.86it/s, loss=0.96, v_num=vijn] \n",
            "Epoch 4:  69% 80/116 [00:04<00:02, 17.92it/s, loss=0.843, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  86% 100/116 [00:05<00:00, 19.40it/s, loss=0.843, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.39it/s]\u001b[A\n",
            "Epoch 4: 100% 116/116 [00:05<00:00, 20.98it/s, loss=0.846, v_num=vijn]\n",
            "Epoch 5:  69% 80/116 [00:04<00:01, 18.10it/s, loss=0.517, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.517, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.04it/s]\u001b[A\n",
            "Epoch 5: 100% 116/116 [00:05<00:00, 21.35it/s, loss=0.507, v_num=vijn]\n",
            "Epoch 6:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.383, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  86% 100/116 [00:05<00:00, 19.70it/s, loss=0.383, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.31it/s]\u001b[A\n",
            "Epoch 6: 100% 116/116 [00:05<00:00, 21.32it/s, loss=0.386, v_num=vijn]\n",
            "Epoch 7:  69% 80/116 [00:04<00:02, 17.95it/s, loss=0.338, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  86% 100/116 [00:05<00:00, 19.56it/s, loss=0.338, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.57it/s]\u001b[A\n",
            "Epoch 7: 100% 116/116 [00:05<00:00, 21.18it/s, loss=0.341, v_num=vijn]\n",
            "Epoch 8:  69% 80/116 [00:04<00:02, 17.82it/s, loss=0.305, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  86% 100/116 [00:05<00:00, 19.25it/s, loss=0.305, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 37.98it/s]\u001b[A\n",
            "Epoch 8: 100% 116/116 [00:05<00:00, 20.74it/s, loss=0.326, v_num=vijn]\n",
            "Epoch 9:  69% 80/116 [00:04<00:01, 18.03it/s, loss=0.276, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  86% 100/116 [00:05<00:00, 19.59it/s, loss=0.276, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.93it/s]\u001b[A\n",
            "Epoch 9: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.274, v_num=vijn]\n",
            "Epoch 10:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.265, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.265, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.01it/s]\u001b[A\n",
            "Epoch 10: 100% 116/116 [00:05<00:00, 21.36it/s, loss=0.261, v_num=vijn]\n",
            "Epoch 11:  69% 80/116 [00:04<00:01, 18.21it/s, loss=0.252, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  86% 100/116 [00:05<00:00, 19.74it/s, loss=0.252, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.40it/s]\u001b[A\n",
            "Epoch 11: 100% 116/116 [00:05<00:00, 21.33it/s, loss=0.25, v_num=vijn] \n",
            "Epoch 12:  69% 80/116 [00:04<00:02, 17.86it/s, loss=0.256, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  86% 100/116 [00:05<00:00, 19.51it/s, loss=0.256, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.87it/s]\u001b[A\n",
            "Epoch 12: 100% 116/116 [00:05<00:00, 21.09it/s, loss=0.259, v_num=vijn]\n",
            "Epoch 13:  69% 80/116 [00:04<00:01, 18.02it/s, loss=0.25, v_num=vijn] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  86% 100/116 [00:05<00:00, 19.60it/s, loss=0.25, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.83it/s]\u001b[A\n",
            "Epoch 13: 100% 116/116 [00:05<00:00, 21.14it/s, loss=0.255, v_num=vijn]\n",
            "Epoch 14:  69% 80/116 [00:04<00:02, 17.82it/s, loss=0.246, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  86% 100/116 [00:05<00:00, 19.43it/s, loss=0.246, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.34it/s]\u001b[A\n",
            "Epoch 14: 100% 116/116 [00:05<00:00, 20.96it/s, loss=0.257, v_num=vijn]\n",
            "Epoch 15:  69% 80/116 [00:04<00:02, 17.74it/s, loss=0.24, v_num=vijn] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  86% 100/116 [00:05<00:00, 19.25it/s, loss=0.24, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.94it/s]\u001b[A\n",
            "Epoch 15: 100% 116/116 [00:05<00:00, 20.83it/s, loss=0.242, v_num=vijn]\n",
            "Epoch 16:  69% 80/116 [00:04<00:01, 18.05it/s, loss=0.233, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  86% 100/116 [00:05<00:00, 19.69it/s, loss=0.233, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.84it/s]\u001b[A\n",
            "Epoch 16: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.233, v_num=vijn]\n",
            "Epoch 17:  69% 80/116 [00:04<00:02, 17.92it/s, loss=0.221, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  86% 100/116 [00:05<00:00, 19.45it/s, loss=0.221, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.24it/s]\u001b[A\n",
            "Epoch 17: 100% 116/116 [00:05<00:00, 21.10it/s, loss=0.247, v_num=vijn]\n",
            "Epoch 18:  69% 80/116 [00:04<00:01, 18.08it/s, loss=0.236, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  86% 100/116 [00:05<00:00, 19.62it/s, loss=0.236, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.15it/s]\u001b[A\n",
            "Epoch 18: 100% 116/116 [00:05<00:00, 21.21it/s, loss=0.25, v_num=vijn] \n",
            "Epoch 19:  69% 80/116 [00:04<00:02, 17.99it/s, loss=0.228, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  86% 100/116 [00:05<00:00, 19.51it/s, loss=0.228, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.63it/s]\u001b[A\n",
            "Epoch 19: 100% 116/116 [00:05<00:00, 20.99it/s, loss=0.239, v_num=vijn]\n",
            "Epoch 20:  69% 80/116 [00:04<00:01, 18.00it/s, loss=0.229, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  86% 100/116 [00:05<00:00, 19.50it/s, loss=0.229, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.82it/s]\u001b[A\n",
            "Epoch 20: 100% 116/116 [00:05<00:00, 21.13it/s, loss=0.231, v_num=vijn]\n",
            "Epoch 21:  69% 80/116 [00:04<00:01, 18.05it/s, loss=0.225, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  86% 100/116 [00:05<00:00, 19.67it/s, loss=0.225, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.43it/s]\u001b[A\n",
            "Epoch 21: 100% 116/116 [00:05<00:00, 21.29it/s, loss=0.231, v_num=vijn]\n",
            "Epoch 22:  69% 80/116 [00:04<00:02, 17.86it/s, loss=0.222, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  86% 100/116 [00:05<00:00, 19.50it/s, loss=0.222, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.66it/s]\u001b[A\n",
            "Epoch 22: 100% 116/116 [00:05<00:00, 21.11it/s, loss=0.238, v_num=vijn]\n",
            "Epoch 23:  69% 80/116 [00:04<00:01, 18.03it/s, loss=0.228, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  86% 100/116 [00:05<00:00, 19.02it/s, loss=0.228, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 31.40it/s]\u001b[A\n",
            "Epoch 23: 100% 116/116 [00:05<00:00, 20.08it/s, loss=0.222, v_num=vijn]\n",
            "Epoch 24:  69% 80/116 [00:05<00:02, 13.65it/s, loss=0.222, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  86% 100/116 [00:06<00:01, 15.36it/s, loss=0.222, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.32it/s]\u001b[A\n",
            "Epoch 24: 100% 116/116 [00:06<00:00, 16.76it/s, loss=0.226, v_num=vijn]\n",
            "Epoch 25:  69% 80/116 [00:04<00:02, 18.00it/s, loss=0.219, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  86% 100/116 [00:05<00:00, 19.63it/s, loss=0.219, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.83it/s]\u001b[A\n",
            "Epoch 25: 100% 116/116 [00:05<00:00, 21.23it/s, loss=0.217, v_num=vijn]\n",
            "Epoch 26:  69% 80/116 [00:04<00:02, 17.74it/s, loss=0.233, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  86% 100/116 [00:05<00:00, 19.30it/s, loss=0.233, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.82it/s]\u001b[A\n",
            "Epoch 26: 100% 116/116 [00:05<00:00, 20.95it/s, loss=0.235, v_num=vijn]\n",
            "Epoch 27:  69% 80/116 [00:04<00:02, 17.50it/s, loss=0.221, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  86% 100/116 [00:05<00:00, 19.11it/s, loss=0.221, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.21it/s]\u001b[A\n",
            "Epoch 27: 100% 116/116 [00:05<00:00, 20.63it/s, loss=0.226, v_num=vijn]\n",
            "Epoch 28:  69% 80/116 [00:04<00:02, 16.90it/s, loss=0.221, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  86% 100/116 [00:05<00:00, 18.50it/s, loss=0.221, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.72it/s]\u001b[A\n",
            "Epoch 28: 100% 116/116 [00:05<00:00, 20.04it/s, loss=0.252, v_num=vijn]\n",
            "Epoch 29:  69% 80/116 [00:04<00:02, 17.27it/s, loss=0.219, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  86% 100/116 [00:05<00:00, 18.83it/s, loss=0.219, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.01it/s]\u001b[A\n",
            "Epoch 29: 100% 116/116 [00:05<00:00, 20.39it/s, loss=0.22, v_num=vijn] \n",
            "Epoch 30:  69% 80/116 [00:04<00:02, 17.53it/s, loss=0.221, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  86% 100/116 [00:05<00:00, 19.13it/s, loss=0.221, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.75it/s]\u001b[A\n",
            "Epoch 30: 100% 116/116 [00:05<00:00, 20.69it/s, loss=0.223, v_num=vijn]\n",
            "Epoch 31:  69% 80/116 [00:04<00:02, 17.35it/s, loss=0.213, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  86% 100/116 [00:05<00:00, 18.78it/s, loss=0.213, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.53it/s]\u001b[A\n",
            "Epoch 31: 100% 116/116 [00:05<00:00, 20.29it/s, loss=0.21, v_num=vijn] \n",
            "Epoch 32:  69% 80/116 [00:04<00:02, 17.33it/s, loss=0.223, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  86% 100/116 [00:05<00:00, 18.88it/s, loss=0.223, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.27it/s]\u001b[A\n",
            "Epoch 32: 100% 116/116 [00:05<00:00, 20.52it/s, loss=0.228, v_num=vijn]\n",
            "Epoch 33:  69% 80/116 [00:04<00:02, 17.56it/s, loss=0.219, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  86% 100/116 [00:05<00:00, 19.06it/s, loss=0.219, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.45it/s]\u001b[A\n",
            "Epoch 33: 100% 116/116 [00:05<00:00, 20.55it/s, loss=0.223, v_num=vijn]\n",
            "Epoch 34:  69% 80/116 [00:04<00:02, 17.59it/s, loss=0.217, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  86% 100/116 [00:05<00:00, 19.17it/s, loss=0.217, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.53it/s]\u001b[A\n",
            "Epoch 34: 100% 116/116 [00:05<00:00, 20.76it/s, loss=0.226, v_num=vijn]\n",
            "Epoch 35:  69% 80/116 [00:04<00:02, 17.72it/s, loss=0.222, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  86% 100/116 [00:05<00:00, 19.24it/s, loss=0.222, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.24it/s]\u001b[A\n",
            "Epoch 35: 100% 116/116 [00:05<00:00, 20.78it/s, loss=0.239, v_num=vijn]\n",
            "Epoch 36:  69% 80/116 [00:04<00:02, 17.73it/s, loss=0.213, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  86% 100/116 [00:05<00:00, 19.31it/s, loss=0.213, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.01it/s]\u001b[A\n",
            "Epoch 36: 100% 116/116 [00:05<00:00, 20.95it/s, loss=0.215, v_num=vijn]\n",
            "Epoch 37:  69% 80/116 [00:04<00:02, 17.73it/s, loss=0.212, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  86% 100/116 [00:05<00:00, 19.27it/s, loss=0.212, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.82it/s]\u001b[A\n",
            "Epoch 37: 100% 116/116 [00:05<00:00, 20.89it/s, loss=0.221, v_num=vijn]\n",
            "Epoch 38:  69% 80/116 [00:04<00:02, 17.83it/s, loss=0.213, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  86% 100/116 [00:05<00:00, 19.34it/s, loss=0.213, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 39.99it/s]\u001b[A\n",
            "Epoch 38: 100% 116/116 [00:05<00:00, 20.91it/s, loss=0.217, v_num=vijn]\n",
            "Epoch 39:  69% 80/116 [00:04<00:02, 17.79it/s, loss=0.214, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  86% 100/116 [00:05<00:00, 19.36it/s, loss=0.214, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.86it/s]\u001b[A\n",
            "Epoch 39: 100% 116/116 [00:05<00:00, 21.00it/s, loss=0.215, v_num=vijn]\n",
            "Epoch 40:  69% 80/116 [00:04<00:01, 18.17it/s, loss=0.224, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  86% 100/116 [00:05<00:00, 19.71it/s, loss=0.224, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.34it/s]\u001b[A\n",
            "Epoch 40: 100% 116/116 [00:05<00:00, 21.33it/s, loss=0.227, v_num=vijn]\n",
            "Epoch 41:  69% 80/116 [00:04<00:01, 18.01it/s, loss=0.214, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.214, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.69it/s]\u001b[A\n",
            "Epoch 41: 100% 116/116 [00:05<00:00, 21.22it/s, loss=0.22, v_num=vijn] \n",
            "Epoch 42:  69% 80/116 [00:04<00:01, 18.09it/s, loss=0.217, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  86% 100/116 [00:05<00:00, 19.64it/s, loss=0.217, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.04it/s]\u001b[A\n",
            "Epoch 42: 100% 116/116 [00:05<00:00, 21.28it/s, loss=0.226, v_num=vijn]\n",
            "Epoch 43:  69% 80/116 [00:04<00:02, 18.00it/s, loss=0.22, v_num=vijn] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  86% 100/116 [00:05<00:00, 19.57it/s, loss=0.22, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.49it/s]\u001b[A\n",
            "Epoch 43: 100% 116/116 [00:05<00:00, 21.14it/s, loss=0.222, v_num=vijn]\n",
            "Epoch 44:  69% 80/116 [00:04<00:01, 18.16it/s, loss=0.219, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  86% 100/116 [00:05<00:00, 19.79it/s, loss=0.219, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.19it/s]\u001b[A\n",
            "Epoch 44: 100% 116/116 [00:05<00:00, 21.43it/s, loss=0.225, v_num=vijn]\n",
            "Epoch 45:  69% 80/116 [00:04<00:01, 18.11it/s, loss=0.218, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  86% 100/116 [00:05<00:00, 19.72it/s, loss=0.218, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 43.40it/s]\u001b[A\n",
            "Epoch 45: 100% 116/116 [00:05<00:00, 21.34it/s, loss=0.224, v_num=vijn]\n",
            "Epoch 46:  69% 80/116 [00:04<00:01, 18.32it/s, loss=0.213, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  86% 100/116 [00:05<00:00, 19.93it/s, loss=0.213, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 42.65it/s]\u001b[A\n",
            "Epoch 46: 100% 116/116 [00:05<00:00, 21.53it/s, loss=0.215, v_num=vijn]\n",
            "Epoch 47:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.218, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  86% 100/116 [00:05<00:00, 19.76it/s, loss=0.218, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 40.49it/s]\u001b[A\n",
            "Epoch 47: 100% 116/116 [00:05<00:00, 21.40it/s, loss=0.219, v_num=vijn]\n",
            "Epoch 48:  69% 80/116 [00:04<00:01, 18.24it/s, loss=0.214, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  86% 100/116 [00:05<00:00, 19.82it/s, loss=0.214, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.68it/s]\u001b[A\n",
            "Epoch 48: 100% 116/116 [00:05<00:00, 21.42it/s, loss=0.221, v_num=vijn]\n",
            "Epoch 49:  69% 80/116 [00:04<00:01, 18.36it/s, loss=0.216, v_num=vijn]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  86% 100/116 [00:05<00:00, 19.89it/s, loss=0.216, v_num=vijn]\n",
            "Validation DataLoader 0:  57% 20/35 [00:00<00:00, 41.72it/s]\u001b[A\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.52it/s, loss=0.222, v_num=vijn]\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.50it/s, loss=0.222, v_num=vijn]`Trainer.fit` stopped: `max_epochs=50` reached.\n",
            "Epoch 49: 100% 116/116 [00:05<00:00, 21.47it/s, loss=0.222, v_num=vijn]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW ▁▂▂▃▄▄▅▆▇▇██▅▅▅▅▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc ▁▁▂▃▃▅▆▇▇███████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss ██▇▇▆▅▅▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss █▇▅▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▇▅▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae █▇▆▆▄▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape ▂▁█▁▁▁▁▁▁▁▂▃▂▁▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▂▅▁▁▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse █▇▅▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae █▇▆▆▄▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse █▇▅▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse █▇▇▆▄▃▂▂▂▂▁▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▂▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape ▇██▇▅▃▃▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▂▁▁▂▁▂▂▂▁▁▂▂▂▁▂▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc ▁▁▂▂▄▅▆▇▇███████████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss ██▇▇▆▅▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss █▆▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss █▆▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae █▆▆▆▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape █▄▂▁▂▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse █▆▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae █▆▆▆▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse █▆▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse █▆▆▆▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape ▇███▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            lr-AdamW 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/class_loss 0.14922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/forecast_loss 0.33623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.33624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.42392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/mape 1.68201\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.33623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mae 0.42392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/norm_mse 0.33623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/rse 0.73026\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/smape 1.08291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 4049\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/acc 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/class_loss 0.15409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/forecast_loss 0.21506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.21508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.36722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/mape 2.53555\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.21506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mae 0.36722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/norm_mse 0.21506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/recon_loss -1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/rse 0.46058\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/smape 0.75301\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mclean-surf-822\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/luca_maurici/EEG%20connectivity%20estimate%20with%20transformers/runs/3ugsvijn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221017_054148-3ugsvijn/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python test_eeg_channels_loss_SNR.py spacetimeformer eeg_social_memory --d_model 10 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --class_loss_imp 0.0001 --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6\n",
        "!python test_eeg_channels_loss_SNR.py spacetimeformer eeg_social_memory --d_model 15 --d_ff 40 --enc_layers 2 --dec_layers 2 --base_lr 1e-3 --l2_coeff 1e-3 --loss mse --d_qk 4 --d_v 4 --n_heads 3 --run_name spatiotemporal_eeg_social_memory --class_loss_imp 0.0001 --dropout_emb 0.0 --dropout_ff 0.0 --initial_downsample_convs 0 --global_self_attn full --global_cross_attn full --local_self_attn full --local_cross_attn none --time_emb_dim 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgQMG9XMZLCl",
        "outputId": "239f7ddc-c349-4326-8fc4-96287b68d3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.run_name: generated_gc_SNR_6\n",
            "File list:  ['./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch0_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch0_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [-0.34327452 -0.6696092   0.045271  ]\n",
            "std_dev yc: [3.68126162 2.96569592 1.25874214]\n",
            "\n",
            "(20, 250, 3)\n",
            "(250, 60)\n",
            "[3.26340089 2.98000027 1.19964105 3.42196559 2.89621285 1.24797031\n",
            " 3.28383927 2.77387769 1.2073929  3.30356706 2.82163991 1.20793307\n",
            " 3.25481015 2.87983581 1.16455849 3.05185588 3.02651184 1.29143061\n",
            " 3.29083408 3.09068162 1.25768193 3.39303804 3.02101036 1.12140427\n",
            " 3.23990594 2.94998341 1.19456292 3.21904584 2.89059317 1.14440419\n",
            " 3.23639683 2.83045268 1.15886466 3.25178862 2.97632638 1.18174412\n",
            " 3.35919122 2.85171736 1.19688428 3.13503403 2.64694571 1.11332737\n",
            " 3.09262181 2.59256666 1.20629434 2.96764441 2.60201857 1.21344584\n",
            " 3.14109851 2.82584695 1.24668411 3.08363261 3.09621694 1.20452045\n",
            " 3.34721324 2.99076202 1.18044312 3.26245521 2.94772642 1.14020384]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-0.08071212 -0.05903198  0.05680284]\n",
            "  [-0.28631714 -0.04183381  0.28516463]\n",
            "  [-0.00341792  0.3640144   1.32194571]\n",
            "  ...\n",
            "  [ 0.08722073 -1.27183346  0.73384405]\n",
            "  [-0.8103204  -0.76727168 -0.554679  ]\n",
            "  [-0.13162789 -0.46327831 -0.48980058]]\n",
            "\n",
            " [[-0.33827084  0.47381772  2.14603356]\n",
            "  [-0.03013187 -0.21580847 -0.16175891]\n",
            "  [-0.11861299  0.17397797  0.06009276]\n",
            "  ...\n",
            "  [ 1.2718812   0.08095203  0.71804806]\n",
            "  [ 0.50608211 -0.2604465  -0.24869963]\n",
            "  [-0.64111236 -0.36922897  0.5886044 ]]\n",
            "\n",
            " [[-0.2913795  -0.31110268 -0.68648652]\n",
            "  [ 0.16097396 -0.26769729  0.44421155]\n",
            "  [-0.15200551 -0.26521476 -0.2453588 ]\n",
            "  ...\n",
            "  [-2.2116967  -0.82578084  0.4056984 ]\n",
            "  [-1.92752735 -0.92949623  0.38965895]\n",
            "  [-0.95073831 -1.31413804  3.00127796]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.17680922  0.33790542  0.45563412]\n",
            "  [ 0.3882447  -0.30020172  1.58539634]\n",
            "  [ 0.09842588 -0.44873988  0.00888062]\n",
            "  ...\n",
            "  [ 0.96230258  1.08569968 -1.60095404]\n",
            "  [ 2.05165878  0.27097232  0.10702531]\n",
            "  [-0.11610523 -0.37825798 -0.82677162]]\n",
            "\n",
            " [[-0.19397693 -0.09295697 -1.09467566]\n",
            "  [-0.25545212 -0.28515797 -0.70562017]\n",
            "  [ 0.08189133  0.13493022  0.81749011]\n",
            "  ...\n",
            "  [-0.61278038  0.81047231 -0.11383555]\n",
            "  [-0.49516224  0.89840158  1.02448288]\n",
            "  [ 0.20515244  0.76044144  0.97076517]]\n",
            "\n",
            " [[-0.23267015 -0.22755386  0.59770924]\n",
            "  [ 0.04271809  0.23556277 -0.55853658]\n",
            "  [ 0.20100922  0.21312967 -0.62127016]\n",
            "  ...\n",
            "  [-1.3054656  -1.17086461  2.28172536]\n",
            "  [-0.79388938 -1.3359853  -0.27568762]\n",
            "  [-0.1907247  -1.54531769  0.25476991]]]---------------------\n",
            "\n",
            "\n",
            "(9, 250, 3)\n",
            "(250, 27)\n",
            "[3.43463074 3.1543322  1.20076043 3.43196594 3.15509591 1.15861721\n",
            " 3.30042661 3.24006534 1.16359426 3.41930561 3.32111364 1.27295341\n",
            " 3.42112254 3.17028077 1.21289459 3.29155865 3.1641306  1.18318647\n",
            " 3.36182518 3.05584819 1.10427117 3.36281684 2.79976511 1.30709613\n",
            " 3.22350716 2.96168676 1.3037222 ]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[-1.25503490e-01 -8.07295074e-02 -3.84935543e-01]\n",
            "  [ 4.36991546e-01  1.52471155e-01  1.06801326e+00]\n",
            "  [-3.66528099e-01  1.48664808e-01 -3.73076088e-02]\n",
            "  ...\n",
            "  [-2.71529920e+00 -1.41529740e+00  5.02663693e-02]\n",
            "  [-1.63633919e+00 -1.15970144e+00  2.39311301e+00]\n",
            "  [-1.64766468e+00 -1.22410686e+00  3.28047155e+00]]\n",
            "\n",
            " [[-1.71581979e-02 -1.53021282e-01  1.98465049e-01]\n",
            "  [-9.27868716e-02 -1.60466936e-01 -5.55115147e-01]\n",
            "  [-1.76086207e-01 -2.69058487e-01 -1.30603169e-01]\n",
            "  ...\n",
            "  [ 6.12592692e-01 -2.92522342e-01  3.64362947e-01]\n",
            "  [ 1.17375977e-01 -7.21940726e-01  6.23738342e-01]\n",
            "  [-1.19832921e-01 -6.03579947e-01 -5.95822180e-01]]\n",
            "\n",
            " [[ 2.27546789e-02  1.19860853e-02  2.08227455e+00]\n",
            "  [ 2.15076117e-01 -3.85408832e-01  4.73203250e-01]\n",
            "  [ 1.30562029e-01  2.17857875e-01  9.67698672e-01]\n",
            "  ...\n",
            "  [-1.60950809e-01  9.23886625e-01 -1.11979540e+00]\n",
            "  [ 8.85203035e-01  9.79980330e-01  1.63923033e+00]\n",
            "  [ 6.72578163e-01  5.49383124e-01  4.12558570e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.19106487e-01 -6.24559514e-01 -1.04217201e+00]\n",
            "  [-6.76442802e-02 -1.72566290e-01  6.77377271e-01]\n",
            "  [ 2.09084133e-01 -4.35501262e-01  8.01814099e-01]\n",
            "  ...\n",
            "  [ 3.92611912e-01 -1.30357923e-01  8.00845883e-02]\n",
            "  [-3.00832195e-01 -6.77815451e-01  5.34368880e-03]\n",
            "  [-3.04370149e-01 -3.04405391e-01 -2.37398554e-01]]\n",
            "\n",
            " [[ 5.50931508e-02  1.40975856e-01  7.97268270e-01]\n",
            "  [ 3.65094168e-01 -3.62356346e-01 -3.82374519e-01]\n",
            "  [-9.11575829e-02 -6.24765142e-02 -1.52877844e-01]\n",
            "  ...\n",
            "  [-1.94738346e+00 -2.10990504e+00  2.46908536e-02]\n",
            "  [-2.36642915e+00 -2.80265333e+00  1.06499634e+00]\n",
            "  [-2.19957885e+00 -2.86958167e+00 -2.85301530e-01]]\n",
            "\n",
            " [[-4.12976066e-01  3.12571819e-02  5.77635169e-01]\n",
            "  [-3.56903956e-01  2.47566470e-02 -4.16246701e-01]\n",
            "  [-2.06639080e-06  3.85477024e-01 -2.23188308e-01]\n",
            "  ...\n",
            "  [-8.47830222e-02  8.53932776e-02 -2.87659122e-01]\n",
            "  [ 6.83833189e-01  3.43157799e-01 -8.52821085e-01]\n",
            "  [ 6.77467697e-03  3.64540173e-01  1.31280634e-01]]]---------------------\n",
            "\n",
            "\n",
            "(1, 250, 3)\n",
            "(250, 3)\n",
            "[3.68126162 2.96569592 1.25874214]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[-1.77097365e-01  7.39417765e-01 -3.49746247e-01]\n",
            "  [ 4.30083574e-01  1.73732750e-01 -1.35308201e+00]\n",
            "  [ 2.72362140e-01 -6.36618413e-02 -5.60562959e-01]\n",
            "  [ 2.78866742e-01  7.18088800e-03 -2.31660960e-01]\n",
            "  [ 1.34060331e-01 -4.98504184e-02 -1.33196586e+00]\n",
            "  [-2.38439703e-01  9.58573090e-01  7.71969040e-01]\n",
            "  [ 2.45339246e-01  1.13202339e+00 -4.14667700e-01]\n",
            "  [ 1.64740141e-02 -4.57968381e-01  1.46025400e-01]\n",
            "  [-7.17658924e-01 -1.14144464e+00 -6.88535572e-01]\n",
            "  [-3.16991877e-01 -8.09662405e-01 -6.39822615e-02]\n",
            "  [-5.27230021e-01 -8.62054270e-01  6.50219626e-01]\n",
            "  [-6.16054331e-01 -4.72436608e-01 -6.54490973e-01]\n",
            "  [-2.08700629e-01  2.08202676e-02 -9.37716426e-02]\n",
            "  [-2.46635572e-01  2.50478237e-01 -2.53898030e-01]\n",
            "  [-2.75107962e-01 -3.61855090e-01  1.71069103e+00]\n",
            "  [-1.45364948e+00 -1.02755781e+00  1.50401095e+00]\n",
            "  [-2.24413311e+00 -1.41554409e+00 -1.69195001e+00]\n",
            "  [-1.07795125e+00 -1.52609209e+00 -1.90531358e-02]\n",
            "  [-6.31490850e-01 -1.36985637e+00  2.26764594e+00]\n",
            "  [-1.68969114e+00 -1.24754605e+00  2.55665014e-01]\n",
            "  [-1.43694116e+00 -1.81912335e+00  1.36156519e+00]\n",
            "  [-1.67231442e+00 -1.97384433e+00  2.22291231e+00]\n",
            "  [-2.18705346e+00 -1.56047601e+00  8.90786120e-01]\n",
            "  [-9.53144333e-01 -1.55491104e+00  7.51244635e-01]\n",
            "  [-1.59294128e+00 -1.25465349e+00  1.74446809e+00]\n",
            "  [-1.77003434e+00 -9.58055662e-01 -4.78014899e-04]\n",
            "  [-8.09930740e-01 -4.56556598e-01  6.54701501e-01]\n",
            "  [-1.73243737e-01  4.90190940e-01  3.35760569e-01]\n",
            "  [-1.49813604e-01  7.78602791e-01  1.82024228e+00]\n",
            "  [-9.11136043e-01  5.48043528e-01 -1.25531493e+00]\n",
            "  [ 2.92516264e-01  1.86656275e-01  9.87519142e-01]\n",
            "  [-5.45805433e-01 -1.22320330e-01  3.98896267e-01]\n",
            "  [-7.77446175e-01 -2.29133388e-02  1.41551869e-01]\n",
            "  [-1.99532490e-01  6.42701323e-01  9.34299374e-01]\n",
            "  [ 9.73943949e-01  1.35176901e+00 -7.39891650e-01]\n",
            "  [ 1.87328055e+00  2.19364244e+00  7.18892616e-01]\n",
            "  [ 1.06292360e+00  2.34976532e+00 -1.78048960e+00]\n",
            "  [ 2.11589205e+00  1.26137508e+00 -8.31289072e-01]\n",
            "  [ 7.58166685e-01  9.97970889e-01 -6.17575988e-01]\n",
            "  [ 1.35538627e+00  1.76159968e+00 -3.01222621e+00]\n",
            "  [ 2.79743625e+00  1.68041085e+00 -3.90518148e-03]\n",
            "  [ 1.62957750e+00  1.36267096e+00  2.67605930e-01]\n",
            "  [ 1.46053995e+00  1.24131244e+00 -2.13158344e+00]\n",
            "  [ 1.81599337e+00  7.71019043e-01 -5.34345711e-01]\n",
            "  [ 4.25668680e-01  2.54504333e-01 -1.07966395e+00]\n",
            "  [-2.94624330e-02  1.86485878e-01 -1.53037455e+00]\n",
            "  [ 8.39968488e-01 -1.71963474e-01 -1.40965726e+00]\n",
            "  [ 6.94620135e-01  4.71523216e-01 -1.41291445e+00]\n",
            "  [ 7.60146352e-01  1.44231917e+00 -9.15169283e-01]\n",
            "  [ 9.20721632e-01  1.15456221e+00 -2.10496729e-01]\n",
            "  [ 2.57603616e-01  8.44728489e-01  8.56048890e-01]\n",
            "  [ 4.08338225e-01  1.54360537e+00 -1.97244939e+00]\n",
            "  [ 1.76095530e+00  1.63143811e+00 -1.50957192e+00]\n",
            "  [ 1.56202654e+00  1.04556008e+00  2.99879393e-01]\n",
            "  [ 1.14301290e+00  1.03326551e+00 -4.04670561e-01]\n",
            "  [ 1.16262443e+00  1.07596201e+00 -1.26181429e+00]\n",
            "  [ 1.00021276e+00  9.89460300e-02 -3.99910218e-01]\n",
            "  [-8.87035217e-02 -1.09308904e+00 -1.41741693e+00]\n",
            "  [ 3.34337273e-01 -1.45182963e+00  7.32727616e-01]\n",
            "  [-8.86665623e-01 -9.82873451e-01  8.93819580e-02]\n",
            "  [-5.24954974e-01 -5.83589629e-01  2.39050973e-01]\n",
            "  [-1.09290627e-01 -5.77351906e-01  1.57733880e+00]\n",
            "  [-6.99135357e-01 -9.26431935e-01  5.47708064e-01]\n",
            "  [-8.88409388e-01 -1.04253730e+00  9.53549970e-01]\n",
            "  [-6.69019601e-01 -2.89050847e-01  4.50092894e-01]\n",
            "  [-9.41388504e-02  2.39318957e-01  1.11535655e+00]\n",
            "  [ 2.37212438e-03  2.44750079e-01 -5.23359049e-01]\n",
            "  [ 3.12541504e-01  5.94071520e-01  2.72589776e-01]\n",
            "  [ 5.75312800e-01  5.93747487e-01 -4.08245389e-02]\n",
            "  [ 7.36265196e-01  1.50874556e-01 -1.05583229e+00]\n",
            "  [ 8.81664671e-01  5.09290991e-01 -1.60745214e+00]\n",
            "  [ 9.42593290e-01  1.32146322e+00 -2.90235037e-01]\n",
            "  [ 5.89351890e-01  1.43015534e+00  2.61547361e-01]\n",
            "  [ 3.58833122e-01  1.48683244e+00 -1.28556360e+00]\n",
            "  [ 9.54684782e-01  2.03151314e+00 -4.59396777e-01]\n",
            "  [ 1.81575143e+00  1.46318429e+00  6.75741653e-02]\n",
            "  [ 1.30706329e+00  7.26164760e-01 -1.15612840e-01]\n",
            "  [-2.17391904e-01  1.37115845e+00 -2.90641275e+00]\n",
            "  [ 1.03975723e+00  1.39486588e+00  1.03618330e-01]\n",
            "  [ 1.48392056e+00  1.29975192e+00 -3.65938381e-01]\n",
            "  [ 1.76047259e+00  2.21181061e+00 -1.25818868e+00]\n",
            "  [ 2.33691694e+00  2.36922871e+00 -4.75558954e-01]\n",
            "  [ 1.64274595e+00  1.77522920e+00 -6.46903035e-01]\n",
            "  [ 6.01867156e-01  1.59581449e+00 -3.30187810e-01]\n",
            "  [ 2.17401759e-01  1.25621349e+00 -7.64838812e-01]\n",
            "  [ 1.01281589e+00  8.78993799e-01 -7.66642065e-01]\n",
            "  [ 1.60568150e+00  7.85783818e-01  1.02857703e+00]\n",
            "  [ 1.29702089e+00  7.48623941e-01 -4.38404211e-01]\n",
            "  [ 1.03499482e+00  5.83427051e-01 -2.38778184e-01]\n",
            "  [ 9.36201953e-01  3.06043508e-02 -1.03152465e+00]\n",
            "  [ 1.16825667e+00 -5.97497524e-02  1.05419002e+00]\n",
            "  [ 6.50925996e-01 -2.20281960e-01 -6.14725283e-01]\n",
            "  [ 1.05283215e+00 -8.34654347e-01  4.32669293e-01]\n",
            "  [ 3.43569407e-01 -7.19306836e-01 -3.36806555e-01]\n",
            "  [ 3.88942785e-01 -4.32501651e-01 -1.22058389e+00]\n",
            "  [ 1.14658719e+00  6.58898495e-02 -4.34583915e-01]\n",
            "  [ 8.60742442e-01  3.71787910e-01 -9.26266803e-01]\n",
            "  [ 1.63065403e+00  5.85661386e-03 -1.04567098e+00]\n",
            "  [ 1.06264912e+00 -8.67146759e-01 -1.11228289e-01]\n",
            "  [-1.01870979e-01 -1.16024029e+00 -3.11091407e-01]\n",
            "  [-3.46180610e-01 -9.71191086e-01 -6.73016114e-01]\n",
            "  [-1.46210888e-01 -9.05589979e-01  3.82256014e-01]\n",
            "  [-2.26239843e-01 -1.21588586e+00  1.05443420e+00]\n",
            "  [-1.11343348e+00 -1.46312870e+00 -8.07313259e-01]\n",
            "  [-2.72857948e-01 -1.66309127e+00  2.81057883e-01]\n",
            "  [-5.40220604e-01 -1.27916222e+00  6.63523266e-01]\n",
            "  [-6.55836308e-02 -5.59763987e-01  1.72707514e+00]\n",
            "  [-1.59859104e-01 -9.83987208e-02 -2.85920130e-01]\n",
            "  [ 4.04660230e-01 -3.56238441e-01  4.51978983e-01]\n",
            "  [-8.56991516e-01 -3.15543608e-01  1.07375605e+00]\n",
            "  [-1.65790441e+00 -5.31251306e-02  8.79210992e-01]\n",
            "  [-1.34660117e+00 -5.79601502e-03  1.01897225e+00]\n",
            "  [-1.21185723e-01  2.25658730e-01  2.96870444e+00]\n",
            "  [-1.22799788e+00  1.03050296e-01  4.37895972e-01]\n",
            "  [-5.16346656e-01 -9.70997175e-01  8.31302119e-01]\n",
            "  [-1.04691444e+00 -1.14303893e+00  1.99025029e+00]\n",
            "  [-2.26465986e+00 -5.33740876e-01  5.96500307e-01]\n",
            "  [-1.05943344e+00 -7.75244417e-01  5.47618529e-01]\n",
            "  [-1.12222899e+00 -6.57298300e-01  2.07263766e+00]\n",
            "  [-1.04016785e+00 -4.85844935e-01  4.38121853e-01]\n",
            "  [-3.98707913e-01 -9.55455979e-01  8.45625228e-01]\n",
            "  [-1.04537638e+00 -1.68901378e+00  1.23619345e+00]\n",
            "  [-1.52771100e+00 -1.78965819e+00 -5.04897074e-01]\n",
            "  [-1.51117348e+00 -1.48431115e+00  6.64714730e-01]\n",
            "  [-2.23679933e+00 -9.94804831e-01  2.05164745e-02]\n",
            "  [-8.44614026e-01 -9.49546385e-01  1.32766849e+00]\n",
            "  [-1.63125666e+00 -6.57272517e-01  1.99888768e-01]\n",
            "  [-2.72648119e-01 -1.02945996e-01  1.19089479e+00]\n",
            "  [ 9.49346831e-02  8.82594581e-01  1.24629525e+00]\n",
            "  [-8.77864746e-02  1.26609041e+00 -6.70973258e-02]\n",
            "  [ 7.75342777e-01  5.01735539e-01 -1.30405691e+00]\n",
            "  [ 8.12561202e-01 -4.06769978e-01 -1.38287848e+00]\n",
            "  [ 5.86497590e-01 -1.03495514e+00  1.14928164e+00]\n",
            "  [-4.49297762e-01 -1.26729424e+00 -6.33523064e-02]\n",
            "  [-7.42019331e-01 -7.83172835e-01 -9.74850076e-01]\n",
            "  [ 8.17654706e-01 -8.15422042e-01 -1.17111067e+00]\n",
            "  [ 6.32064016e-01 -1.60972481e+00  8.05194221e-01]\n",
            "  [-1.03565253e+00 -1.94393898e+00 -4.85466000e-01]\n",
            "  [-5.09542806e-01 -1.75177104e+00 -1.17674786e+00]\n",
            "  [ 2.88285426e-01 -1.31927451e+00  1.35225324e+00]\n",
            "  [-6.06275356e-01 -7.25401175e-01 -4.42807554e-01]\n",
            "  [ 4.37852742e-01 -4.25232309e-01 -1.14738342e+00]\n",
            "  [ 8.79277479e-01 -5.21166719e-01 -3.49805822e-01]\n",
            "  [-8.47097605e-02 -1.87738757e-01 -7.08312519e-01]\n",
            "  [ 7.12892433e-01 -2.75543142e-01 -2.43875540e+00]\n",
            "  [ 1.11173466e+00 -5.79436817e-01 -9.15877808e-01]\n",
            "  [ 8.26556390e-01  7.09228652e-02 -2.14012171e+00]\n",
            "  [ 1.33726985e+00  1.04479230e+00 -7.93884394e-01]\n",
            "  [ 1.68699051e+00  1.56730169e+00 -6.86901898e-01]\n",
            "  [ 2.27005476e+00  1.49273665e+00  1.07096125e-01]\n",
            "  [ 1.10759179e+00  1.54073385e+00 -1.94726862e+00]\n",
            "  [ 2.06550401e+00  1.02886231e+00 -4.43611444e-01]\n",
            "  [ 2.02245918e+00  6.32560869e-01 -2.15054176e-01]\n",
            "  [ 1.20138185e+00  1.01168895e+00 -1.30167141e+00]\n",
            "  [ 7.50285884e-01  1.02931660e+00 -1.90543219e+00]\n",
            "  [ 1.12247471e+00  2.12635003e-01 -7.27906521e-01]\n",
            "  [ 1.74742940e-02 -8.74643937e-02  2.93239993e-01]\n",
            "  [-3.36258943e-01  2.53065871e-02 -9.12245060e-01]\n",
            "  [ 6.58265002e-01 -8.42044946e-02 -1.51929414e-02]\n",
            "  [-1.71226987e-01  5.20986458e-01 -1.00964830e+00]\n",
            "  [-5.18459470e-01  1.42758424e+00 -2.85533250e-01]\n",
            "  [-1.40998250e-01  9.81472208e-01  9.03231475e-01]\n",
            "  [-1.29602039e+00  2.38256807e-01 -9.81994060e-02]\n",
            "  [-1.08329909e+00  4.87439478e-01 -8.32136823e-01]\n",
            "  [-9.38399757e-01  1.23184871e+00  1.92319216e+00]\n",
            "  [-6.93294260e-01  1.64376399e+00  9.64040669e-01]\n",
            "  [-2.59695955e-01  1.88102827e+00  9.34355732e-01]\n",
            "  [-9.56289568e-01  1.02559429e+00  1.13697037e+00]\n",
            "  [-2.09663612e-01  1.40369135e-02  5.00851250e-01]\n",
            "  [-5.29397196e-01  3.53724329e-02  5.07585240e-01]\n",
            "  [-3.40594210e-02  7.49171096e-01  4.83722733e-01]\n",
            "  [ 1.17144412e-01  1.58671989e+00  9.43699793e-01]\n",
            "  [ 6.29985640e-02  1.58261867e+00  7.26409795e-01]\n",
            "  [ 5.11163728e-01  5.10584650e-01  2.41759683e-02]\n",
            "  [-9.18669436e-01  3.03385758e-01 -4.84505295e-01]\n",
            "  [-1.25790887e-01  1.28154881e+00 -9.00574564e-01]\n",
            "  [ 1.01829888e+00  1.52996404e+00  8.97923676e-01]\n",
            "  [ 1.31055916e+00  8.45206985e-01  1.32423106e+00]\n",
            "  [ 1.15856346e+00  6.80437246e-01  1.35444861e-01]\n",
            "  [ 4.78682555e-01  8.42458379e-01  8.22474158e-01]\n",
            "  [ 6.24450424e-01  7.61649284e-01 -6.54735406e-01]\n",
            "  [ 1.05447419e+00  5.58511292e-01 -7.38900025e-01]\n",
            "  [ 8.58492976e-01 -3.36460204e-01  1.00871205e+00]\n",
            "  [-5.27698560e-01 -7.17986803e-01  1.08775047e+00]\n",
            "  [ 7.33948618e-02  8.28052984e-02 -1.28924672e+00]\n",
            "  [ 8.98079327e-01  2.90706190e-01 -4.98417518e-01]\n",
            "  [-1.52745097e-01  6.55651055e-02  1.95119787e-02]\n",
            "  [-4.38745325e-01 -1.49958390e-01 -1.36680227e-01]\n",
            "  [-7.00080080e-01 -7.13565687e-01 -8.82892694e-01]\n",
            "  [-1.01329732e+00 -1.04610344e+00  1.35939702e-02]\n",
            "  [-6.76220854e-01 -1.10657216e+00  9.88741716e-01]\n",
            "  [-1.44507293e-01 -7.21287533e-01 -1.43600862e-01]\n",
            "  [-4.94980691e-03 -5.99797089e-01  8.34508577e-01]\n",
            "  [-8.89670333e-02 -4.78795250e-01 -1.25301518e-01]\n",
            "  [-3.84764128e-02 -1.71183912e-01 -8.03344959e-01]\n",
            "  [ 6.76325073e-01  6.18819318e-02  4.16457657e-01]\n",
            "  [ 4.21334528e-01 -1.38145515e-02  5.79239446e-01]\n",
            "  [-1.10666894e-01  2.48101076e-01 -6.56428946e-01]\n",
            "  [ 2.89978825e-01  3.34482657e-01 -3.62463909e-01]\n",
            "  [-1.26315805e-01 -2.20101941e-01  4.55241018e-02]\n",
            "  [-8.59310146e-01 -7.10273988e-01 -6.05984564e-01]\n",
            "  [-5.01395258e-01 -4.96520543e-01 -1.85222253e+00]\n",
            "  [-2.88210743e-01 -1.71526012e-01  1.80874246e+00]\n",
            "  [-5.72424826e-01 -8.41449205e-02  5.34915785e-01]\n",
            "  [-3.12638582e-01 -1.08877416e-01  7.57623315e-01]\n",
            "  [-1.22511007e+00 -3.26786951e-01 -4.10976459e-01]\n",
            "  [-2.01981680e-01 -7.24230650e-01 -3.12236656e-01]\n",
            "  [-1.07996832e+00 -3.17681675e-01  2.29704295e-01]\n",
            "  [-1.00503740e+00 -2.15203766e-01  6.52543221e-01]\n",
            "  [-5.80620044e-01 -4.89536438e-01  1.50654768e-01]\n",
            "  [-8.75000030e-01 -7.52017011e-01  2.16038233e-01]\n",
            "  [-1.23291836e+00 -8.54276325e-01  8.16678799e-01]\n",
            "  [-1.83586735e+00 -7.62484586e-01  7.23090460e-01]\n",
            "  [-1.06448918e+00 -2.93480044e-01  5.10051740e-01]\n",
            "  [-3.32969755e-01  1.43363742e-01  5.59879031e-01]\n",
            "  [ 1.56632002e-01  7.59944587e-01  2.34712677e-01]\n",
            "  [ 5.17608261e-01  4.14317316e-01 -3.37694871e-01]\n",
            "  [ 1.06097634e+00 -5.80516469e-01 -5.49649141e-01]\n",
            "  [-1.04518439e+00 -4.40974649e-01 -9.40105165e-01]\n",
            "  [-7.77201863e-01 -4.38309647e-01  2.28717702e-01]\n",
            "  [-1.16015027e+00 -6.24656737e-01 -2.96425116e-01]\n",
            "  [-1.03103325e+00 -1.97072106e-01  3.60777812e-01]\n",
            "  [-1.23662307e+00 -1.56069611e-01  1.28710151e+00]\n",
            "  [-1.91854964e+00 -1.59619095e-01  8.05151607e-01]\n",
            "  [-1.45264314e+00  6.76457869e-01  7.38349311e-01]\n",
            "  [-5.87243258e-01  1.22675897e+00 -1.68605278e-01]\n",
            "  [ 2.44334862e-01  6.90525747e-01  1.09534227e+00]\n",
            "  [ 1.93755776e-01 -2.16227114e-01  4.47310306e-01]\n",
            "  [-1.26813618e-01 -4.38598312e-01  3.08959707e-01]\n",
            "  [-3.33890407e-01 -4.15461965e-01 -7.46354159e-01]\n",
            "  [-1.14121041e+00  1.41272575e-01 -4.94212111e-01]\n",
            "  [-5.90158068e-02  1.24247205e+00  4.11007525e-01]\n",
            "  [ 6.31316061e-01  1.41122487e+00  3.09435122e-01]\n",
            "  [ 8.87826953e-01  3.18839377e-01 -2.21264584e-03]\n",
            "  [ 2.49355955e-01 -1.28824538e-01  1.91934967e-01]\n",
            "  [ 1.16846051e+00  8.03616292e-02 -1.29219402e+00]\n",
            "  [ 1.30006947e+00 -1.55345362e-01  1.06931954e+00]\n",
            "  [ 2.75472213e-01 -2.45072972e-01 -1.40875228e+00]\n",
            "  [ 8.82478839e-01 -4.92497763e-01  4.00078412e-01]\n",
            "  [-5.22423512e-01 -1.11471997e+00  5.54098591e-01]\n",
            "  [-2.42503669e-01 -6.89668104e-01 -1.55745205e+00]\n",
            "  [ 4.48996034e-03 -5.84451786e-01 -1.15584577e+00]\n",
            "  [-4.22982185e-02 -1.86674245e+00  8.52016227e-01]\n",
            "  [-1.59069104e+00 -3.07426065e+00  6.28896606e-01]\n",
            "  [-2.34985297e+00 -3.05184616e+00 -1.11999104e-03]\n",
            "  [-2.35525941e+00 -2.09489505e+00  2.28313610e+00]\n",
            "  [-1.87487267e+00 -1.14357260e+00  2.30696832e+00]\n",
            "  [-1.08172585e+00 -4.86533191e-01 -3.47453605e-01]\n",
            "  [ 3.11514604e-01 -5.20683778e-01  7.48210464e-01]\n",
            "  [ 5.62354654e-01 -1.02910207e+00  2.19570542e+00]]]---------------------\n",
            "\n",
            "\n",
            "(20, 250, 3)\n",
            "(9, 250, 3)\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [-8.65973959e-17  1.17239551e-16  6.92779167e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "context_points 6\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 9\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch0_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 10\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [40, 58, 189, 242, 17, 108, 32, 114, 234, 235, 84, 136, 97, 179, 129, 220, 187, 113, 44, 205, 100, 67, 105, 163, 138, 128, 26, 202, 157, 22, 76, 208, 68, 56, 153, 82, 190, 112, 73, 131, 110, 36, 195, 98, 104, 227, 123, 184, 147, 188, 206, 146, 6, 229, 177, 59, 51, 241, 170, 134, 16, 64, 31, 223, 91, 161, 211, 168, 109, 130, 24, 55, 99, 191, 90, 47, 139, 164, 27, 228, 62, 57, 37, 171, 120, 199, 209, 1, 49, 7, 28, 194, 144, 8, 11, 81, 203, 72, 169, 222, 19, 87, 148, 125, 124, 175, 212, 63, 89, 14, 118, 15, 243, 127, 217, 107, 43, 60, 182, 116, 196, 132, 13, 158, 141, 42, 215, 103, 198, 18, 159, 78, 231, 61, 122, 178, 75, 219, 192, 92, 180, 152, 41, 151, 181, 237, 149, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 117, 236, 201, 216, 145, 93, 221, 142, 66, 52, 174, 10, 4, 197, 39, 186, 88, 165, 200, 133, 167, 34, 65, 29, 115, 70, 5, 20, 160, 101, 121, 204, 46, 176, 71, 77, 50, 166, 155, 30, 225, 25, 156, 135, 172, 213, 240, 183, 218, 54, 239, 80, 238, 173, 102, 3, 33, 185, 233, 21, 162, 140, 48, 106, 230, 224, 210, 207, 12, 119, 214, 79, 83, 126, 9, 226, 0, 193, 154, 143, 232]\n",
            "self.series.num_trials(split): 9\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch0_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch0_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "Testing DataLoader 0: 100% 18/18 [00:02<00:00,  8.10it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9998734593391418\n",
            "     test/class_loss        0.04253941401839256\n",
            "   test/forecast_loss       0.34527355432510376\n",
            "        test/loss           0.3452777564525604\n",
            "        test/mae            0.4368096590042114\n",
            "        test/mape            2.956775665283203\n",
            "        test/mse            0.34527361392974854\n",
            "      test/norm_mae         0.4368096590042114\n",
            "      test/norm_mse         0.34527361392974854\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.5860063433647156\n",
            "       test/smape           0.8624972105026245\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: generated_gc_SNR_6\n",
            "File list:  ['./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch1_sub_SNR_inf_1/epoch=59-val/norm_mse=0.31.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch1_sub_SNR_inf_1/epoch=59-val/norm_mse=0.31.ckpt\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.27998699 -0.6696092   0.045271  ]\n",
            "std_dev yc: [3.31902638 2.96569592 1.25874214]\n",
            "\n",
            "(20, 250, 3)\n",
            "(250, 60)\n",
            "[3.05155337 2.98000027 1.19964105 3.0214477  2.89621285 1.24797031\n",
            " 2.99083682 2.77387769 1.2073929  2.87150134 2.82163991 1.20793307\n",
            " 2.80944211 2.87983581 1.16455849 2.79110019 3.02651184 1.29143061\n",
            " 2.93848899 3.09068162 1.25768193 3.07804889 3.02101036 1.12140427\n",
            " 3.0943009  2.94998341 1.19456292 3.07192807 2.89059317 1.14440419\n",
            " 3.02060323 2.83045268 1.15886466 2.93851658 2.97632638 1.18174412\n",
            " 2.87424645 2.85171736 1.19688428 3.02498457 2.64694571 1.11332737\n",
            " 2.97660322 2.59256666 1.20629434 2.96610999 2.60201857 1.21344584\n",
            " 3.00647783 2.82584695 1.24668411 2.97822199 3.09621694 1.20452045\n",
            " 3.06566746 2.99076202 1.18044312 3.18324482 2.94772642 1.14020384]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-1.47064781e-01 -5.90319844e-02  5.68028385e-02]\n",
            "  [ 1.27861967e-01 -4.18338073e-02  2.85164633e-01]\n",
            "  [ 3.64936877e-01  3.64014397e-01  1.32194571e+00]\n",
            "  ...\n",
            "  [ 1.10051218e+00 -1.27183346e+00  7.33844055e-01]\n",
            "  [ 1.00824230e+00 -7.67271681e-01 -5.54679002e-01]\n",
            "  [ 1.00983631e+00 -4.63278309e-01 -4.89800580e-01]]\n",
            "\n",
            " [[-2.32454054e-03  4.73817719e-01  2.14603356e+00]\n",
            "  [-1.88596993e-01 -2.15808473e-01 -1.61758912e-01]\n",
            "  [-7.99210189e-01  1.73977973e-01  6.00927553e-02]\n",
            "  ...\n",
            "  [ 4.62747248e-01  8.09520283e-02  7.18048056e-01]\n",
            "  [-3.78541252e-01 -2.60446502e-01 -2.48699626e-01]\n",
            "  [-6.44791561e-01 -3.69228970e-01  5.88604403e-01]]\n",
            "\n",
            " [[-3.86002373e-01 -3.11102679e-01 -6.86486522e-01]\n",
            "  [-2.99383267e-01 -2.67697291e-01  4.44211553e-01]\n",
            "  [ 7.12281567e-02 -2.65214756e-01 -2.45358797e-01]\n",
            "  ...\n",
            "  [-1.87261195e+00 -8.25780843e-01  4.05698402e-01]\n",
            "  [-1.02053355e+00 -9.29496225e-01  3.89658951e-01]\n",
            "  [-4.93597248e-01 -1.31413804e+00  3.00127796e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-7.53270544e-02  3.37905415e-01  4.55634119e-01]\n",
            "  [-4.29709767e-02 -3.00201720e-01  1.58539634e+00]\n",
            "  [ 6.91824908e-01 -4.48739880e-01  8.88061901e-03]\n",
            "  ...\n",
            "  [ 6.05513237e-01  1.08569968e+00 -1.60095404e+00]\n",
            "  [ 1.65928924e-01  2.70972325e-01  1.07025307e-01]\n",
            "  [-1.31618693e-01 -3.78257984e-01 -8.26771617e-01]]\n",
            "\n",
            " [[ 3.12299388e-01 -9.29569747e-02 -1.09467566e+00]\n",
            "  [-6.09581019e-02 -2.85157975e-01 -7.05620165e-01]\n",
            "  [ 2.17973766e-01  1.34930221e-01  8.17490106e-01]\n",
            "  ...\n",
            "  [-8.79779793e-01  8.10472307e-01 -1.13835551e-01]\n",
            "  [-3.78302541e-01  8.98401584e-01  1.02448288e+00]\n",
            "  [ 6.21921531e-01  7.60441438e-01  9.70765169e-01]]\n",
            "\n",
            " [[ 4.15001797e-01 -2.27553858e-01  5.97709241e-01]\n",
            "  [ 5.88132504e-01  2.35562766e-01 -5.58536578e-01]\n",
            "  [-3.73112428e-01  2.13129665e-01 -6.21270159e-01]\n",
            "  ...\n",
            "  [ 1.51210856e+00 -1.17086461e+00  2.28172536e+00]\n",
            "  [ 1.03261009e+00 -1.33598530e+00 -2.75687619e-01]\n",
            "  [ 8.12416900e-01 -1.54531769e+00  2.54769912e-01]]]---------------------\n",
            "\n",
            "\n",
            "(9, 250, 3)\n",
            "(250, 27)\n",
            "[3.24312957 3.1543322  1.20076043 3.2437275  3.15509591 1.15861721\n",
            " 3.20387068 3.24006534 1.16359426 3.23875997 3.32111364 1.27295341\n",
            " 3.19143936 3.17028077 1.21289459 3.26098638 3.1641306  1.18318647\n",
            " 3.30045875 3.05584819 1.10427117 3.13039169 2.79976511 1.30709613\n",
            " 3.11079628 2.96168676 1.3037222 ]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 0.11535181 -0.08072951 -0.38493554]\n",
            "  [ 0.09162852  0.15247116  1.06801326]\n",
            "  [ 0.19748553  0.14866481 -0.03730761]\n",
            "  ...\n",
            "  [-1.17852523 -1.4152974   0.05026637]\n",
            "  [-1.09479846 -1.15970144  2.39311301]\n",
            "  [-1.39550954 -1.22410686  3.28047155]]\n",
            "\n",
            " [[-0.58583813 -0.15302128  0.19846505]\n",
            "  [-0.12859694 -0.16046694 -0.55511515]\n",
            "  [-0.42709893 -0.26905849 -0.13060317]\n",
            "  ...\n",
            "  [ 0.12548281 -0.29252234  0.36436295]\n",
            "  [ 0.32131137 -0.72194073  0.62373834]\n",
            "  [ 0.03086025 -0.60357995 -0.59582218]]\n",
            "\n",
            " [[ 0.01361202  0.01198609  2.08227455]\n",
            "  [ 0.00793534 -0.38540883  0.47320325]\n",
            "  [-0.15822062  0.21785787  0.96769867]\n",
            "  ...\n",
            "  [-0.21989417  0.92388663 -1.1197954 ]\n",
            "  [ 0.38742881  0.97998033  1.63923033]\n",
            "  [ 0.28159107  0.54938312  0.41255857]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.23584371 -0.62455951 -1.04217201]\n",
            "  [ 0.13456006 -0.17256629  0.67737727]\n",
            "  [ 0.57698677 -0.43550126  0.8018141 ]\n",
            "  ...\n",
            "  [-0.25422863 -0.13035792  0.08008459]\n",
            "  [-0.03983778 -0.67781545  0.00534369]\n",
            "  [-0.0415511  -0.30440539 -0.23739855]]\n",
            "\n",
            " [[ 0.06809714  0.14097586  0.79726827]\n",
            "  [ 0.51091019 -0.36235635 -0.38237452]\n",
            "  [-0.10342567 -0.06247651 -0.15287784]\n",
            "  ...\n",
            "  [-1.06542558 -2.10990504  0.02469085]\n",
            "  [-1.09986152 -2.80265333  1.06499634]\n",
            "  [-1.23716466 -2.86958167 -0.28530153]]\n",
            "\n",
            " [[-0.30978693  0.03125718  0.57763517]\n",
            "  [ 0.01522411  0.02475665 -0.4162467 ]\n",
            "  [-0.38048354  0.38547702 -0.22318831]\n",
            "  ...\n",
            "  [ 0.17628383  0.08539328 -0.28765912]\n",
            "  [ 0.07375118  0.3431578  -0.85282108]\n",
            "  [-0.11029679  0.36454017  0.13128063]]]---------------------\n",
            "\n",
            "\n",
            "(1, 250, 3)\n",
            "(250, 3)\n",
            "[3.31902638 2.96569592 1.25874214]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 2.28731421e-01  7.39417765e-01 -3.49746247e-01]\n",
            "  [-6.33344887e-01  1.73732750e-01 -1.35308201e+00]\n",
            "  [ 1.63359423e-01 -6.36618413e-02 -5.60562959e-01]\n",
            "  [-8.18037410e-02  7.18088800e-03 -2.31660960e-01]\n",
            "  [-3.24673407e-01 -4.98504184e-02 -1.33196586e+00]\n",
            "  [-5.25490062e-01  9.58573090e-01  7.71969040e-01]\n",
            "  [-1.29577894e+00  1.13202339e+00 -4.14667700e-01]\n",
            "  [-9.97644099e-01 -4.57968381e-01  1.46025400e-01]\n",
            "  [-8.06342129e-02 -1.14144464e+00 -6.88535572e-01]\n",
            "  [ 4.91099940e-02 -8.09662405e-01 -6.39822615e-02]\n",
            "  [-2.74119603e-01 -8.62054270e-01  6.50219626e-01]\n",
            "  [-1.68436395e-01 -4.72436608e-01 -6.54490973e-01]\n",
            "  [-4.39099593e-01  2.08202676e-02 -9.37716426e-02]\n",
            "  [-1.06908941e+00  2.50478237e-01 -2.53898030e-01]\n",
            "  [-1.43908711e+00 -3.61855090e-01  1.71069103e+00]\n",
            "  [-1.34778886e+00 -1.02755781e+00  1.50401095e+00]\n",
            "  [-1.02632990e+00 -1.41554409e+00 -1.69195001e+00]\n",
            "  [-7.91899346e-01 -1.52609209e+00 -1.90531358e-02]\n",
            "  [-8.07040636e-01 -1.36985637e+00  2.26764594e+00]\n",
            "  [-5.67171002e-01 -1.24754605e+00  2.55665014e-01]\n",
            "  [-5.24948252e-01 -1.81912335e+00  1.36156519e+00]\n",
            "  [-8.16816224e-01 -1.97384433e+00  2.22291231e+00]\n",
            "  [-9.57502575e-01 -1.56047601e+00  8.90786120e-01]\n",
            "  [-6.11267400e-01 -1.55491104e+00  7.51244635e-01]\n",
            "  [-2.75949361e-02 -1.25465349e+00  1.74446809e+00]\n",
            "  [ 1.16060446e-01 -9.58055662e-01 -4.78014899e-04]\n",
            "  [-2.21168533e-02 -4.56556598e-01  6.54701501e-01]\n",
            "  [-5.56015910e-01  4.90190940e-01  3.35760569e-01]\n",
            "  [-1.52014703e+00  7.78602791e-01  1.82024228e+00]\n",
            "  [-1.42030303e+00  5.48043528e-01 -1.25531493e+00]\n",
            "  [-1.05418313e+00  1.86656275e-01  9.87519142e-01]\n",
            "  [-7.77345723e-01 -1.22320330e-01  3.98896267e-01]\n",
            "  [ 2.67327868e-01 -2.29133388e-02  1.41551869e-01]\n",
            "  [ 1.33854724e+00  6.42701323e-01  9.34299374e-01]\n",
            "  [ 9.66286967e-01  1.35176901e+00 -7.39891650e-01]\n",
            "  [ 3.57458817e-01  2.19364244e+00  7.18892616e-01]\n",
            "  [ 2.71142617e-01  2.34976532e+00 -1.78048960e+00]\n",
            "  [ 6.06667067e-01  1.26137508e+00 -8.31289072e-01]\n",
            "  [ 1.59370397e+00  9.97970889e-01 -6.17575988e-01]\n",
            "  [ 2.42028305e+00  1.76159968e+00 -3.01222621e+00]\n",
            "  [ 1.91403596e+00  1.68041085e+00 -3.90518148e-03]\n",
            "  [ 9.98160830e-01  1.36267096e+00  2.67605930e-01]\n",
            "  [ 2.46530869e-01  1.24131244e+00 -2.13158344e+00]\n",
            "  [-1.97725403e-01  7.71019043e-01 -5.34345711e-01]\n",
            "  [-1.38550229e-02  2.54504333e-01 -1.07966395e+00]\n",
            "  [ 7.69375338e-01  1.86485878e-01 -1.53037455e+00]\n",
            "  [ 8.84020839e-01 -1.71963474e-01 -1.40965726e+00]\n",
            "  [ 1.33124090e-01  4.71523216e-01 -1.41291445e+00]\n",
            "  [-6.33505193e-01  1.44231917e+00 -9.15169283e-01]\n",
            "  [-4.32623031e-01  1.15456221e+00 -2.10496729e-01]\n",
            "  [ 1.04622661e-01  8.44728489e-01  8.56048890e-01]\n",
            "  [ 5.33806640e-01  1.54360537e+00 -1.97244939e+00]\n",
            "  [ 6.07947535e-01  1.63143811e+00 -1.50957192e+00]\n",
            "  [ 5.64813494e-01  1.04556008e+00  2.99879393e-01]\n",
            "  [ 3.83046015e-01  1.03326551e+00 -4.04670561e-01]\n",
            "  [ 4.59728178e-02  1.07596201e+00 -1.26181429e+00]\n",
            "  [ 9.22450583e-02  9.89460300e-02 -3.99910218e-01]\n",
            "  [ 7.56811585e-01 -1.09308904e+00 -1.41741693e+00]\n",
            "  [ 9.54345417e-01 -1.45182963e+00  7.32727616e-01]\n",
            "  [-6.27338312e-02 -9.82873451e-01  8.93819580e-02]\n",
            "  [-3.85443416e-01 -5.83589629e-01  2.39050973e-01]\n",
            "  [ 1.11746092e-01 -5.77351906e-01  1.57733880e+00]\n",
            "  [-5.10670899e-02 -9.26431935e-01  5.47708064e-01]\n",
            "  [-3.95578675e-01 -1.04253730e+00  9.53549970e-01]\n",
            "  [-4.84154025e-01 -2.89050847e-01  4.50092894e-01]\n",
            "  [-1.20519474e-01  2.39318957e-01  1.11535655e+00]\n",
            "  [ 6.81498868e-02  2.44750079e-01 -5.23359049e-01]\n",
            "  [ 3.99459177e-01  5.94071520e-01  2.72589776e-01]\n",
            "  [ 6.88657444e-01  5.93747487e-01 -4.08245389e-02]\n",
            "  [ 4.86273042e-01  1.50874556e-01 -1.05583229e+00]\n",
            "  [ 1.34903754e-01  5.09290991e-01 -1.60745214e+00]\n",
            "  [-3.47936893e-01  1.32146322e+00 -2.90235037e-01]\n",
            "  [-6.30543179e-01  1.43015534e+00  2.61547361e-01]\n",
            "  [-9.49144483e-02  1.48683244e+00 -1.28556360e+00]\n",
            "  [ 3.34518589e-01  2.03151314e+00 -4.59396777e-01]\n",
            "  [ 2.34412549e-01  1.46318429e+00  6.75741653e-02]\n",
            "  [-4.04628435e-01  7.26164760e-01 -1.15612840e-01]\n",
            "  [-5.26328869e-01  1.37115845e+00 -2.90641275e+00]\n",
            "  [ 7.18821772e-01  1.39486588e+00  1.03618330e-01]\n",
            "  [ 1.59463990e+00  1.29975192e+00 -3.65938381e-01]\n",
            "  [ 6.91203750e-01  2.21181061e+00 -1.25818868e+00]\n",
            "  [-5.21474950e-03  2.36922871e+00 -4.75558954e-01]\n",
            "  [-4.16846397e-01  1.77522920e+00 -6.46903035e-01]\n",
            "  [-7.65144693e-01  1.59581449e+00 -3.30187810e-01]\n",
            "  [-5.52919486e-03  1.25621349e+00 -7.64838812e-01]\n",
            "  [ 1.13886337e+00  8.78993799e-01 -7.66642065e-01]\n",
            "  [ 1.57110452e+00  7.85783818e-01  1.02857703e+00]\n",
            "  [ 1.32854169e+00  7.48623941e-01 -4.38404211e-01]\n",
            "  [ 8.86963404e-01  5.83427051e-01 -2.38778184e-01]\n",
            "  [ 1.27605072e+00  3.06043508e-02 -1.03152465e+00]\n",
            "  [ 1.99901938e+00 -5.97497524e-02  1.05419002e+00]\n",
            "  [ 1.90644317e+00 -2.20281960e-01 -6.14725283e-01]\n",
            "  [ 1.76707231e+00 -8.34654347e-01  4.32669293e-01]\n",
            "  [ 1.95354407e+00 -7.19306836e-01 -3.36806555e-01]\n",
            "  [ 1.93440512e+00 -4.32501651e-01 -1.22058389e+00]\n",
            "  [ 1.88557611e+00  6.58898495e-02 -4.34583915e-01]\n",
            "  [ 1.96144280e+00  3.71787910e-01 -9.26266803e-01]\n",
            "  [ 1.40720564e+00  5.85661386e-03 -1.04567098e+00]\n",
            "  [ 9.10234694e-01 -8.67146759e-01 -1.11228289e-01]\n",
            "  [ 6.72635758e-01 -1.16024029e+00 -3.11091407e-01]\n",
            "  [ 4.86285375e-01 -9.71191086e-01 -6.73016114e-01]\n",
            "  [ 2.28618636e-02 -9.05589979e-01  3.82256014e-01]\n",
            "  [ 1.45891827e-01 -1.21588586e+00  1.05443420e+00]\n",
            "  [ 4.57947993e-01 -1.46312870e+00 -8.07313259e-01]\n",
            "  [ 4.23762732e-01 -1.66309127e+00  2.81057883e-01]\n",
            "  [ 6.99626127e-01 -1.27916222e+00  6.63523266e-01]\n",
            "  [ 7.79693718e-01 -5.59763987e-01  1.72707514e+00]\n",
            "  [ 4.65431265e-01 -9.83987208e-02 -2.85920130e-01]\n",
            "  [-1.17238718e-01 -3.56238441e-01  4.51978983e-01]\n",
            "  [-1.31759058e+00 -3.15543608e-01  1.07375605e+00]\n",
            "  [-1.40531022e+00 -5.31251306e-02  8.79210992e-01]\n",
            "  [-8.04401419e-01 -5.79601502e-03  1.01897225e+00]\n",
            "  [-9.58106988e-01  2.25658730e-01  2.96870444e+00]\n",
            "  [-1.10194180e+00  1.03050296e-01  4.37895972e-01]\n",
            "  [-1.12027384e+00 -9.70997175e-01  8.31302119e-01]\n",
            "  [-1.31036201e+00 -1.14303893e+00  1.99025029e+00]\n",
            "  [-1.00125963e+00 -5.33740876e-01  5.96500307e-01]\n",
            "  [-1.43189716e-01 -7.75244417e-01  5.47618529e-01]\n",
            "  [ 4.18657751e-01 -6.57298300e-01  2.07263766e+00]\n",
            "  [ 1.84424874e-01 -4.85844935e-01  4.38121853e-01]\n",
            "  [-4.46594192e-01 -9.55455979e-01  8.45625228e-01]\n",
            "  [-5.24868358e-01 -1.68901378e+00  1.23619345e+00]\n",
            "  [-1.10161883e+00 -1.78965819e+00 -5.04897074e-01]\n",
            "  [-1.88162223e+00 -1.48431115e+00  6.64714730e-01]\n",
            "  [-1.44661998e+00 -9.94804831e-01  2.05164745e-02]\n",
            "  [-2.98307610e-01 -9.49546385e-01  1.32766849e+00]\n",
            "  [ 2.03901684e-02 -6.57272517e-01  1.99888768e-01]\n",
            "  [ 4.57542740e-01 -1.02945996e-01  1.19089479e+00]\n",
            "  [ 6.64413261e-01  8.82594581e-01  1.24629525e+00]\n",
            "  [ 6.44984059e-01  1.26609041e+00 -6.70973258e-02]\n",
            "  [ 7.68418512e-01  5.01735539e-01 -1.30405691e+00]\n",
            "  [ 7.30317603e-01 -4.06769978e-01 -1.38287848e+00]\n",
            "  [ 6.80128022e-01 -1.03495514e+00  1.14928164e+00]\n",
            "  [ 9.21266799e-01 -1.26729424e+00 -6.33523064e-02]\n",
            "  [ 1.36728641e+00 -7.83172835e-01 -9.74850076e-01]\n",
            "  [ 1.33079737e+00 -8.15422042e-01 -1.17111067e+00]\n",
            "  [ 8.45635563e-01 -1.60972481e+00  8.05194221e-01]\n",
            "  [ 8.84163134e-01 -1.94393898e+00 -4.85466000e-01]\n",
            "  [ 1.00476429e+00 -1.75177104e+00 -1.17674786e+00]\n",
            "  [ 9.83665947e-01 -1.31927451e+00  1.35225324e+00]\n",
            "  [ 8.95693644e-01 -7.25401175e-01 -4.42807554e-01]\n",
            "  [ 4.47108331e-01 -4.25232309e-01 -1.14738342e+00]\n",
            "  [-1.49243200e-01 -5.21166719e-01 -3.49805822e-01]\n",
            "  [ 8.17945271e-03 -1.87738757e-01 -7.08312519e-01]\n",
            "  [ 7.51525354e-01 -2.75543142e-01 -2.43875540e+00]\n",
            "  [ 9.69985306e-01 -5.79436817e-01 -9.15877808e-01]\n",
            "  [ 1.68819842e+00  7.09228652e-02 -2.14012171e+00]\n",
            "  [ 2.49576819e+00  1.04479230e+00 -7.93884394e-01]\n",
            "  [ 2.19339669e+00  1.56730169e+00 -6.86901898e-01]\n",
            "  [ 1.37539323e+00  1.49273665e+00  1.07096125e-01]\n",
            "  [ 1.29230761e+00  1.54073385e+00 -1.94726862e+00]\n",
            "  [ 1.53230023e+00  1.02886231e+00 -4.43611444e-01]\n",
            "  [ 1.21231114e+00  6.32560869e-01 -2.15054176e-01]\n",
            "  [ 7.19455185e-01  1.01168895e+00 -1.30167141e+00]\n",
            "  [ 4.38662820e-01  1.02931660e+00 -1.90543219e+00]\n",
            "  [ 2.62793766e-01  2.12635003e-01 -7.27906521e-01]\n",
            "  [ 4.93844705e-01 -8.74643937e-02  2.93239993e-01]\n",
            "  [ 7.15640239e-01  2.53065871e-02 -9.12245060e-01]\n",
            "  [-2.90635073e-02 -8.42044946e-02 -1.51929414e-02]\n",
            "  [-1.45404311e+00  5.20986458e-01 -1.00964830e+00]\n",
            "  [-2.52553057e+00  1.42758424e+00 -2.85533250e-01]\n",
            "  [-2.88749151e+00  9.81472208e-01  9.03231475e-01]\n",
            "  [-2.78401974e+00  2.38256807e-01 -9.81994060e-02]\n",
            "  [-2.45993724e+00  4.87439478e-01 -8.32136823e-01]\n",
            "  [-2.22586496e+00  1.23184871e+00  1.92319216e+00]\n",
            "  [-2.22172330e+00  1.64376399e+00  9.64040669e-01]\n",
            "  [-1.82526223e+00  1.88102827e+00  9.34355732e-01]\n",
            "  [-9.51584459e-01  1.02559429e+00  1.13697037e+00]\n",
            "  [-7.37440910e-01  1.40369135e-02  5.00851250e-01]\n",
            "  [-3.90738528e-01  3.53724329e-02  5.07585240e-01]\n",
            "  [-6.74998175e-02  7.49171096e-01  4.83722733e-01]\n",
            "  [-4.31399934e-01  1.58671989e+00  9.43699793e-01]\n",
            "  [-8.18733649e-01  1.58261867e+00  7.26409795e-01]\n",
            "  [-9.59634944e-01  5.10584650e-01  2.41759683e-02]\n",
            "  [-8.36528014e-01  3.03385758e-01 -4.84505295e-01]\n",
            "  [-2.48655818e-01  1.28154881e+00 -9.00574564e-01]\n",
            "  [ 4.50190155e-01  1.52996404e+00  8.97923676e-01]\n",
            "  [ 2.48048285e-01  8.45206985e-01  1.32423106e+00]\n",
            "  [ 5.93694373e-02  6.80437246e-01  1.35444861e-01]\n",
            "  [ 3.73244910e-01  8.42458379e-01  8.22474158e-01]\n",
            "  [ 7.45274556e-01  7.61649284e-01 -6.54735406e-01]\n",
            "  [ 7.17878041e-01  5.58511292e-01 -7.38900025e-01]\n",
            "  [ 4.07440759e-01 -3.36460204e-01  1.00871205e+00]\n",
            "  [ 6.59144083e-01 -7.17986803e-01  1.08775047e+00]\n",
            "  [ 6.95000960e-01  8.28052984e-02 -1.28924672e+00]\n",
            "  [-2.99759038e-02  2.90706190e-01 -4.98417518e-01]\n",
            "  [-5.79682720e-01  6.55651055e-02  1.95119787e-02]\n",
            "  [-1.09910480e+00 -1.49958390e-01 -1.36680227e-01]\n",
            "  [-1.16077387e+00 -7.13565687e-01 -8.82892694e-01]\n",
            "  [-3.50263438e-01 -1.04610344e+00  1.35939702e-02]\n",
            "  [ 7.65180144e-01 -1.10657216e+00  9.88741716e-01]\n",
            "  [ 1.05272174e+00 -7.21287533e-01 -1.43600862e-01]\n",
            "  [ 9.64980550e-01 -5.99797089e-01  8.34508577e-01]\n",
            "  [ 8.22207048e-01 -4.78795250e-01 -1.25301518e-01]\n",
            "  [ 7.27283205e-01 -1.71183912e-01 -8.03344959e-01]\n",
            "  [ 3.87485549e-01  6.18819318e-02  4.16457657e-01]\n",
            "  [ 1.63139547e-02 -1.38145515e-02  5.79239446e-01]\n",
            "  [-3.36925271e-01  2.48101076e-01 -6.56428946e-01]\n",
            "  [-8.07360486e-01  3.34482657e-01 -3.62463909e-01]\n",
            "  [-9.89482429e-01 -2.20101941e-01  4.55241018e-02]\n",
            "  [-7.22861693e-01 -7.10273988e-01 -6.05984564e-01]\n",
            "  [-4.37836629e-01 -4.96520543e-01 -1.85222253e+00]\n",
            "  [-3.91178257e-01 -1.71526012e-01  1.80874246e+00]\n",
            "  [-5.51863984e-01 -8.41449205e-02  5.34915785e-01]\n",
            "  [-7.59369004e-01 -1.08877416e-01  7.57623315e-01]\n",
            "  [-7.35566975e-01 -3.26786951e-01 -4.10976459e-01]\n",
            "  [-1.24514695e+00 -7.24230650e-01 -3.12236656e-01]\n",
            "  [-1.71476406e+00 -3.17681675e-01  2.29704295e-01]\n",
            "  [-1.21670593e+00 -2.15203766e-01  6.52543221e-01]\n",
            "  [-5.22806856e-01 -4.89536438e-01  1.50654768e-01]\n",
            "  [-7.75496870e-01 -7.52017011e-01  2.16038233e-01]\n",
            "  [-1.49524797e+00 -8.54276325e-01  8.16678799e-01]\n",
            "  [-8.96971691e-01 -7.62484586e-01  7.23090460e-01]\n",
            "  [ 1.01250332e-01 -2.93480044e-01  5.10051740e-01]\n",
            "  [ 1.12186274e-01  1.43363742e-01  5.59879031e-01]\n",
            "  [ 4.17253223e-01  7.59944587e-01  2.34712677e-01]\n",
            "  [ 6.84578125e-01  4.14317316e-01 -3.37694871e-01]\n",
            "  [-4.96762214e-01 -5.80516469e-01 -5.49649141e-01]\n",
            "  [-1.46914158e+00 -4.40974649e-01 -9.40105165e-01]\n",
            "  [-1.55317268e+00 -4.38309647e-01  2.28717702e-01]\n",
            "  [-1.47417532e+00 -6.24656737e-01 -2.96425116e-01]\n",
            "  [-2.03237638e+00 -1.97072106e-01  3.60777812e-01]\n",
            "  [-2.30812688e+00 -1.56069611e-01  1.28710151e+00]\n",
            "  [-2.02423238e+00 -1.59619095e-01  8.05151607e-01]\n",
            "  [-1.41974006e+00  6.76457869e-01  7.38349311e-01]\n",
            "  [-8.13152818e-01  1.22675897e+00 -1.68605278e-01]\n",
            "  [-1.72027812e-01  6.90525747e-01  1.09534227e+00]\n",
            "  [ 2.30539777e-02 -2.16227114e-01  4.47310306e-01]\n",
            "  [-3.71466141e-01 -4.38598312e-01  3.08959707e-01]\n",
            "  [-6.45639171e-01 -4.15461965e-01 -7.46354159e-01]\n",
            "  [-1.86868507e-01  1.41272575e-01 -4.94212111e-01]\n",
            "  [ 3.94995113e-01  1.24247205e+00  4.11007525e-01]\n",
            "  [ 3.56792574e-01  1.41122487e+00  3.09435122e-01]\n",
            "  [ 6.05100380e-01  3.18839377e-01 -2.21264584e-03]\n",
            "  [ 1.14686692e+00 -1.28824538e-01  1.91934967e-01]\n",
            "  [ 1.53981697e+00  8.03616292e-02 -1.29219402e+00]\n",
            "  [ 1.45809503e+00 -1.55345362e-01  1.06931954e+00]\n",
            "  [ 1.13160908e+00 -2.45072972e-01 -1.40875228e+00]\n",
            "  [ 5.73899656e-01 -4.92497763e-01  4.00078412e-01]\n",
            "  [-6.48481438e-02 -1.11471997e+00  5.54098591e-01]\n",
            "  [-4.46736495e-01 -6.89668104e-01 -1.55745205e+00]\n",
            "  [-5.92058437e-01 -5.84451786e-01 -1.15584577e+00]\n",
            "  [-6.82911136e-01 -1.86674245e+00  8.52016227e-01]\n",
            "  [-3.31356968e-01 -3.07426065e+00  6.28896606e-01]\n",
            "  [-1.57100379e-01 -3.05184616e+00 -1.11999104e-03]\n",
            "  [-2.57729151e-01 -2.09489505e+00  2.28313610e+00]\n",
            "  [ 2.68173509e-01 -1.14357260e+00  2.30696832e+00]\n",
            "  [ 8.40720214e-01 -4.86533191e-01 -3.47453605e-01]\n",
            "  [ 1.15862908e+00 -5.20683778e-01  7.48210464e-01]\n",
            "  [ 1.21954199e+00 -1.02910207e+00  2.19570542e+00]]]---------------------\n",
            "\n",
            "\n",
            "(20, 250, 3)\n",
            "(9, 250, 3)\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [2.83328916e-16 1.17239551e-16 6.92779167e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "context_points 6\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 9\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch1_sub_SNR_inf_1/epoch=59-val/norm_mse=0.31.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 10\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [40, 58, 189, 242, 17, 108, 32, 114, 234, 235, 84, 136, 97, 179, 129, 220, 187, 113, 44, 205, 100, 67, 105, 163, 138, 128, 26, 202, 157, 22, 76, 208, 68, 56, 153, 82, 190, 112, 73, 131, 110, 36, 195, 98, 104, 227, 123, 184, 147, 188, 206, 146, 6, 229, 177, 59, 51, 241, 170, 134, 16, 64, 31, 223, 91, 161, 211, 168, 109, 130, 24, 55, 99, 191, 90, 47, 139, 164, 27, 228, 62, 57, 37, 171, 120, 199, 209, 1, 49, 7, 28, 194, 144, 8, 11, 81, 203, 72, 169, 222, 19, 87, 148, 125, 124, 175, 212, 63, 89, 14, 118, 15, 243, 127, 217, 107, 43, 60, 182, 116, 196, 132, 13, 158, 141, 42, 215, 103, 198, 18, 159, 78, 231, 61, 122, 178, 75, 219, 192, 92, 180, 152, 41, 151, 181, 237, 149, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 117, 236, 201, 216, 145, 93, 221, 142, 66, 52, 174, 10, 4, 197, 39, 186, 88, 165, 200, 133, 167, 34, 65, 29, 115, 70, 5, 20, 160, 101, 121, 204, 46, 176, 71, 77, 50, 166, 155, 30, 225, 25, 156, 135, 172, 213, 240, 183, 218, 54, 239, 80, 238, 173, 102, 3, 33, 185, 233, 21, 162, 140, 48, 106, 230, 224, 210, 207, 12, 119, 214, 79, 83, 126, 9, 226, 0, 193, 154, 143, 232]\n",
            "self.series.num_trials(split): 9\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch1_sub_SNR_inf_1/epoch=59-val/norm_mse=0.31.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch1_sub_SNR_inf_1/epoch=59-val/norm_mse=0.31.ckpt\n",
            "Testing DataLoader 0: 100% 18/18 [00:00<00:00, 24.13it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9998988509178162\n",
            "     test/class_loss       0.049531206488609314\n",
            "   test/forecast_loss       0.31442663073539734\n",
            "        test/loss           0.31443166732788086\n",
            "        test/mae            0.4014582931995392\n",
            "        test/mape            2.681421995162964\n",
            "        test/mse            0.31442663073539734\n",
            "      test/norm_mae         0.4014582931995392\n",
            "      test/norm_mse         0.31442663073539734\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.5586891770362854\n",
            "       test/smape           0.8079477548599243\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: generated_gc_SNR_6\n",
            "File list:  ['./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch2_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch2_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.27998699 -0.34327452  0.045271  ]\n",
            "std_dev yc: [3.31902638 3.68126162 1.25874214]\n",
            "\n",
            "(20, 250, 3)\n",
            "(250, 60)\n",
            "[3.05155337 3.26340089 1.19964105 3.0214477  3.42196559 1.24797031\n",
            " 2.99083682 3.28383927 1.2073929  2.87150134 3.30356706 1.20793307\n",
            " 2.80944211 3.25481015 1.16455849 2.79110019 3.05185588 1.29143061\n",
            " 2.93848899 3.29083408 1.25768193 3.07804889 3.39303804 1.12140427\n",
            " 3.0943009  3.23990594 1.19456292 3.07192807 3.21904584 1.14440419\n",
            " 3.02060323 3.23639683 1.15886466 2.93851658 3.25178862 1.18174412\n",
            " 2.87424645 3.35919122 1.19688428 3.02498457 3.13503403 1.11332737\n",
            " 2.97660322 3.09262181 1.20629434 2.96610999 2.96764441 1.21344584\n",
            " 3.00647783 3.14109851 1.24668411 2.97822199 3.08363261 1.20452045\n",
            " 3.06566746 3.34721324 1.18044312 3.18324482 3.26245521 1.14020384]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-1.47064781e-01 -8.07121154e-02  5.68028385e-02]\n",
            "  [ 1.27861967e-01 -2.86317142e-01  2.85164633e-01]\n",
            "  [ 3.64936877e-01 -3.41791937e-03  1.32194571e+00]\n",
            "  ...\n",
            "  [ 1.10051218e+00  8.72207325e-02  7.33844055e-01]\n",
            "  [ 1.00824230e+00 -8.10320402e-01 -5.54679002e-01]\n",
            "  [ 1.00983631e+00 -1.31627886e-01 -4.89800580e-01]]\n",
            "\n",
            " [[-2.32454054e-03 -3.38270837e-01  2.14603356e+00]\n",
            "  [-1.88596993e-01 -3.01318705e-02 -1.61758912e-01]\n",
            "  [-7.99210189e-01 -1.18612991e-01  6.00927553e-02]\n",
            "  ...\n",
            "  [ 4.62747248e-01  1.27188120e+00  7.18048056e-01]\n",
            "  [-3.78541252e-01  5.06082111e-01 -2.48699626e-01]\n",
            "  [-6.44791561e-01 -6.41112363e-01  5.88604403e-01]]\n",
            "\n",
            " [[-3.86002373e-01 -2.91379504e-01 -6.86486522e-01]\n",
            "  [-2.99383267e-01  1.60973962e-01  4.44211553e-01]\n",
            "  [ 7.12281567e-02 -1.52005506e-01 -2.45358797e-01]\n",
            "  ...\n",
            "  [-1.87261195e+00 -2.21169670e+00  4.05698402e-01]\n",
            "  [-1.02053355e+00 -1.92752735e+00  3.89658951e-01]\n",
            "  [-4.93597248e-01 -9.50738313e-01  3.00127796e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-7.53270544e-02 -1.76809216e-01  4.55634119e-01]\n",
            "  [-4.29709767e-02  3.88244702e-01  1.58539634e+00]\n",
            "  [ 6.91824908e-01  9.84258847e-02  8.88061901e-03]\n",
            "  ...\n",
            "  [ 6.05513237e-01  9.62302582e-01 -1.60095404e+00]\n",
            "  [ 1.65928924e-01  2.05165878e+00  1.07025307e-01]\n",
            "  [-1.31618693e-01 -1.16105235e-01 -8.26771617e-01]]\n",
            "\n",
            " [[ 3.12299388e-01 -1.93976929e-01 -1.09467566e+00]\n",
            "  [-6.09581019e-02 -2.55452116e-01 -7.05620165e-01]\n",
            "  [ 2.17973766e-01  8.18913336e-02  8.17490106e-01]\n",
            "  ...\n",
            "  [-8.79779793e-01 -6.12780379e-01 -1.13835551e-01]\n",
            "  [-3.78302541e-01 -4.95162242e-01  1.02448288e+00]\n",
            "  [ 6.21921531e-01  2.05152444e-01  9.70765169e-01]]\n",
            "\n",
            " [[ 4.15001797e-01 -2.32670152e-01  5.97709241e-01]\n",
            "  [ 5.88132504e-01  4.27180906e-02 -5.58536578e-01]\n",
            "  [-3.73112428e-01  2.01009221e-01 -6.21270159e-01]\n",
            "  ...\n",
            "  [ 1.51210856e+00 -1.30546560e+00  2.28172536e+00]\n",
            "  [ 1.03261009e+00 -7.93889382e-01 -2.75687619e-01]\n",
            "  [ 8.12416900e-01 -1.90724702e-01  2.54769912e-01]]]---------------------\n",
            "\n",
            "\n",
            "(9, 250, 3)\n",
            "(250, 27)\n",
            "[3.24312957 3.43463074 1.20076043 3.2437275  3.43196594 1.15861721\n",
            " 3.20387068 3.30042661 1.16359426 3.23875997 3.41930561 1.27295341\n",
            " 3.19143936 3.42112254 1.21289459 3.26098638 3.29155865 1.18318647\n",
            " 3.30045875 3.36182518 1.10427117 3.13039169 3.36281684 1.30709613\n",
            " 3.11079628 3.22350716 1.3037222 ]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 1.15351807e-01 -1.25503490e-01 -3.84935543e-01]\n",
            "  [ 9.16285242e-02  4.36991546e-01  1.06801326e+00]\n",
            "  [ 1.97485528e-01 -3.66528099e-01 -3.73076088e-02]\n",
            "  ...\n",
            "  [-1.17852523e+00 -2.71529920e+00  5.02663693e-02]\n",
            "  [-1.09479846e+00 -1.63633919e+00  2.39311301e+00]\n",
            "  [-1.39550954e+00 -1.64766468e+00  3.28047155e+00]]\n",
            "\n",
            " [[-5.85838132e-01 -1.71581979e-02  1.98465049e-01]\n",
            "  [-1.28596942e-01 -9.27868716e-02 -5.55115147e-01]\n",
            "  [-4.27098928e-01 -1.76086207e-01 -1.30603169e-01]\n",
            "  ...\n",
            "  [ 1.25482808e-01  6.12592692e-01  3.64362947e-01]\n",
            "  [ 3.21311371e-01  1.17375977e-01  6.23738342e-01]\n",
            "  [ 3.08602468e-02 -1.19832921e-01 -5.95822180e-01]]\n",
            "\n",
            " [[ 1.36120235e-02  2.27546789e-02  2.08227455e+00]\n",
            "  [ 7.93534404e-03  2.15076117e-01  4.73203250e-01]\n",
            "  [-1.58220624e-01  1.30562029e-01  9.67698672e-01]\n",
            "  ...\n",
            "  [-2.19894166e-01 -1.60950809e-01 -1.11979540e+00]\n",
            "  [ 3.87428814e-01  8.85203035e-01  1.63923033e+00]\n",
            "  [ 2.81591066e-01  6.72578163e-01  4.12558570e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.35843714e-01 -2.19106487e-01 -1.04217201e+00]\n",
            "  [ 1.34560056e-01 -6.76442802e-02  6.77377271e-01]\n",
            "  [ 5.76986767e-01  2.09084133e-01  8.01814099e-01]\n",
            "  ...\n",
            "  [-2.54228634e-01  3.92611912e-01  8.00845883e-02]\n",
            "  [-3.98377810e-02 -3.00832195e-01  5.34368880e-03]\n",
            "  [-4.15511035e-02 -3.04370149e-01 -2.37398554e-01]]\n",
            "\n",
            " [[ 6.80971382e-02  5.50931508e-02  7.97268270e-01]\n",
            "  [ 5.10910193e-01  3.65094168e-01 -3.82374519e-01]\n",
            "  [-1.03425665e-01 -9.11575829e-02 -1.52877844e-01]\n",
            "  ...\n",
            "  [-1.06542558e+00 -1.94738346e+00  2.46908536e-02]\n",
            "  [-1.09986152e+00 -2.36642915e+00  1.06499634e+00]\n",
            "  [-1.23716466e+00 -2.19957885e+00 -2.85301530e-01]]\n",
            "\n",
            " [[-3.09786926e-01 -4.12976066e-01  5.77635169e-01]\n",
            "  [ 1.52241116e-02 -3.56903956e-01 -4.16246701e-01]\n",
            "  [-3.80483536e-01 -2.06639080e-06 -2.23188308e-01]\n",
            "  ...\n",
            "  [ 1.76283826e-01 -8.47830222e-02 -2.87659122e-01]\n",
            "  [ 7.37511797e-02  6.83833189e-01 -8.52821085e-01]\n",
            "  [-1.10296793e-01  6.77467697e-03  1.31280634e-01]]]---------------------\n",
            "\n",
            "\n",
            "(1, 250, 3)\n",
            "(250, 3)\n",
            "[3.31902638 3.68126162 1.25874214]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 2.28731421e-01 -1.77097365e-01 -3.49746247e-01]\n",
            "  [-6.33344887e-01  4.30083574e-01 -1.35308201e+00]\n",
            "  [ 1.63359423e-01  2.72362140e-01 -5.60562959e-01]\n",
            "  [-8.18037410e-02  2.78866742e-01 -2.31660960e-01]\n",
            "  [-3.24673407e-01  1.34060331e-01 -1.33196586e+00]\n",
            "  [-5.25490062e-01 -2.38439703e-01  7.71969040e-01]\n",
            "  [-1.29577894e+00  2.45339246e-01 -4.14667700e-01]\n",
            "  [-9.97644099e-01  1.64740141e-02  1.46025400e-01]\n",
            "  [-8.06342129e-02 -7.17658924e-01 -6.88535572e-01]\n",
            "  [ 4.91099940e-02 -3.16991877e-01 -6.39822615e-02]\n",
            "  [-2.74119603e-01 -5.27230021e-01  6.50219626e-01]\n",
            "  [-1.68436395e-01 -6.16054331e-01 -6.54490973e-01]\n",
            "  [-4.39099593e-01 -2.08700629e-01 -9.37716426e-02]\n",
            "  [-1.06908941e+00 -2.46635572e-01 -2.53898030e-01]\n",
            "  [-1.43908711e+00 -2.75107962e-01  1.71069103e+00]\n",
            "  [-1.34778886e+00 -1.45364948e+00  1.50401095e+00]\n",
            "  [-1.02632990e+00 -2.24413311e+00 -1.69195001e+00]\n",
            "  [-7.91899346e-01 -1.07795125e+00 -1.90531358e-02]\n",
            "  [-8.07040636e-01 -6.31490850e-01  2.26764594e+00]\n",
            "  [-5.67171002e-01 -1.68969114e+00  2.55665014e-01]\n",
            "  [-5.24948252e-01 -1.43694116e+00  1.36156519e+00]\n",
            "  [-8.16816224e-01 -1.67231442e+00  2.22291231e+00]\n",
            "  [-9.57502575e-01 -2.18705346e+00  8.90786120e-01]\n",
            "  [-6.11267400e-01 -9.53144333e-01  7.51244635e-01]\n",
            "  [-2.75949361e-02 -1.59294128e+00  1.74446809e+00]\n",
            "  [ 1.16060446e-01 -1.77003434e+00 -4.78014899e-04]\n",
            "  [-2.21168533e-02 -8.09930740e-01  6.54701501e-01]\n",
            "  [-5.56015910e-01 -1.73243737e-01  3.35760569e-01]\n",
            "  [-1.52014703e+00 -1.49813604e-01  1.82024228e+00]\n",
            "  [-1.42030303e+00 -9.11136043e-01 -1.25531493e+00]\n",
            "  [-1.05418313e+00  2.92516264e-01  9.87519142e-01]\n",
            "  [-7.77345723e-01 -5.45805433e-01  3.98896267e-01]\n",
            "  [ 2.67327868e-01 -7.77446175e-01  1.41551869e-01]\n",
            "  [ 1.33854724e+00 -1.99532490e-01  9.34299374e-01]\n",
            "  [ 9.66286967e-01  9.73943949e-01 -7.39891650e-01]\n",
            "  [ 3.57458817e-01  1.87328055e+00  7.18892616e-01]\n",
            "  [ 2.71142617e-01  1.06292360e+00 -1.78048960e+00]\n",
            "  [ 6.06667067e-01  2.11589205e+00 -8.31289072e-01]\n",
            "  [ 1.59370397e+00  7.58166685e-01 -6.17575988e-01]\n",
            "  [ 2.42028305e+00  1.35538627e+00 -3.01222621e+00]\n",
            "  [ 1.91403596e+00  2.79743625e+00 -3.90518148e-03]\n",
            "  [ 9.98160830e-01  1.62957750e+00  2.67605930e-01]\n",
            "  [ 2.46530869e-01  1.46053995e+00 -2.13158344e+00]\n",
            "  [-1.97725403e-01  1.81599337e+00 -5.34345711e-01]\n",
            "  [-1.38550229e-02  4.25668680e-01 -1.07966395e+00]\n",
            "  [ 7.69375338e-01 -2.94624330e-02 -1.53037455e+00]\n",
            "  [ 8.84020839e-01  8.39968488e-01 -1.40965726e+00]\n",
            "  [ 1.33124090e-01  6.94620135e-01 -1.41291445e+00]\n",
            "  [-6.33505193e-01  7.60146352e-01 -9.15169283e-01]\n",
            "  [-4.32623031e-01  9.20721632e-01 -2.10496729e-01]\n",
            "  [ 1.04622661e-01  2.57603616e-01  8.56048890e-01]\n",
            "  [ 5.33806640e-01  4.08338225e-01 -1.97244939e+00]\n",
            "  [ 6.07947535e-01  1.76095530e+00 -1.50957192e+00]\n",
            "  [ 5.64813494e-01  1.56202654e+00  2.99879393e-01]\n",
            "  [ 3.83046015e-01  1.14301290e+00 -4.04670561e-01]\n",
            "  [ 4.59728178e-02  1.16262443e+00 -1.26181429e+00]\n",
            "  [ 9.22450583e-02  1.00021276e+00 -3.99910218e-01]\n",
            "  [ 7.56811585e-01 -8.87035217e-02 -1.41741693e+00]\n",
            "  [ 9.54345417e-01  3.34337273e-01  7.32727616e-01]\n",
            "  [-6.27338312e-02 -8.86665623e-01  8.93819580e-02]\n",
            "  [-3.85443416e-01 -5.24954974e-01  2.39050973e-01]\n",
            "  [ 1.11746092e-01 -1.09290627e-01  1.57733880e+00]\n",
            "  [-5.10670899e-02 -6.99135357e-01  5.47708064e-01]\n",
            "  [-3.95578675e-01 -8.88409388e-01  9.53549970e-01]\n",
            "  [-4.84154025e-01 -6.69019601e-01  4.50092894e-01]\n",
            "  [-1.20519474e-01 -9.41388504e-02  1.11535655e+00]\n",
            "  [ 6.81498868e-02  2.37212438e-03 -5.23359049e-01]\n",
            "  [ 3.99459177e-01  3.12541504e-01  2.72589776e-01]\n",
            "  [ 6.88657444e-01  5.75312800e-01 -4.08245389e-02]\n",
            "  [ 4.86273042e-01  7.36265196e-01 -1.05583229e+00]\n",
            "  [ 1.34903754e-01  8.81664671e-01 -1.60745214e+00]\n",
            "  [-3.47936893e-01  9.42593290e-01 -2.90235037e-01]\n",
            "  [-6.30543179e-01  5.89351890e-01  2.61547361e-01]\n",
            "  [-9.49144483e-02  3.58833122e-01 -1.28556360e+00]\n",
            "  [ 3.34518589e-01  9.54684782e-01 -4.59396777e-01]\n",
            "  [ 2.34412549e-01  1.81575143e+00  6.75741653e-02]\n",
            "  [-4.04628435e-01  1.30706329e+00 -1.15612840e-01]\n",
            "  [-5.26328869e-01 -2.17391904e-01 -2.90641275e+00]\n",
            "  [ 7.18821772e-01  1.03975723e+00  1.03618330e-01]\n",
            "  [ 1.59463990e+00  1.48392056e+00 -3.65938381e-01]\n",
            "  [ 6.91203750e-01  1.76047259e+00 -1.25818868e+00]\n",
            "  [-5.21474950e-03  2.33691694e+00 -4.75558954e-01]\n",
            "  [-4.16846397e-01  1.64274595e+00 -6.46903035e-01]\n",
            "  [-7.65144693e-01  6.01867156e-01 -3.30187810e-01]\n",
            "  [-5.52919486e-03  2.17401759e-01 -7.64838812e-01]\n",
            "  [ 1.13886337e+00  1.01281589e+00 -7.66642065e-01]\n",
            "  [ 1.57110452e+00  1.60568150e+00  1.02857703e+00]\n",
            "  [ 1.32854169e+00  1.29702089e+00 -4.38404211e-01]\n",
            "  [ 8.86963404e-01  1.03499482e+00 -2.38778184e-01]\n",
            "  [ 1.27605072e+00  9.36201953e-01 -1.03152465e+00]\n",
            "  [ 1.99901938e+00  1.16825667e+00  1.05419002e+00]\n",
            "  [ 1.90644317e+00  6.50925996e-01 -6.14725283e-01]\n",
            "  [ 1.76707231e+00  1.05283215e+00  4.32669293e-01]\n",
            "  [ 1.95354407e+00  3.43569407e-01 -3.36806555e-01]\n",
            "  [ 1.93440512e+00  3.88942785e-01 -1.22058389e+00]\n",
            "  [ 1.88557611e+00  1.14658719e+00 -4.34583915e-01]\n",
            "  [ 1.96144280e+00  8.60742442e-01 -9.26266803e-01]\n",
            "  [ 1.40720564e+00  1.63065403e+00 -1.04567098e+00]\n",
            "  [ 9.10234694e-01  1.06264912e+00 -1.11228289e-01]\n",
            "  [ 6.72635758e-01 -1.01870979e-01 -3.11091407e-01]\n",
            "  [ 4.86285375e-01 -3.46180610e-01 -6.73016114e-01]\n",
            "  [ 2.28618636e-02 -1.46210888e-01  3.82256014e-01]\n",
            "  [ 1.45891827e-01 -2.26239843e-01  1.05443420e+00]\n",
            "  [ 4.57947993e-01 -1.11343348e+00 -8.07313259e-01]\n",
            "  [ 4.23762732e-01 -2.72857948e-01  2.81057883e-01]\n",
            "  [ 6.99626127e-01 -5.40220604e-01  6.63523266e-01]\n",
            "  [ 7.79693718e-01 -6.55836308e-02  1.72707514e+00]\n",
            "  [ 4.65431265e-01 -1.59859104e-01 -2.85920130e-01]\n",
            "  [-1.17238718e-01  4.04660230e-01  4.51978983e-01]\n",
            "  [-1.31759058e+00 -8.56991516e-01  1.07375605e+00]\n",
            "  [-1.40531022e+00 -1.65790441e+00  8.79210992e-01]\n",
            "  [-8.04401419e-01 -1.34660117e+00  1.01897225e+00]\n",
            "  [-9.58106988e-01 -1.21185723e-01  2.96870444e+00]\n",
            "  [-1.10194180e+00 -1.22799788e+00  4.37895972e-01]\n",
            "  [-1.12027384e+00 -5.16346656e-01  8.31302119e-01]\n",
            "  [-1.31036201e+00 -1.04691444e+00  1.99025029e+00]\n",
            "  [-1.00125963e+00 -2.26465986e+00  5.96500307e-01]\n",
            "  [-1.43189716e-01 -1.05943344e+00  5.47618529e-01]\n",
            "  [ 4.18657751e-01 -1.12222899e+00  2.07263766e+00]\n",
            "  [ 1.84424874e-01 -1.04016785e+00  4.38121853e-01]\n",
            "  [-4.46594192e-01 -3.98707913e-01  8.45625228e-01]\n",
            "  [-5.24868358e-01 -1.04537638e+00  1.23619345e+00]\n",
            "  [-1.10161883e+00 -1.52771100e+00 -5.04897074e-01]\n",
            "  [-1.88162223e+00 -1.51117348e+00  6.64714730e-01]\n",
            "  [-1.44661998e+00 -2.23679933e+00  2.05164745e-02]\n",
            "  [-2.98307610e-01 -8.44614026e-01  1.32766849e+00]\n",
            "  [ 2.03901684e-02 -1.63125666e+00  1.99888768e-01]\n",
            "  [ 4.57542740e-01 -2.72648119e-01  1.19089479e+00]\n",
            "  [ 6.64413261e-01  9.49346831e-02  1.24629525e+00]\n",
            "  [ 6.44984059e-01 -8.77864746e-02 -6.70973258e-02]\n",
            "  [ 7.68418512e-01  7.75342777e-01 -1.30405691e+00]\n",
            "  [ 7.30317603e-01  8.12561202e-01 -1.38287848e+00]\n",
            "  [ 6.80128022e-01  5.86497590e-01  1.14928164e+00]\n",
            "  [ 9.21266799e-01 -4.49297762e-01 -6.33523064e-02]\n",
            "  [ 1.36728641e+00 -7.42019331e-01 -9.74850076e-01]\n",
            "  [ 1.33079737e+00  8.17654706e-01 -1.17111067e+00]\n",
            "  [ 8.45635563e-01  6.32064016e-01  8.05194221e-01]\n",
            "  [ 8.84163134e-01 -1.03565253e+00 -4.85466000e-01]\n",
            "  [ 1.00476429e+00 -5.09542806e-01 -1.17674786e+00]\n",
            "  [ 9.83665947e-01  2.88285426e-01  1.35225324e+00]\n",
            "  [ 8.95693644e-01 -6.06275356e-01 -4.42807554e-01]\n",
            "  [ 4.47108331e-01  4.37852742e-01 -1.14738342e+00]\n",
            "  [-1.49243200e-01  8.79277479e-01 -3.49805822e-01]\n",
            "  [ 8.17945271e-03 -8.47097605e-02 -7.08312519e-01]\n",
            "  [ 7.51525354e-01  7.12892433e-01 -2.43875540e+00]\n",
            "  [ 9.69985306e-01  1.11173466e+00 -9.15877808e-01]\n",
            "  [ 1.68819842e+00  8.26556390e-01 -2.14012171e+00]\n",
            "  [ 2.49576819e+00  1.33726985e+00 -7.93884394e-01]\n",
            "  [ 2.19339669e+00  1.68699051e+00 -6.86901898e-01]\n",
            "  [ 1.37539323e+00  2.27005476e+00  1.07096125e-01]\n",
            "  [ 1.29230761e+00  1.10759179e+00 -1.94726862e+00]\n",
            "  [ 1.53230023e+00  2.06550401e+00 -4.43611444e-01]\n",
            "  [ 1.21231114e+00  2.02245918e+00 -2.15054176e-01]\n",
            "  [ 7.19455185e-01  1.20138185e+00 -1.30167141e+00]\n",
            "  [ 4.38662820e-01  7.50285884e-01 -1.90543219e+00]\n",
            "  [ 2.62793766e-01  1.12247471e+00 -7.27906521e-01]\n",
            "  [ 4.93844705e-01  1.74742940e-02  2.93239993e-01]\n",
            "  [ 7.15640239e-01 -3.36258943e-01 -9.12245060e-01]\n",
            "  [-2.90635073e-02  6.58265002e-01 -1.51929414e-02]\n",
            "  [-1.45404311e+00 -1.71226987e-01 -1.00964830e+00]\n",
            "  [-2.52553057e+00 -5.18459470e-01 -2.85533250e-01]\n",
            "  [-2.88749151e+00 -1.40998250e-01  9.03231475e-01]\n",
            "  [-2.78401974e+00 -1.29602039e+00 -9.81994060e-02]\n",
            "  [-2.45993724e+00 -1.08329909e+00 -8.32136823e-01]\n",
            "  [-2.22586496e+00 -9.38399757e-01  1.92319216e+00]\n",
            "  [-2.22172330e+00 -6.93294260e-01  9.64040669e-01]\n",
            "  [-1.82526223e+00 -2.59695955e-01  9.34355732e-01]\n",
            "  [-9.51584459e-01 -9.56289568e-01  1.13697037e+00]\n",
            "  [-7.37440910e-01 -2.09663612e-01  5.00851250e-01]\n",
            "  [-3.90738528e-01 -5.29397196e-01  5.07585240e-01]\n",
            "  [-6.74998175e-02 -3.40594210e-02  4.83722733e-01]\n",
            "  [-4.31399934e-01  1.17144412e-01  9.43699793e-01]\n",
            "  [-8.18733649e-01  6.29985640e-02  7.26409795e-01]\n",
            "  [-9.59634944e-01  5.11163728e-01  2.41759683e-02]\n",
            "  [-8.36528014e-01 -9.18669436e-01 -4.84505295e-01]\n",
            "  [-2.48655818e-01 -1.25790887e-01 -9.00574564e-01]\n",
            "  [ 4.50190155e-01  1.01829888e+00  8.97923676e-01]\n",
            "  [ 2.48048285e-01  1.31055916e+00  1.32423106e+00]\n",
            "  [ 5.93694373e-02  1.15856346e+00  1.35444861e-01]\n",
            "  [ 3.73244910e-01  4.78682555e-01  8.22474158e-01]\n",
            "  [ 7.45274556e-01  6.24450424e-01 -6.54735406e-01]\n",
            "  [ 7.17878041e-01  1.05447419e+00 -7.38900025e-01]\n",
            "  [ 4.07440759e-01  8.58492976e-01  1.00871205e+00]\n",
            "  [ 6.59144083e-01 -5.27698560e-01  1.08775047e+00]\n",
            "  [ 6.95000960e-01  7.33948618e-02 -1.28924672e+00]\n",
            "  [-2.99759038e-02  8.98079327e-01 -4.98417518e-01]\n",
            "  [-5.79682720e-01 -1.52745097e-01  1.95119787e-02]\n",
            "  [-1.09910480e+00 -4.38745325e-01 -1.36680227e-01]\n",
            "  [-1.16077387e+00 -7.00080080e-01 -8.82892694e-01]\n",
            "  [-3.50263438e-01 -1.01329732e+00  1.35939702e-02]\n",
            "  [ 7.65180144e-01 -6.76220854e-01  9.88741716e-01]\n",
            "  [ 1.05272174e+00 -1.44507293e-01 -1.43600862e-01]\n",
            "  [ 9.64980550e-01 -4.94980691e-03  8.34508577e-01]\n",
            "  [ 8.22207048e-01 -8.89670333e-02 -1.25301518e-01]\n",
            "  [ 7.27283205e-01 -3.84764128e-02 -8.03344959e-01]\n",
            "  [ 3.87485549e-01  6.76325073e-01  4.16457657e-01]\n",
            "  [ 1.63139547e-02  4.21334528e-01  5.79239446e-01]\n",
            "  [-3.36925271e-01 -1.10666894e-01 -6.56428946e-01]\n",
            "  [-8.07360486e-01  2.89978825e-01 -3.62463909e-01]\n",
            "  [-9.89482429e-01 -1.26315805e-01  4.55241018e-02]\n",
            "  [-7.22861693e-01 -8.59310146e-01 -6.05984564e-01]\n",
            "  [-4.37836629e-01 -5.01395258e-01 -1.85222253e+00]\n",
            "  [-3.91178257e-01 -2.88210743e-01  1.80874246e+00]\n",
            "  [-5.51863984e-01 -5.72424826e-01  5.34915785e-01]\n",
            "  [-7.59369004e-01 -3.12638582e-01  7.57623315e-01]\n",
            "  [-7.35566975e-01 -1.22511007e+00 -4.10976459e-01]\n",
            "  [-1.24514695e+00 -2.01981680e-01 -3.12236656e-01]\n",
            "  [-1.71476406e+00 -1.07996832e+00  2.29704295e-01]\n",
            "  [-1.21670593e+00 -1.00503740e+00  6.52543221e-01]\n",
            "  [-5.22806856e-01 -5.80620044e-01  1.50654768e-01]\n",
            "  [-7.75496870e-01 -8.75000030e-01  2.16038233e-01]\n",
            "  [-1.49524797e+00 -1.23291836e+00  8.16678799e-01]\n",
            "  [-8.96971691e-01 -1.83586735e+00  7.23090460e-01]\n",
            "  [ 1.01250332e-01 -1.06448918e+00  5.10051740e-01]\n",
            "  [ 1.12186274e-01 -3.32969755e-01  5.59879031e-01]\n",
            "  [ 4.17253223e-01  1.56632002e-01  2.34712677e-01]\n",
            "  [ 6.84578125e-01  5.17608261e-01 -3.37694871e-01]\n",
            "  [-4.96762214e-01  1.06097634e+00 -5.49649141e-01]\n",
            "  [-1.46914158e+00 -1.04518439e+00 -9.40105165e-01]\n",
            "  [-1.55317268e+00 -7.77201863e-01  2.28717702e-01]\n",
            "  [-1.47417532e+00 -1.16015027e+00 -2.96425116e-01]\n",
            "  [-2.03237638e+00 -1.03103325e+00  3.60777812e-01]\n",
            "  [-2.30812688e+00 -1.23662307e+00  1.28710151e+00]\n",
            "  [-2.02423238e+00 -1.91854964e+00  8.05151607e-01]\n",
            "  [-1.41974006e+00 -1.45264314e+00  7.38349311e-01]\n",
            "  [-8.13152818e-01 -5.87243258e-01 -1.68605278e-01]\n",
            "  [-1.72027812e-01  2.44334862e-01  1.09534227e+00]\n",
            "  [ 2.30539777e-02  1.93755776e-01  4.47310306e-01]\n",
            "  [-3.71466141e-01 -1.26813618e-01  3.08959707e-01]\n",
            "  [-6.45639171e-01 -3.33890407e-01 -7.46354159e-01]\n",
            "  [-1.86868507e-01 -1.14121041e+00 -4.94212111e-01]\n",
            "  [ 3.94995113e-01 -5.90158068e-02  4.11007525e-01]\n",
            "  [ 3.56792574e-01  6.31316061e-01  3.09435122e-01]\n",
            "  [ 6.05100380e-01  8.87826953e-01 -2.21264584e-03]\n",
            "  [ 1.14686692e+00  2.49355955e-01  1.91934967e-01]\n",
            "  [ 1.53981697e+00  1.16846051e+00 -1.29219402e+00]\n",
            "  [ 1.45809503e+00  1.30006947e+00  1.06931954e+00]\n",
            "  [ 1.13160908e+00  2.75472213e-01 -1.40875228e+00]\n",
            "  [ 5.73899656e-01  8.82478839e-01  4.00078412e-01]\n",
            "  [-6.48481438e-02 -5.22423512e-01  5.54098591e-01]\n",
            "  [-4.46736495e-01 -2.42503669e-01 -1.55745205e+00]\n",
            "  [-5.92058437e-01  4.48996034e-03 -1.15584577e+00]\n",
            "  [-6.82911136e-01 -4.22982185e-02  8.52016227e-01]\n",
            "  [-3.31356968e-01 -1.59069104e+00  6.28896606e-01]\n",
            "  [-1.57100379e-01 -2.34985297e+00 -1.11999104e-03]\n",
            "  [-2.57729151e-01 -2.35525941e+00  2.28313610e+00]\n",
            "  [ 2.68173509e-01 -1.87487267e+00  2.30696832e+00]\n",
            "  [ 8.40720214e-01 -1.08172585e+00 -3.47453605e-01]\n",
            "  [ 1.15862908e+00  3.11514604e-01  7.48210464e-01]\n",
            "  [ 1.21954199e+00  5.62354654e-01  2.19570542e+00]]]---------------------\n",
            "\n",
            "\n",
            "(20, 250, 3)\n",
            "(9, 250, 3)\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.83328916e-16 -8.65973959e-17  6.92779167e-17]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "context_points 6\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 9\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch2_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 10\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [40, 58, 189, 242, 17, 108, 32, 114, 234, 235, 84, 136, 97, 179, 129, 220, 187, 113, 44, 205, 100, 67, 105, 163, 138, 128, 26, 202, 157, 22, 76, 208, 68, 56, 153, 82, 190, 112, 73, 131, 110, 36, 195, 98, 104, 227, 123, 184, 147, 188, 206, 146, 6, 229, 177, 59, 51, 241, 170, 134, 16, 64, 31, 223, 91, 161, 211, 168, 109, 130, 24, 55, 99, 191, 90, 47, 139, 164, 27, 228, 62, 57, 37, 171, 120, 199, 209, 1, 49, 7, 28, 194, 144, 8, 11, 81, 203, 72, 169, 222, 19, 87, 148, 125, 124, 175, 212, 63, 89, 14, 118, 15, 243, 127, 217, 107, 43, 60, 182, 116, 196, 132, 13, 158, 141, 42, 215, 103, 198, 18, 159, 78, 231, 61, 122, 178, 75, 219, 192, 92, 180, 152, 41, 151, 181, 237, 149, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 117, 236, 201, 216, 145, 93, 221, 142, 66, 52, 174, 10, 4, 197, 39, 186, 88, 165, 200, 133, 167, 34, 65, 29, 115, 70, 5, 20, 160, 101, 121, 204, 46, 176, 71, 77, 50, 166, 155, 30, 225, 25, 156, 135, 172, 213, 240, 183, 218, 54, 239, 80, 238, 173, 102, 3, 33, 185, 233, 21, 162, 140, 48, 106, 230, 224, 210, 207, 12, 119, 214, 79, 83, 126, 9, 226, 0, 193, 154, 143, 232]\n",
            "self.series.num_trials(split): 9\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch2_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch2_sub_SNR_inf_1/epoch=56-val/norm_mse=0.35.ckpt\n",
            "Testing DataLoader 0: 100% 18/18 [00:00<00:00, 24.23it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9998988509178162\n",
            "     test/class_loss        0.03551191836595535\n",
            "   test/forecast_loss       0.3545471131801605\n",
            "        test/loss           0.35455062985420227\n",
            "        test/mae            0.4435878098011017\n",
            "        test/mape           2.6624412536621094\n",
            "        test/mse            0.35454708337783813\n",
            "      test/norm_mae         0.4435878098011017\n",
            "      test/norm_mse         0.35454708337783813\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.5964226126670837\n",
            "       test/smape           0.8733440041542053\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: generated_gc_SNR_6\n",
            "File list:  ['./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch3_sub_SNR_inf_1/epoch=54-val/norm_mse=0.12.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch3_sub_SNR_inf_1/epoch=54-val/norm_mse=0.12.ckpt\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.27998699 -0.34327452 -0.6696092 ]\n",
            "std_dev yc: [3.31902638 3.68126162 2.96569592]\n",
            "\n",
            "(20, 250, 3)\n",
            "(250, 60)\n",
            "[3.05155337 3.26340089 2.98000027 3.0214477  3.42196559 2.89621285\n",
            " 2.99083682 3.28383927 2.77387769 2.87150134 3.30356706 2.82163991\n",
            " 2.80944211 3.25481015 2.87983581 2.79110019 3.05185588 3.02651184\n",
            " 2.93848899 3.29083408 3.09068162 3.07804889 3.39303804 3.02101036\n",
            " 3.0943009  3.23990594 2.94998341 3.07192807 3.21904584 2.89059317\n",
            " 3.02060323 3.23639683 2.83045268 2.93851658 3.25178862 2.97632638\n",
            " 2.87424645 3.35919122 2.85171736 3.02498457 3.13503403 2.64694571\n",
            " 2.97660322 3.09262181 2.59256666 2.96610999 2.96764441 2.60201857\n",
            " 3.00647783 3.14109851 2.82584695 2.97822199 3.08363261 3.09621694\n",
            " 3.06566746 3.34721324 2.99076202 3.18324482 3.26245521 2.94772642]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-0.14706478 -0.08071212 -0.05903198]\n",
            "  [ 0.12786197 -0.28631714 -0.04183381]\n",
            "  [ 0.36493688 -0.00341792  0.3640144 ]\n",
            "  ...\n",
            "  [ 1.10051218  0.08722073 -1.27183346]\n",
            "  [ 1.0082423  -0.8103204  -0.76727168]\n",
            "  [ 1.00983631 -0.13162789 -0.46327831]]\n",
            "\n",
            " [[-0.00232454 -0.33827084  0.47381772]\n",
            "  [-0.18859699 -0.03013187 -0.21580847]\n",
            "  [-0.79921019 -0.11861299  0.17397797]\n",
            "  ...\n",
            "  [ 0.46274725  1.2718812   0.08095203]\n",
            "  [-0.37854125  0.50608211 -0.2604465 ]\n",
            "  [-0.64479156 -0.64111236 -0.36922897]]\n",
            "\n",
            " [[-0.38600237 -0.2913795  -0.31110268]\n",
            "  [-0.29938327  0.16097396 -0.26769729]\n",
            "  [ 0.07122816 -0.15200551 -0.26521476]\n",
            "  ...\n",
            "  [-1.87261195 -2.2116967  -0.82578084]\n",
            "  [-1.02053355 -1.92752735 -0.92949623]\n",
            "  [-0.49359725 -0.95073831 -1.31413804]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.07532705 -0.17680922  0.33790542]\n",
            "  [-0.04297098  0.3882447  -0.30020172]\n",
            "  [ 0.69182491  0.09842588 -0.44873988]\n",
            "  ...\n",
            "  [ 0.60551324  0.96230258  1.08569968]\n",
            "  [ 0.16592892  2.05165878  0.27097232]\n",
            "  [-0.13161869 -0.11610523 -0.37825798]]\n",
            "\n",
            " [[ 0.31229939 -0.19397693 -0.09295697]\n",
            "  [-0.0609581  -0.25545212 -0.28515797]\n",
            "  [ 0.21797377  0.08189133  0.13493022]\n",
            "  ...\n",
            "  [-0.87977979 -0.61278038  0.81047231]\n",
            "  [-0.37830254 -0.49516224  0.89840158]\n",
            "  [ 0.62192153  0.20515244  0.76044144]]\n",
            "\n",
            " [[ 0.4150018  -0.23267015 -0.22755386]\n",
            "  [ 0.5881325   0.04271809  0.23556277]\n",
            "  [-0.37311243  0.20100922  0.21312967]\n",
            "  ...\n",
            "  [ 1.51210856 -1.3054656  -1.17086461]\n",
            "  [ 1.03261009 -0.79388938 -1.3359853 ]\n",
            "  [ 0.8124169  -0.1907247  -1.54531769]]]---------------------\n",
            "\n",
            "\n",
            "(9, 250, 3)\n",
            "(250, 27)\n",
            "[3.24312957 3.43463074 3.1543322  3.2437275  3.43196594 3.15509591\n",
            " 3.20387068 3.30042661 3.24006534 3.23875997 3.41930561 3.32111364\n",
            " 3.19143936 3.42112254 3.17028077 3.26098638 3.29155865 3.1641306\n",
            " 3.30045875 3.36182518 3.05584819 3.13039169 3.36281684 2.79976511\n",
            " 3.11079628 3.22350716 2.96168676]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 1.15351807e-01 -1.25503490e-01 -8.07295074e-02]\n",
            "  [ 9.16285242e-02  4.36991546e-01  1.52471155e-01]\n",
            "  [ 1.97485528e-01 -3.66528099e-01  1.48664808e-01]\n",
            "  ...\n",
            "  [-1.17852523e+00 -2.71529920e+00 -1.41529740e+00]\n",
            "  [-1.09479846e+00 -1.63633919e+00 -1.15970144e+00]\n",
            "  [-1.39550954e+00 -1.64766468e+00 -1.22410686e+00]]\n",
            "\n",
            " [[-5.85838132e-01 -1.71581979e-02 -1.53021282e-01]\n",
            "  [-1.28596942e-01 -9.27868716e-02 -1.60466936e-01]\n",
            "  [-4.27098928e-01 -1.76086207e-01 -2.69058487e-01]\n",
            "  ...\n",
            "  [ 1.25482808e-01  6.12592692e-01 -2.92522342e-01]\n",
            "  [ 3.21311371e-01  1.17375977e-01 -7.21940726e-01]\n",
            "  [ 3.08602468e-02 -1.19832921e-01 -6.03579947e-01]]\n",
            "\n",
            " [[ 1.36120235e-02  2.27546789e-02  1.19860853e-02]\n",
            "  [ 7.93534404e-03  2.15076117e-01 -3.85408832e-01]\n",
            "  [-1.58220624e-01  1.30562029e-01  2.17857875e-01]\n",
            "  ...\n",
            "  [-2.19894166e-01 -1.60950809e-01  9.23886625e-01]\n",
            "  [ 3.87428814e-01  8.85203035e-01  9.79980330e-01]\n",
            "  [ 2.81591066e-01  6.72578163e-01  5.49383124e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.35843714e-01 -2.19106487e-01 -6.24559514e-01]\n",
            "  [ 1.34560056e-01 -6.76442802e-02 -1.72566290e-01]\n",
            "  [ 5.76986767e-01  2.09084133e-01 -4.35501262e-01]\n",
            "  ...\n",
            "  [-2.54228634e-01  3.92611912e-01 -1.30357923e-01]\n",
            "  [-3.98377810e-02 -3.00832195e-01 -6.77815451e-01]\n",
            "  [-4.15511035e-02 -3.04370149e-01 -3.04405391e-01]]\n",
            "\n",
            " [[ 6.80971382e-02  5.50931508e-02  1.40975856e-01]\n",
            "  [ 5.10910193e-01  3.65094168e-01 -3.62356346e-01]\n",
            "  [-1.03425665e-01 -9.11575829e-02 -6.24765142e-02]\n",
            "  ...\n",
            "  [-1.06542558e+00 -1.94738346e+00 -2.10990504e+00]\n",
            "  [-1.09986152e+00 -2.36642915e+00 -2.80265333e+00]\n",
            "  [-1.23716466e+00 -2.19957885e+00 -2.86958167e+00]]\n",
            "\n",
            " [[-3.09786926e-01 -4.12976066e-01  3.12571819e-02]\n",
            "  [ 1.52241116e-02 -3.56903956e-01  2.47566470e-02]\n",
            "  [-3.80483536e-01 -2.06639080e-06  3.85477024e-01]\n",
            "  ...\n",
            "  [ 1.76283826e-01 -8.47830222e-02  8.53932776e-02]\n",
            "  [ 7.37511797e-02  6.83833189e-01  3.43157799e-01]\n",
            "  [-1.10296793e-01  6.77467697e-03  3.64540173e-01]]]---------------------\n",
            "\n",
            "\n",
            "(1, 250, 3)\n",
            "(250, 3)\n",
            "[3.31902638 3.68126162 2.96569592]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 2.28731421e-01 -1.77097365e-01  7.39417765e-01]\n",
            "  [-6.33344887e-01  4.30083574e-01  1.73732750e-01]\n",
            "  [ 1.63359423e-01  2.72362140e-01 -6.36618413e-02]\n",
            "  [-8.18037410e-02  2.78866742e-01  7.18088800e-03]\n",
            "  [-3.24673407e-01  1.34060331e-01 -4.98504184e-02]\n",
            "  [-5.25490062e-01 -2.38439703e-01  9.58573090e-01]\n",
            "  [-1.29577894e+00  2.45339246e-01  1.13202339e+00]\n",
            "  [-9.97644099e-01  1.64740141e-02 -4.57968381e-01]\n",
            "  [-8.06342129e-02 -7.17658924e-01 -1.14144464e+00]\n",
            "  [ 4.91099940e-02 -3.16991877e-01 -8.09662405e-01]\n",
            "  [-2.74119603e-01 -5.27230021e-01 -8.62054270e-01]\n",
            "  [-1.68436395e-01 -6.16054331e-01 -4.72436608e-01]\n",
            "  [-4.39099593e-01 -2.08700629e-01  2.08202676e-02]\n",
            "  [-1.06908941e+00 -2.46635572e-01  2.50478237e-01]\n",
            "  [-1.43908711e+00 -2.75107962e-01 -3.61855090e-01]\n",
            "  [-1.34778886e+00 -1.45364948e+00 -1.02755781e+00]\n",
            "  [-1.02632990e+00 -2.24413311e+00 -1.41554409e+00]\n",
            "  [-7.91899346e-01 -1.07795125e+00 -1.52609209e+00]\n",
            "  [-8.07040636e-01 -6.31490850e-01 -1.36985637e+00]\n",
            "  [-5.67171002e-01 -1.68969114e+00 -1.24754605e+00]\n",
            "  [-5.24948252e-01 -1.43694116e+00 -1.81912335e+00]\n",
            "  [-8.16816224e-01 -1.67231442e+00 -1.97384433e+00]\n",
            "  [-9.57502575e-01 -2.18705346e+00 -1.56047601e+00]\n",
            "  [-6.11267400e-01 -9.53144333e-01 -1.55491104e+00]\n",
            "  [-2.75949361e-02 -1.59294128e+00 -1.25465349e+00]\n",
            "  [ 1.16060446e-01 -1.77003434e+00 -9.58055662e-01]\n",
            "  [-2.21168533e-02 -8.09930740e-01 -4.56556598e-01]\n",
            "  [-5.56015910e-01 -1.73243737e-01  4.90190940e-01]\n",
            "  [-1.52014703e+00 -1.49813604e-01  7.78602791e-01]\n",
            "  [-1.42030303e+00 -9.11136043e-01  5.48043528e-01]\n",
            "  [-1.05418313e+00  2.92516264e-01  1.86656275e-01]\n",
            "  [-7.77345723e-01 -5.45805433e-01 -1.22320330e-01]\n",
            "  [ 2.67327868e-01 -7.77446175e-01 -2.29133388e-02]\n",
            "  [ 1.33854724e+00 -1.99532490e-01  6.42701323e-01]\n",
            "  [ 9.66286967e-01  9.73943949e-01  1.35176901e+00]\n",
            "  [ 3.57458817e-01  1.87328055e+00  2.19364244e+00]\n",
            "  [ 2.71142617e-01  1.06292360e+00  2.34976532e+00]\n",
            "  [ 6.06667067e-01  2.11589205e+00  1.26137508e+00]\n",
            "  [ 1.59370397e+00  7.58166685e-01  9.97970889e-01]\n",
            "  [ 2.42028305e+00  1.35538627e+00  1.76159968e+00]\n",
            "  [ 1.91403596e+00  2.79743625e+00  1.68041085e+00]\n",
            "  [ 9.98160830e-01  1.62957750e+00  1.36267096e+00]\n",
            "  [ 2.46530869e-01  1.46053995e+00  1.24131244e+00]\n",
            "  [-1.97725403e-01  1.81599337e+00  7.71019043e-01]\n",
            "  [-1.38550229e-02  4.25668680e-01  2.54504333e-01]\n",
            "  [ 7.69375338e-01 -2.94624330e-02  1.86485878e-01]\n",
            "  [ 8.84020839e-01  8.39968488e-01 -1.71963474e-01]\n",
            "  [ 1.33124090e-01  6.94620135e-01  4.71523216e-01]\n",
            "  [-6.33505193e-01  7.60146352e-01  1.44231917e+00]\n",
            "  [-4.32623031e-01  9.20721632e-01  1.15456221e+00]\n",
            "  [ 1.04622661e-01  2.57603616e-01  8.44728489e-01]\n",
            "  [ 5.33806640e-01  4.08338225e-01  1.54360537e+00]\n",
            "  [ 6.07947535e-01  1.76095530e+00  1.63143811e+00]\n",
            "  [ 5.64813494e-01  1.56202654e+00  1.04556008e+00]\n",
            "  [ 3.83046015e-01  1.14301290e+00  1.03326551e+00]\n",
            "  [ 4.59728178e-02  1.16262443e+00  1.07596201e+00]\n",
            "  [ 9.22450583e-02  1.00021276e+00  9.89460300e-02]\n",
            "  [ 7.56811585e-01 -8.87035217e-02 -1.09308904e+00]\n",
            "  [ 9.54345417e-01  3.34337273e-01 -1.45182963e+00]\n",
            "  [-6.27338312e-02 -8.86665623e-01 -9.82873451e-01]\n",
            "  [-3.85443416e-01 -5.24954974e-01 -5.83589629e-01]\n",
            "  [ 1.11746092e-01 -1.09290627e-01 -5.77351906e-01]\n",
            "  [-5.10670899e-02 -6.99135357e-01 -9.26431935e-01]\n",
            "  [-3.95578675e-01 -8.88409388e-01 -1.04253730e+00]\n",
            "  [-4.84154025e-01 -6.69019601e-01 -2.89050847e-01]\n",
            "  [-1.20519474e-01 -9.41388504e-02  2.39318957e-01]\n",
            "  [ 6.81498868e-02  2.37212438e-03  2.44750079e-01]\n",
            "  [ 3.99459177e-01  3.12541504e-01  5.94071520e-01]\n",
            "  [ 6.88657444e-01  5.75312800e-01  5.93747487e-01]\n",
            "  [ 4.86273042e-01  7.36265196e-01  1.50874556e-01]\n",
            "  [ 1.34903754e-01  8.81664671e-01  5.09290991e-01]\n",
            "  [-3.47936893e-01  9.42593290e-01  1.32146322e+00]\n",
            "  [-6.30543179e-01  5.89351890e-01  1.43015534e+00]\n",
            "  [-9.49144483e-02  3.58833122e-01  1.48683244e+00]\n",
            "  [ 3.34518589e-01  9.54684782e-01  2.03151314e+00]\n",
            "  [ 2.34412549e-01  1.81575143e+00  1.46318429e+00]\n",
            "  [-4.04628435e-01  1.30706329e+00  7.26164760e-01]\n",
            "  [-5.26328869e-01 -2.17391904e-01  1.37115845e+00]\n",
            "  [ 7.18821772e-01  1.03975723e+00  1.39486588e+00]\n",
            "  [ 1.59463990e+00  1.48392056e+00  1.29975192e+00]\n",
            "  [ 6.91203750e-01  1.76047259e+00  2.21181061e+00]\n",
            "  [-5.21474950e-03  2.33691694e+00  2.36922871e+00]\n",
            "  [-4.16846397e-01  1.64274595e+00  1.77522920e+00]\n",
            "  [-7.65144693e-01  6.01867156e-01  1.59581449e+00]\n",
            "  [-5.52919486e-03  2.17401759e-01  1.25621349e+00]\n",
            "  [ 1.13886337e+00  1.01281589e+00  8.78993799e-01]\n",
            "  [ 1.57110452e+00  1.60568150e+00  7.85783818e-01]\n",
            "  [ 1.32854169e+00  1.29702089e+00  7.48623941e-01]\n",
            "  [ 8.86963404e-01  1.03499482e+00  5.83427051e-01]\n",
            "  [ 1.27605072e+00  9.36201953e-01  3.06043508e-02]\n",
            "  [ 1.99901938e+00  1.16825667e+00 -5.97497524e-02]\n",
            "  [ 1.90644317e+00  6.50925996e-01 -2.20281960e-01]\n",
            "  [ 1.76707231e+00  1.05283215e+00 -8.34654347e-01]\n",
            "  [ 1.95354407e+00  3.43569407e-01 -7.19306836e-01]\n",
            "  [ 1.93440512e+00  3.88942785e-01 -4.32501651e-01]\n",
            "  [ 1.88557611e+00  1.14658719e+00  6.58898495e-02]\n",
            "  [ 1.96144280e+00  8.60742442e-01  3.71787910e-01]\n",
            "  [ 1.40720564e+00  1.63065403e+00  5.85661386e-03]\n",
            "  [ 9.10234694e-01  1.06264912e+00 -8.67146759e-01]\n",
            "  [ 6.72635758e-01 -1.01870979e-01 -1.16024029e+00]\n",
            "  [ 4.86285375e-01 -3.46180610e-01 -9.71191086e-01]\n",
            "  [ 2.28618636e-02 -1.46210888e-01 -9.05589979e-01]\n",
            "  [ 1.45891827e-01 -2.26239843e-01 -1.21588586e+00]\n",
            "  [ 4.57947993e-01 -1.11343348e+00 -1.46312870e+00]\n",
            "  [ 4.23762732e-01 -2.72857948e-01 -1.66309127e+00]\n",
            "  [ 6.99626127e-01 -5.40220604e-01 -1.27916222e+00]\n",
            "  [ 7.79693718e-01 -6.55836308e-02 -5.59763987e-01]\n",
            "  [ 4.65431265e-01 -1.59859104e-01 -9.83987208e-02]\n",
            "  [-1.17238718e-01  4.04660230e-01 -3.56238441e-01]\n",
            "  [-1.31759058e+00 -8.56991516e-01 -3.15543608e-01]\n",
            "  [-1.40531022e+00 -1.65790441e+00 -5.31251306e-02]\n",
            "  [-8.04401419e-01 -1.34660117e+00 -5.79601502e-03]\n",
            "  [-9.58106988e-01 -1.21185723e-01  2.25658730e-01]\n",
            "  [-1.10194180e+00 -1.22799788e+00  1.03050296e-01]\n",
            "  [-1.12027384e+00 -5.16346656e-01 -9.70997175e-01]\n",
            "  [-1.31036201e+00 -1.04691444e+00 -1.14303893e+00]\n",
            "  [-1.00125963e+00 -2.26465986e+00 -5.33740876e-01]\n",
            "  [-1.43189716e-01 -1.05943344e+00 -7.75244417e-01]\n",
            "  [ 4.18657751e-01 -1.12222899e+00 -6.57298300e-01]\n",
            "  [ 1.84424874e-01 -1.04016785e+00 -4.85844935e-01]\n",
            "  [-4.46594192e-01 -3.98707913e-01 -9.55455979e-01]\n",
            "  [-5.24868358e-01 -1.04537638e+00 -1.68901378e+00]\n",
            "  [-1.10161883e+00 -1.52771100e+00 -1.78965819e+00]\n",
            "  [-1.88162223e+00 -1.51117348e+00 -1.48431115e+00]\n",
            "  [-1.44661998e+00 -2.23679933e+00 -9.94804831e-01]\n",
            "  [-2.98307610e-01 -8.44614026e-01 -9.49546385e-01]\n",
            "  [ 2.03901684e-02 -1.63125666e+00 -6.57272517e-01]\n",
            "  [ 4.57542740e-01 -2.72648119e-01 -1.02945996e-01]\n",
            "  [ 6.64413261e-01  9.49346831e-02  8.82594581e-01]\n",
            "  [ 6.44984059e-01 -8.77864746e-02  1.26609041e+00]\n",
            "  [ 7.68418512e-01  7.75342777e-01  5.01735539e-01]\n",
            "  [ 7.30317603e-01  8.12561202e-01 -4.06769978e-01]\n",
            "  [ 6.80128022e-01  5.86497590e-01 -1.03495514e+00]\n",
            "  [ 9.21266799e-01 -4.49297762e-01 -1.26729424e+00]\n",
            "  [ 1.36728641e+00 -7.42019331e-01 -7.83172835e-01]\n",
            "  [ 1.33079737e+00  8.17654706e-01 -8.15422042e-01]\n",
            "  [ 8.45635563e-01  6.32064016e-01 -1.60972481e+00]\n",
            "  [ 8.84163134e-01 -1.03565253e+00 -1.94393898e+00]\n",
            "  [ 1.00476429e+00 -5.09542806e-01 -1.75177104e+00]\n",
            "  [ 9.83665947e-01  2.88285426e-01 -1.31927451e+00]\n",
            "  [ 8.95693644e-01 -6.06275356e-01 -7.25401175e-01]\n",
            "  [ 4.47108331e-01  4.37852742e-01 -4.25232309e-01]\n",
            "  [-1.49243200e-01  8.79277479e-01 -5.21166719e-01]\n",
            "  [ 8.17945271e-03 -8.47097605e-02 -1.87738757e-01]\n",
            "  [ 7.51525354e-01  7.12892433e-01 -2.75543142e-01]\n",
            "  [ 9.69985306e-01  1.11173466e+00 -5.79436817e-01]\n",
            "  [ 1.68819842e+00  8.26556390e-01  7.09228652e-02]\n",
            "  [ 2.49576819e+00  1.33726985e+00  1.04479230e+00]\n",
            "  [ 2.19339669e+00  1.68699051e+00  1.56730169e+00]\n",
            "  [ 1.37539323e+00  2.27005476e+00  1.49273665e+00]\n",
            "  [ 1.29230761e+00  1.10759179e+00  1.54073385e+00]\n",
            "  [ 1.53230023e+00  2.06550401e+00  1.02886231e+00]\n",
            "  [ 1.21231114e+00  2.02245918e+00  6.32560869e-01]\n",
            "  [ 7.19455185e-01  1.20138185e+00  1.01168895e+00]\n",
            "  [ 4.38662820e-01  7.50285884e-01  1.02931660e+00]\n",
            "  [ 2.62793766e-01  1.12247471e+00  2.12635003e-01]\n",
            "  [ 4.93844705e-01  1.74742940e-02 -8.74643937e-02]\n",
            "  [ 7.15640239e-01 -3.36258943e-01  2.53065871e-02]\n",
            "  [-2.90635073e-02  6.58265002e-01 -8.42044946e-02]\n",
            "  [-1.45404311e+00 -1.71226987e-01  5.20986458e-01]\n",
            "  [-2.52553057e+00 -5.18459470e-01  1.42758424e+00]\n",
            "  [-2.88749151e+00 -1.40998250e-01  9.81472208e-01]\n",
            "  [-2.78401974e+00 -1.29602039e+00  2.38256807e-01]\n",
            "  [-2.45993724e+00 -1.08329909e+00  4.87439478e-01]\n",
            "  [-2.22586496e+00 -9.38399757e-01  1.23184871e+00]\n",
            "  [-2.22172330e+00 -6.93294260e-01  1.64376399e+00]\n",
            "  [-1.82526223e+00 -2.59695955e-01  1.88102827e+00]\n",
            "  [-9.51584459e-01 -9.56289568e-01  1.02559429e+00]\n",
            "  [-7.37440910e-01 -2.09663612e-01  1.40369135e-02]\n",
            "  [-3.90738528e-01 -5.29397196e-01  3.53724329e-02]\n",
            "  [-6.74998175e-02 -3.40594210e-02  7.49171096e-01]\n",
            "  [-4.31399934e-01  1.17144412e-01  1.58671989e+00]\n",
            "  [-8.18733649e-01  6.29985640e-02  1.58261867e+00]\n",
            "  [-9.59634944e-01  5.11163728e-01  5.10584650e-01]\n",
            "  [-8.36528014e-01 -9.18669436e-01  3.03385758e-01]\n",
            "  [-2.48655818e-01 -1.25790887e-01  1.28154881e+00]\n",
            "  [ 4.50190155e-01  1.01829888e+00  1.52996404e+00]\n",
            "  [ 2.48048285e-01  1.31055916e+00  8.45206985e-01]\n",
            "  [ 5.93694373e-02  1.15856346e+00  6.80437246e-01]\n",
            "  [ 3.73244910e-01  4.78682555e-01  8.42458379e-01]\n",
            "  [ 7.45274556e-01  6.24450424e-01  7.61649284e-01]\n",
            "  [ 7.17878041e-01  1.05447419e+00  5.58511292e-01]\n",
            "  [ 4.07440759e-01  8.58492976e-01 -3.36460204e-01]\n",
            "  [ 6.59144083e-01 -5.27698560e-01 -7.17986803e-01]\n",
            "  [ 6.95000960e-01  7.33948618e-02  8.28052984e-02]\n",
            "  [-2.99759038e-02  8.98079327e-01  2.90706190e-01]\n",
            "  [-5.79682720e-01 -1.52745097e-01  6.55651055e-02]\n",
            "  [-1.09910480e+00 -4.38745325e-01 -1.49958390e-01]\n",
            "  [-1.16077387e+00 -7.00080080e-01 -7.13565687e-01]\n",
            "  [-3.50263438e-01 -1.01329732e+00 -1.04610344e+00]\n",
            "  [ 7.65180144e-01 -6.76220854e-01 -1.10657216e+00]\n",
            "  [ 1.05272174e+00 -1.44507293e-01 -7.21287533e-01]\n",
            "  [ 9.64980550e-01 -4.94980691e-03 -5.99797089e-01]\n",
            "  [ 8.22207048e-01 -8.89670333e-02 -4.78795250e-01]\n",
            "  [ 7.27283205e-01 -3.84764128e-02 -1.71183912e-01]\n",
            "  [ 3.87485549e-01  6.76325073e-01  6.18819318e-02]\n",
            "  [ 1.63139547e-02  4.21334528e-01 -1.38145515e-02]\n",
            "  [-3.36925271e-01 -1.10666894e-01  2.48101076e-01]\n",
            "  [-8.07360486e-01  2.89978825e-01  3.34482657e-01]\n",
            "  [-9.89482429e-01 -1.26315805e-01 -2.20101941e-01]\n",
            "  [-7.22861693e-01 -8.59310146e-01 -7.10273988e-01]\n",
            "  [-4.37836629e-01 -5.01395258e-01 -4.96520543e-01]\n",
            "  [-3.91178257e-01 -2.88210743e-01 -1.71526012e-01]\n",
            "  [-5.51863984e-01 -5.72424826e-01 -8.41449205e-02]\n",
            "  [-7.59369004e-01 -3.12638582e-01 -1.08877416e-01]\n",
            "  [-7.35566975e-01 -1.22511007e+00 -3.26786951e-01]\n",
            "  [-1.24514695e+00 -2.01981680e-01 -7.24230650e-01]\n",
            "  [-1.71476406e+00 -1.07996832e+00 -3.17681675e-01]\n",
            "  [-1.21670593e+00 -1.00503740e+00 -2.15203766e-01]\n",
            "  [-5.22806856e-01 -5.80620044e-01 -4.89536438e-01]\n",
            "  [-7.75496870e-01 -8.75000030e-01 -7.52017011e-01]\n",
            "  [-1.49524797e+00 -1.23291836e+00 -8.54276325e-01]\n",
            "  [-8.96971691e-01 -1.83586735e+00 -7.62484586e-01]\n",
            "  [ 1.01250332e-01 -1.06448918e+00 -2.93480044e-01]\n",
            "  [ 1.12186274e-01 -3.32969755e-01  1.43363742e-01]\n",
            "  [ 4.17253223e-01  1.56632002e-01  7.59944587e-01]\n",
            "  [ 6.84578125e-01  5.17608261e-01  4.14317316e-01]\n",
            "  [-4.96762214e-01  1.06097634e+00 -5.80516469e-01]\n",
            "  [-1.46914158e+00 -1.04518439e+00 -4.40974649e-01]\n",
            "  [-1.55317268e+00 -7.77201863e-01 -4.38309647e-01]\n",
            "  [-1.47417532e+00 -1.16015027e+00 -6.24656737e-01]\n",
            "  [-2.03237638e+00 -1.03103325e+00 -1.97072106e-01]\n",
            "  [-2.30812688e+00 -1.23662307e+00 -1.56069611e-01]\n",
            "  [-2.02423238e+00 -1.91854964e+00 -1.59619095e-01]\n",
            "  [-1.41974006e+00 -1.45264314e+00  6.76457869e-01]\n",
            "  [-8.13152818e-01 -5.87243258e-01  1.22675897e+00]\n",
            "  [-1.72027812e-01  2.44334862e-01  6.90525747e-01]\n",
            "  [ 2.30539777e-02  1.93755776e-01 -2.16227114e-01]\n",
            "  [-3.71466141e-01 -1.26813618e-01 -4.38598312e-01]\n",
            "  [-6.45639171e-01 -3.33890407e-01 -4.15461965e-01]\n",
            "  [-1.86868507e-01 -1.14121041e+00  1.41272575e-01]\n",
            "  [ 3.94995113e-01 -5.90158068e-02  1.24247205e+00]\n",
            "  [ 3.56792574e-01  6.31316061e-01  1.41122487e+00]\n",
            "  [ 6.05100380e-01  8.87826953e-01  3.18839377e-01]\n",
            "  [ 1.14686692e+00  2.49355955e-01 -1.28824538e-01]\n",
            "  [ 1.53981697e+00  1.16846051e+00  8.03616292e-02]\n",
            "  [ 1.45809503e+00  1.30006947e+00 -1.55345362e-01]\n",
            "  [ 1.13160908e+00  2.75472213e-01 -2.45072972e-01]\n",
            "  [ 5.73899656e-01  8.82478839e-01 -4.92497763e-01]\n",
            "  [-6.48481438e-02 -5.22423512e-01 -1.11471997e+00]\n",
            "  [-4.46736495e-01 -2.42503669e-01 -6.89668104e-01]\n",
            "  [-5.92058437e-01  4.48996034e-03 -5.84451786e-01]\n",
            "  [-6.82911136e-01 -4.22982185e-02 -1.86674245e+00]\n",
            "  [-3.31356968e-01 -1.59069104e+00 -3.07426065e+00]\n",
            "  [-1.57100379e-01 -2.34985297e+00 -3.05184616e+00]\n",
            "  [-2.57729151e-01 -2.35525941e+00 -2.09489505e+00]\n",
            "  [ 2.68173509e-01 -1.87487267e+00 -1.14357260e+00]\n",
            "  [ 8.40720214e-01 -1.08172585e+00 -4.86533191e-01]\n",
            "  [ 1.15862908e+00  3.11514604e-01 -5.20683778e-01]\n",
            "  [ 1.21954199e+00  5.62354654e-01 -1.02910207e+00]]]---------------------\n",
            "\n",
            "\n",
            "(20, 250, 3)\n",
            "(9, 250, 3)\n",
            "(1, 250, 3)\n",
            "yc.shape: (1, 250, 3)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.83328916e-16 -8.65973959e-17  1.17239551e-16]\n",
            "std_dev yc: [1. 1. 1.]\n",
            "\n",
            "context_points 6\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 9\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch3_sub_SNR_inf_1/epoch=54-val/norm_mse=0.12.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 10\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [40, 58, 189, 242, 17, 108, 32, 114, 234, 235, 84, 136, 97, 179, 129, 220, 187, 113, 44, 205, 100, 67, 105, 163, 138, 128, 26, 202, 157, 22, 76, 208, 68, 56, 153, 82, 190, 112, 73, 131, 110, 36, 195, 98, 104, 227, 123, 184, 147, 188, 206, 146, 6, 229, 177, 59, 51, 241, 170, 134, 16, 64, 31, 223, 91, 161, 211, 168, 109, 130, 24, 55, 99, 191, 90, 47, 139, 164, 27, 228, 62, 57, 37, 171, 120, 199, 209, 1, 49, 7, 28, 194, 144, 8, 11, 81, 203, 72, 169, 222, 19, 87, 148, 125, 124, 175, 212, 63, 89, 14, 118, 15, 243, 127, 217, 107, 43, 60, 182, 116, 196, 132, 13, 158, 141, 42, 215, 103, 198, 18, 159, 78, 231, 61, 122, 178, 75, 219, 192, 92, 180, 152, 41, 151, 181, 237, 149, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 117, 236, 201, 216, 145, 93, 221, 142, 66, 52, 174, 10, 4, 197, 39, 186, 88, 165, 200, 133, 167, 34, 65, 29, 115, 70, 5, 20, 160, 101, 121, 204, 46, 176, 71, 77, 50, 166, 155, 30, 225, 25, 156, 135, 172, 213, 240, 183, 218, 54, 239, 80, 238, 173, 102, 3, 33, 185, 233, 21, 162, 140, 48, 106, 230, 224, 210, 207, 12, 119, 214, 79, 83, 126, 9, 226, 0, 193, 154, 143, 232]\n",
            "self.series.num_trials(split): 9\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch3_sub_SNR_inf_1/epoch=54-val/norm_mse=0.12.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch3_sub_SNR_inf_1/epoch=54-val/norm_mse=0.12.ckpt\n",
            "Testing DataLoader 0: 100% 18/18 [00:00<00:00, 26.22it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc            0.9999746680259705\n",
            "     test/class_loss        0.04232928156852722\n",
            "   test/forecast_loss       0.12140898406505585\n",
            "        test/loss           0.1214132234454155\n",
            "        test/mae            0.27684831619262695\n",
            "        test/mape            2.18997859954834\n",
            "        test/mse            0.12140898406505585\n",
            "      test/norm_mae         0.27684831619262695\n",
            "      test/norm_mse         0.12140898406505585\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.3504909873008728\n",
            "       test/smape           0.6181790828704834\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "args.run_name: generated_gc_SNR_6\n",
            "File list:  ['./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch4_sub_SNR_inf_1/epoch=55-val/norm_mse=0.26.ckpt']\n",
            "0\n",
            "checkpoint_path: ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch4_sub_SNR_inf_1/epoch=55-val/norm_mse=0.26.ckpt\n",
            "(1, 250, 4)\n",
            "yc.shape: (1, 250, 4)\n",
            "\n",
            "\n",
            "-----------Statistical properties before scaling------------\n",
            "mean yc: [ 0.27998699 -0.34327452 -0.6696092   0.045271  ]\n",
            "std_dev yc: [3.31902638 3.68126162 2.96569592 1.25874214]\n",
            "\n",
            "(20, 250, 4)\n",
            "(250, 80)\n",
            "[3.05155337 3.26340089 2.98000027 1.19964105 3.0214477  3.42196559\n",
            " 2.89621285 1.24797031 2.99083682 3.28383927 2.77387769 1.2073929\n",
            " 2.87150134 3.30356706 2.82163991 1.20793307 2.80944211 3.25481015\n",
            " 2.87983581 1.16455849 2.79110019 3.05185588 3.02651184 1.29143061\n",
            " 2.93848899 3.29083408 3.09068162 1.25768193 3.07804889 3.39303804\n",
            " 3.02101036 1.12140427 3.0943009  3.23990594 2.94998341 1.19456292\n",
            " 3.07192807 3.21904584 2.89059317 1.14440419 3.02060323 3.23639683\n",
            " 2.83045268 1.15886466 2.93851658 3.25178862 2.97632638 1.18174412\n",
            " 2.87424645 3.35919122 2.85171736 1.19688428 3.02498457 3.13503403\n",
            " 2.64694571 1.11332737 2.97660322 3.09262181 2.59256666 1.20629434\n",
            " 2.96610999 2.96764441 2.60201857 1.21344584 3.00647783 3.14109851\n",
            " 2.82584695 1.24668411 2.97822199 3.08363261 3.09621694 1.20452045\n",
            " 3.06566746 3.34721324 2.99076202 1.18044312 3.18324482 3.26245521\n",
            " 2.94772642 1.14020384]\n",
            "\n",
            "\n",
            "---------------self._train_data [[[-1.47064781e-01 -8.07121154e-02 -5.90319844e-02  5.68028385e-02]\n",
            "  [ 1.27861967e-01 -2.86317142e-01 -4.18338073e-02  2.85164633e-01]\n",
            "  [ 3.64936877e-01 -3.41791937e-03  3.64014397e-01  1.32194571e+00]\n",
            "  ...\n",
            "  [ 1.10051218e+00  8.72207325e-02 -1.27183346e+00  7.33844055e-01]\n",
            "  [ 1.00824230e+00 -8.10320402e-01 -7.67271681e-01 -5.54679002e-01]\n",
            "  [ 1.00983631e+00 -1.31627886e-01 -4.63278309e-01 -4.89800580e-01]]\n",
            "\n",
            " [[-2.32454054e-03 -3.38270837e-01  4.73817719e-01  2.14603356e+00]\n",
            "  [-1.88596993e-01 -3.01318705e-02 -2.15808473e-01 -1.61758912e-01]\n",
            "  [-7.99210189e-01 -1.18612991e-01  1.73977973e-01  6.00927553e-02]\n",
            "  ...\n",
            "  [ 4.62747248e-01  1.27188120e+00  8.09520283e-02  7.18048056e-01]\n",
            "  [-3.78541252e-01  5.06082111e-01 -2.60446502e-01 -2.48699626e-01]\n",
            "  [-6.44791561e-01 -6.41112363e-01 -3.69228970e-01  5.88604403e-01]]\n",
            "\n",
            " [[-3.86002373e-01 -2.91379504e-01 -3.11102679e-01 -6.86486522e-01]\n",
            "  [-2.99383267e-01  1.60973962e-01 -2.67697291e-01  4.44211553e-01]\n",
            "  [ 7.12281567e-02 -1.52005506e-01 -2.65214756e-01 -2.45358797e-01]\n",
            "  ...\n",
            "  [-1.87261195e+00 -2.21169670e+00 -8.25780843e-01  4.05698402e-01]\n",
            "  [-1.02053355e+00 -1.92752735e+00 -9.29496225e-01  3.89658951e-01]\n",
            "  [-4.93597248e-01 -9.50738313e-01 -1.31413804e+00  3.00127796e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-7.53270544e-02 -1.76809216e-01  3.37905415e-01  4.55634119e-01]\n",
            "  [-4.29709767e-02  3.88244702e-01 -3.00201720e-01  1.58539634e+00]\n",
            "  [ 6.91824908e-01  9.84258847e-02 -4.48739880e-01  8.88061901e-03]\n",
            "  ...\n",
            "  [ 6.05513237e-01  9.62302582e-01  1.08569968e+00 -1.60095404e+00]\n",
            "  [ 1.65928924e-01  2.05165878e+00  2.70972325e-01  1.07025307e-01]\n",
            "  [-1.31618693e-01 -1.16105235e-01 -3.78257984e-01 -8.26771617e-01]]\n",
            "\n",
            " [[ 3.12299388e-01 -1.93976929e-01 -9.29569747e-02 -1.09467566e+00]\n",
            "  [-6.09581019e-02 -2.55452116e-01 -2.85157975e-01 -7.05620165e-01]\n",
            "  [ 2.17973766e-01  8.18913336e-02  1.34930221e-01  8.17490106e-01]\n",
            "  ...\n",
            "  [-8.79779793e-01 -6.12780379e-01  8.10472307e-01 -1.13835551e-01]\n",
            "  [-3.78302541e-01 -4.95162242e-01  8.98401584e-01  1.02448288e+00]\n",
            "  [ 6.21921531e-01  2.05152444e-01  7.60441438e-01  9.70765169e-01]]\n",
            "\n",
            " [[ 4.15001797e-01 -2.32670152e-01 -2.27553858e-01  5.97709241e-01]\n",
            "  [ 5.88132504e-01  4.27180906e-02  2.35562766e-01 -5.58536578e-01]\n",
            "  [-3.73112428e-01  2.01009221e-01  2.13129665e-01 -6.21270159e-01]\n",
            "  ...\n",
            "  [ 1.51210856e+00 -1.30546560e+00 -1.17086461e+00  2.28172536e+00]\n",
            "  [ 1.03261009e+00 -7.93889382e-01 -1.33598530e+00 -2.75687619e-01]\n",
            "  [ 8.12416900e-01 -1.90724702e-01 -1.54531769e+00  2.54769912e-01]]]---------------------\n",
            "\n",
            "\n",
            "(9, 250, 4)\n",
            "(250, 36)\n",
            "[3.24312957 3.43463074 3.1543322  1.20076043 3.2437275  3.43196594\n",
            " 3.15509591 1.15861721 3.20387068 3.30042661 3.24006534 1.16359426\n",
            " 3.23875997 3.41930561 3.32111364 1.27295341 3.19143936 3.42112254\n",
            " 3.17028077 1.21289459 3.26098638 3.29155865 3.1641306  1.18318647\n",
            " 3.30045875 3.36182518 3.05584819 1.10427117 3.13039169 3.36281684\n",
            " 2.79976511 1.30709613 3.11079628 3.22350716 2.96168676 1.3037222 ]\n",
            "\n",
            "\n",
            "---------------self._val_data [[[ 1.15351807e-01 -1.25503490e-01 -8.07295074e-02 -3.84935543e-01]\n",
            "  [ 9.16285242e-02  4.36991546e-01  1.52471155e-01  1.06801326e+00]\n",
            "  [ 1.97485528e-01 -3.66528099e-01  1.48664808e-01 -3.73076088e-02]\n",
            "  ...\n",
            "  [-1.17852523e+00 -2.71529920e+00 -1.41529740e+00  5.02663693e-02]\n",
            "  [-1.09479846e+00 -1.63633919e+00 -1.15970144e+00  2.39311301e+00]\n",
            "  [-1.39550954e+00 -1.64766468e+00 -1.22410686e+00  3.28047155e+00]]\n",
            "\n",
            " [[-5.85838132e-01 -1.71581979e-02 -1.53021282e-01  1.98465049e-01]\n",
            "  [-1.28596942e-01 -9.27868716e-02 -1.60466936e-01 -5.55115147e-01]\n",
            "  [-4.27098928e-01 -1.76086207e-01 -2.69058487e-01 -1.30603169e-01]\n",
            "  ...\n",
            "  [ 1.25482808e-01  6.12592692e-01 -2.92522342e-01  3.64362947e-01]\n",
            "  [ 3.21311371e-01  1.17375977e-01 -7.21940726e-01  6.23738342e-01]\n",
            "  [ 3.08602468e-02 -1.19832921e-01 -6.03579947e-01 -5.95822180e-01]]\n",
            "\n",
            " [[ 1.36120235e-02  2.27546789e-02  1.19860853e-02  2.08227455e+00]\n",
            "  [ 7.93534404e-03  2.15076117e-01 -3.85408832e-01  4.73203250e-01]\n",
            "  [-1.58220624e-01  1.30562029e-01  2.17857875e-01  9.67698672e-01]\n",
            "  ...\n",
            "  [-2.19894166e-01 -1.60950809e-01  9.23886625e-01 -1.11979540e+00]\n",
            "  [ 3.87428814e-01  8.85203035e-01  9.79980330e-01  1.63923033e+00]\n",
            "  [ 2.81591066e-01  6.72578163e-01  5.49383124e-01  4.12558570e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-2.35843714e-01 -2.19106487e-01 -6.24559514e-01 -1.04217201e+00]\n",
            "  [ 1.34560056e-01 -6.76442802e-02 -1.72566290e-01  6.77377271e-01]\n",
            "  [ 5.76986767e-01  2.09084133e-01 -4.35501262e-01  8.01814099e-01]\n",
            "  ...\n",
            "  [-2.54228634e-01  3.92611912e-01 -1.30357923e-01  8.00845883e-02]\n",
            "  [-3.98377810e-02 -3.00832195e-01 -6.77815451e-01  5.34368880e-03]\n",
            "  [-4.15511035e-02 -3.04370149e-01 -3.04405391e-01 -2.37398554e-01]]\n",
            "\n",
            " [[ 6.80971382e-02  5.50931508e-02  1.40975856e-01  7.97268270e-01]\n",
            "  [ 5.10910193e-01  3.65094168e-01 -3.62356346e-01 -3.82374519e-01]\n",
            "  [-1.03425665e-01 -9.11575829e-02 -6.24765142e-02 -1.52877844e-01]\n",
            "  ...\n",
            "  [-1.06542558e+00 -1.94738346e+00 -2.10990504e+00  2.46908536e-02]\n",
            "  [-1.09986152e+00 -2.36642915e+00 -2.80265333e+00  1.06499634e+00]\n",
            "  [-1.23716466e+00 -2.19957885e+00 -2.86958167e+00 -2.85301530e-01]]\n",
            "\n",
            " [[-3.09786926e-01 -4.12976066e-01  3.12571819e-02  5.77635169e-01]\n",
            "  [ 1.52241116e-02 -3.56903956e-01  2.47566470e-02 -4.16246701e-01]\n",
            "  [-3.80483536e-01 -2.06639080e-06  3.85477024e-01 -2.23188308e-01]\n",
            "  ...\n",
            "  [ 1.76283826e-01 -8.47830222e-02  8.53932776e-02 -2.87659122e-01]\n",
            "  [ 7.37511797e-02  6.83833189e-01  3.43157799e-01 -8.52821085e-01]\n",
            "  [-1.10296793e-01  6.77467697e-03  3.64540173e-01  1.31280634e-01]]]---------------------\n",
            "\n",
            "\n",
            "(1, 250, 4)\n",
            "(250, 4)\n",
            "[3.31902638 3.68126162 2.96569592 1.25874214]\n",
            "\n",
            "\n",
            "---------------self._test_data [[[ 2.28731421e-01 -1.77097365e-01  7.39417765e-01 -3.49746247e-01]\n",
            "  [-6.33344887e-01  4.30083574e-01  1.73732750e-01 -1.35308201e+00]\n",
            "  [ 1.63359423e-01  2.72362140e-01 -6.36618413e-02 -5.60562959e-01]\n",
            "  [-8.18037410e-02  2.78866742e-01  7.18088800e-03 -2.31660960e-01]\n",
            "  [-3.24673407e-01  1.34060331e-01 -4.98504184e-02 -1.33196586e+00]\n",
            "  [-5.25490062e-01 -2.38439703e-01  9.58573090e-01  7.71969040e-01]\n",
            "  [-1.29577894e+00  2.45339246e-01  1.13202339e+00 -4.14667700e-01]\n",
            "  [-9.97644099e-01  1.64740141e-02 -4.57968381e-01  1.46025400e-01]\n",
            "  [-8.06342129e-02 -7.17658924e-01 -1.14144464e+00 -6.88535572e-01]\n",
            "  [ 4.91099940e-02 -3.16991877e-01 -8.09662405e-01 -6.39822615e-02]\n",
            "  [-2.74119603e-01 -5.27230021e-01 -8.62054270e-01  6.50219626e-01]\n",
            "  [-1.68436395e-01 -6.16054331e-01 -4.72436608e-01 -6.54490973e-01]\n",
            "  [-4.39099593e-01 -2.08700629e-01  2.08202676e-02 -9.37716426e-02]\n",
            "  [-1.06908941e+00 -2.46635572e-01  2.50478237e-01 -2.53898030e-01]\n",
            "  [-1.43908711e+00 -2.75107962e-01 -3.61855090e-01  1.71069103e+00]\n",
            "  [-1.34778886e+00 -1.45364948e+00 -1.02755781e+00  1.50401095e+00]\n",
            "  [-1.02632990e+00 -2.24413311e+00 -1.41554409e+00 -1.69195001e+00]\n",
            "  [-7.91899346e-01 -1.07795125e+00 -1.52609209e+00 -1.90531358e-02]\n",
            "  [-8.07040636e-01 -6.31490850e-01 -1.36985637e+00  2.26764594e+00]\n",
            "  [-5.67171002e-01 -1.68969114e+00 -1.24754605e+00  2.55665014e-01]\n",
            "  [-5.24948252e-01 -1.43694116e+00 -1.81912335e+00  1.36156519e+00]\n",
            "  [-8.16816224e-01 -1.67231442e+00 -1.97384433e+00  2.22291231e+00]\n",
            "  [-9.57502575e-01 -2.18705346e+00 -1.56047601e+00  8.90786120e-01]\n",
            "  [-6.11267400e-01 -9.53144333e-01 -1.55491104e+00  7.51244635e-01]\n",
            "  [-2.75949361e-02 -1.59294128e+00 -1.25465349e+00  1.74446809e+00]\n",
            "  [ 1.16060446e-01 -1.77003434e+00 -9.58055662e-01 -4.78014899e-04]\n",
            "  [-2.21168533e-02 -8.09930740e-01 -4.56556598e-01  6.54701501e-01]\n",
            "  [-5.56015910e-01 -1.73243737e-01  4.90190940e-01  3.35760569e-01]\n",
            "  [-1.52014703e+00 -1.49813604e-01  7.78602791e-01  1.82024228e+00]\n",
            "  [-1.42030303e+00 -9.11136043e-01  5.48043528e-01 -1.25531493e+00]\n",
            "  [-1.05418313e+00  2.92516264e-01  1.86656275e-01  9.87519142e-01]\n",
            "  [-7.77345723e-01 -5.45805433e-01 -1.22320330e-01  3.98896267e-01]\n",
            "  [ 2.67327868e-01 -7.77446175e-01 -2.29133388e-02  1.41551869e-01]\n",
            "  [ 1.33854724e+00 -1.99532490e-01  6.42701323e-01  9.34299374e-01]\n",
            "  [ 9.66286967e-01  9.73943949e-01  1.35176901e+00 -7.39891650e-01]\n",
            "  [ 3.57458817e-01  1.87328055e+00  2.19364244e+00  7.18892616e-01]\n",
            "  [ 2.71142617e-01  1.06292360e+00  2.34976532e+00 -1.78048960e+00]\n",
            "  [ 6.06667067e-01  2.11589205e+00  1.26137508e+00 -8.31289072e-01]\n",
            "  [ 1.59370397e+00  7.58166685e-01  9.97970889e-01 -6.17575988e-01]\n",
            "  [ 2.42028305e+00  1.35538627e+00  1.76159968e+00 -3.01222621e+00]\n",
            "  [ 1.91403596e+00  2.79743625e+00  1.68041085e+00 -3.90518148e-03]\n",
            "  [ 9.98160830e-01  1.62957750e+00  1.36267096e+00  2.67605930e-01]\n",
            "  [ 2.46530869e-01  1.46053995e+00  1.24131244e+00 -2.13158344e+00]\n",
            "  [-1.97725403e-01  1.81599337e+00  7.71019043e-01 -5.34345711e-01]\n",
            "  [-1.38550229e-02  4.25668680e-01  2.54504333e-01 -1.07966395e+00]\n",
            "  [ 7.69375338e-01 -2.94624330e-02  1.86485878e-01 -1.53037455e+00]\n",
            "  [ 8.84020839e-01  8.39968488e-01 -1.71963474e-01 -1.40965726e+00]\n",
            "  [ 1.33124090e-01  6.94620135e-01  4.71523216e-01 -1.41291445e+00]\n",
            "  [-6.33505193e-01  7.60146352e-01  1.44231917e+00 -9.15169283e-01]\n",
            "  [-4.32623031e-01  9.20721632e-01  1.15456221e+00 -2.10496729e-01]\n",
            "  [ 1.04622661e-01  2.57603616e-01  8.44728489e-01  8.56048890e-01]\n",
            "  [ 5.33806640e-01  4.08338225e-01  1.54360537e+00 -1.97244939e+00]\n",
            "  [ 6.07947535e-01  1.76095530e+00  1.63143811e+00 -1.50957192e+00]\n",
            "  [ 5.64813494e-01  1.56202654e+00  1.04556008e+00  2.99879393e-01]\n",
            "  [ 3.83046015e-01  1.14301290e+00  1.03326551e+00 -4.04670561e-01]\n",
            "  [ 4.59728178e-02  1.16262443e+00  1.07596201e+00 -1.26181429e+00]\n",
            "  [ 9.22450583e-02  1.00021276e+00  9.89460300e-02 -3.99910218e-01]\n",
            "  [ 7.56811585e-01 -8.87035217e-02 -1.09308904e+00 -1.41741693e+00]\n",
            "  [ 9.54345417e-01  3.34337273e-01 -1.45182963e+00  7.32727616e-01]\n",
            "  [-6.27338312e-02 -8.86665623e-01 -9.82873451e-01  8.93819580e-02]\n",
            "  [-3.85443416e-01 -5.24954974e-01 -5.83589629e-01  2.39050973e-01]\n",
            "  [ 1.11746092e-01 -1.09290627e-01 -5.77351906e-01  1.57733880e+00]\n",
            "  [-5.10670899e-02 -6.99135357e-01 -9.26431935e-01  5.47708064e-01]\n",
            "  [-3.95578675e-01 -8.88409388e-01 -1.04253730e+00  9.53549970e-01]\n",
            "  [-4.84154025e-01 -6.69019601e-01 -2.89050847e-01  4.50092894e-01]\n",
            "  [-1.20519474e-01 -9.41388504e-02  2.39318957e-01  1.11535655e+00]\n",
            "  [ 6.81498868e-02  2.37212438e-03  2.44750079e-01 -5.23359049e-01]\n",
            "  [ 3.99459177e-01  3.12541504e-01  5.94071520e-01  2.72589776e-01]\n",
            "  [ 6.88657444e-01  5.75312800e-01  5.93747487e-01 -4.08245389e-02]\n",
            "  [ 4.86273042e-01  7.36265196e-01  1.50874556e-01 -1.05583229e+00]\n",
            "  [ 1.34903754e-01  8.81664671e-01  5.09290991e-01 -1.60745214e+00]\n",
            "  [-3.47936893e-01  9.42593290e-01  1.32146322e+00 -2.90235037e-01]\n",
            "  [-6.30543179e-01  5.89351890e-01  1.43015534e+00  2.61547361e-01]\n",
            "  [-9.49144483e-02  3.58833122e-01  1.48683244e+00 -1.28556360e+00]\n",
            "  [ 3.34518589e-01  9.54684782e-01  2.03151314e+00 -4.59396777e-01]\n",
            "  [ 2.34412549e-01  1.81575143e+00  1.46318429e+00  6.75741653e-02]\n",
            "  [-4.04628435e-01  1.30706329e+00  7.26164760e-01 -1.15612840e-01]\n",
            "  [-5.26328869e-01 -2.17391904e-01  1.37115845e+00 -2.90641275e+00]\n",
            "  [ 7.18821772e-01  1.03975723e+00  1.39486588e+00  1.03618330e-01]\n",
            "  [ 1.59463990e+00  1.48392056e+00  1.29975192e+00 -3.65938381e-01]\n",
            "  [ 6.91203750e-01  1.76047259e+00  2.21181061e+00 -1.25818868e+00]\n",
            "  [-5.21474950e-03  2.33691694e+00  2.36922871e+00 -4.75558954e-01]\n",
            "  [-4.16846397e-01  1.64274595e+00  1.77522920e+00 -6.46903035e-01]\n",
            "  [-7.65144693e-01  6.01867156e-01  1.59581449e+00 -3.30187810e-01]\n",
            "  [-5.52919486e-03  2.17401759e-01  1.25621349e+00 -7.64838812e-01]\n",
            "  [ 1.13886337e+00  1.01281589e+00  8.78993799e-01 -7.66642065e-01]\n",
            "  [ 1.57110452e+00  1.60568150e+00  7.85783818e-01  1.02857703e+00]\n",
            "  [ 1.32854169e+00  1.29702089e+00  7.48623941e-01 -4.38404211e-01]\n",
            "  [ 8.86963404e-01  1.03499482e+00  5.83427051e-01 -2.38778184e-01]\n",
            "  [ 1.27605072e+00  9.36201953e-01  3.06043508e-02 -1.03152465e+00]\n",
            "  [ 1.99901938e+00  1.16825667e+00 -5.97497524e-02  1.05419002e+00]\n",
            "  [ 1.90644317e+00  6.50925996e-01 -2.20281960e-01 -6.14725283e-01]\n",
            "  [ 1.76707231e+00  1.05283215e+00 -8.34654347e-01  4.32669293e-01]\n",
            "  [ 1.95354407e+00  3.43569407e-01 -7.19306836e-01 -3.36806555e-01]\n",
            "  [ 1.93440512e+00  3.88942785e-01 -4.32501651e-01 -1.22058389e+00]\n",
            "  [ 1.88557611e+00  1.14658719e+00  6.58898495e-02 -4.34583915e-01]\n",
            "  [ 1.96144280e+00  8.60742442e-01  3.71787910e-01 -9.26266803e-01]\n",
            "  [ 1.40720564e+00  1.63065403e+00  5.85661386e-03 -1.04567098e+00]\n",
            "  [ 9.10234694e-01  1.06264912e+00 -8.67146759e-01 -1.11228289e-01]\n",
            "  [ 6.72635758e-01 -1.01870979e-01 -1.16024029e+00 -3.11091407e-01]\n",
            "  [ 4.86285375e-01 -3.46180610e-01 -9.71191086e-01 -6.73016114e-01]\n",
            "  [ 2.28618636e-02 -1.46210888e-01 -9.05589979e-01  3.82256014e-01]\n",
            "  [ 1.45891827e-01 -2.26239843e-01 -1.21588586e+00  1.05443420e+00]\n",
            "  [ 4.57947993e-01 -1.11343348e+00 -1.46312870e+00 -8.07313259e-01]\n",
            "  [ 4.23762732e-01 -2.72857948e-01 -1.66309127e+00  2.81057883e-01]\n",
            "  [ 6.99626127e-01 -5.40220604e-01 -1.27916222e+00  6.63523266e-01]\n",
            "  [ 7.79693718e-01 -6.55836308e-02 -5.59763987e-01  1.72707514e+00]\n",
            "  [ 4.65431265e-01 -1.59859104e-01 -9.83987208e-02 -2.85920130e-01]\n",
            "  [-1.17238718e-01  4.04660230e-01 -3.56238441e-01  4.51978983e-01]\n",
            "  [-1.31759058e+00 -8.56991516e-01 -3.15543608e-01  1.07375605e+00]\n",
            "  [-1.40531022e+00 -1.65790441e+00 -5.31251306e-02  8.79210992e-01]\n",
            "  [-8.04401419e-01 -1.34660117e+00 -5.79601502e-03  1.01897225e+00]\n",
            "  [-9.58106988e-01 -1.21185723e-01  2.25658730e-01  2.96870444e+00]\n",
            "  [-1.10194180e+00 -1.22799788e+00  1.03050296e-01  4.37895972e-01]\n",
            "  [-1.12027384e+00 -5.16346656e-01 -9.70997175e-01  8.31302119e-01]\n",
            "  [-1.31036201e+00 -1.04691444e+00 -1.14303893e+00  1.99025029e+00]\n",
            "  [-1.00125963e+00 -2.26465986e+00 -5.33740876e-01  5.96500307e-01]\n",
            "  [-1.43189716e-01 -1.05943344e+00 -7.75244417e-01  5.47618529e-01]\n",
            "  [ 4.18657751e-01 -1.12222899e+00 -6.57298300e-01  2.07263766e+00]\n",
            "  [ 1.84424874e-01 -1.04016785e+00 -4.85844935e-01  4.38121853e-01]\n",
            "  [-4.46594192e-01 -3.98707913e-01 -9.55455979e-01  8.45625228e-01]\n",
            "  [-5.24868358e-01 -1.04537638e+00 -1.68901378e+00  1.23619345e+00]\n",
            "  [-1.10161883e+00 -1.52771100e+00 -1.78965819e+00 -5.04897074e-01]\n",
            "  [-1.88162223e+00 -1.51117348e+00 -1.48431115e+00  6.64714730e-01]\n",
            "  [-1.44661998e+00 -2.23679933e+00 -9.94804831e-01  2.05164745e-02]\n",
            "  [-2.98307610e-01 -8.44614026e-01 -9.49546385e-01  1.32766849e+00]\n",
            "  [ 2.03901684e-02 -1.63125666e+00 -6.57272517e-01  1.99888768e-01]\n",
            "  [ 4.57542740e-01 -2.72648119e-01 -1.02945996e-01  1.19089479e+00]\n",
            "  [ 6.64413261e-01  9.49346831e-02  8.82594581e-01  1.24629525e+00]\n",
            "  [ 6.44984059e-01 -8.77864746e-02  1.26609041e+00 -6.70973258e-02]\n",
            "  [ 7.68418512e-01  7.75342777e-01  5.01735539e-01 -1.30405691e+00]\n",
            "  [ 7.30317603e-01  8.12561202e-01 -4.06769978e-01 -1.38287848e+00]\n",
            "  [ 6.80128022e-01  5.86497590e-01 -1.03495514e+00  1.14928164e+00]\n",
            "  [ 9.21266799e-01 -4.49297762e-01 -1.26729424e+00 -6.33523064e-02]\n",
            "  [ 1.36728641e+00 -7.42019331e-01 -7.83172835e-01 -9.74850076e-01]\n",
            "  [ 1.33079737e+00  8.17654706e-01 -8.15422042e-01 -1.17111067e+00]\n",
            "  [ 8.45635563e-01  6.32064016e-01 -1.60972481e+00  8.05194221e-01]\n",
            "  [ 8.84163134e-01 -1.03565253e+00 -1.94393898e+00 -4.85466000e-01]\n",
            "  [ 1.00476429e+00 -5.09542806e-01 -1.75177104e+00 -1.17674786e+00]\n",
            "  [ 9.83665947e-01  2.88285426e-01 -1.31927451e+00  1.35225324e+00]\n",
            "  [ 8.95693644e-01 -6.06275356e-01 -7.25401175e-01 -4.42807554e-01]\n",
            "  [ 4.47108331e-01  4.37852742e-01 -4.25232309e-01 -1.14738342e+00]\n",
            "  [-1.49243200e-01  8.79277479e-01 -5.21166719e-01 -3.49805822e-01]\n",
            "  [ 8.17945271e-03 -8.47097605e-02 -1.87738757e-01 -7.08312519e-01]\n",
            "  [ 7.51525354e-01  7.12892433e-01 -2.75543142e-01 -2.43875540e+00]\n",
            "  [ 9.69985306e-01  1.11173466e+00 -5.79436817e-01 -9.15877808e-01]\n",
            "  [ 1.68819842e+00  8.26556390e-01  7.09228652e-02 -2.14012171e+00]\n",
            "  [ 2.49576819e+00  1.33726985e+00  1.04479230e+00 -7.93884394e-01]\n",
            "  [ 2.19339669e+00  1.68699051e+00  1.56730169e+00 -6.86901898e-01]\n",
            "  [ 1.37539323e+00  2.27005476e+00  1.49273665e+00  1.07096125e-01]\n",
            "  [ 1.29230761e+00  1.10759179e+00  1.54073385e+00 -1.94726862e+00]\n",
            "  [ 1.53230023e+00  2.06550401e+00  1.02886231e+00 -4.43611444e-01]\n",
            "  [ 1.21231114e+00  2.02245918e+00  6.32560869e-01 -2.15054176e-01]\n",
            "  [ 7.19455185e-01  1.20138185e+00  1.01168895e+00 -1.30167141e+00]\n",
            "  [ 4.38662820e-01  7.50285884e-01  1.02931660e+00 -1.90543219e+00]\n",
            "  [ 2.62793766e-01  1.12247471e+00  2.12635003e-01 -7.27906521e-01]\n",
            "  [ 4.93844705e-01  1.74742940e-02 -8.74643937e-02  2.93239993e-01]\n",
            "  [ 7.15640239e-01 -3.36258943e-01  2.53065871e-02 -9.12245060e-01]\n",
            "  [-2.90635073e-02  6.58265002e-01 -8.42044946e-02 -1.51929414e-02]\n",
            "  [-1.45404311e+00 -1.71226987e-01  5.20986458e-01 -1.00964830e+00]\n",
            "  [-2.52553057e+00 -5.18459470e-01  1.42758424e+00 -2.85533250e-01]\n",
            "  [-2.88749151e+00 -1.40998250e-01  9.81472208e-01  9.03231475e-01]\n",
            "  [-2.78401974e+00 -1.29602039e+00  2.38256807e-01 -9.81994060e-02]\n",
            "  [-2.45993724e+00 -1.08329909e+00  4.87439478e-01 -8.32136823e-01]\n",
            "  [-2.22586496e+00 -9.38399757e-01  1.23184871e+00  1.92319216e+00]\n",
            "  [-2.22172330e+00 -6.93294260e-01  1.64376399e+00  9.64040669e-01]\n",
            "  [-1.82526223e+00 -2.59695955e-01  1.88102827e+00  9.34355732e-01]\n",
            "  [-9.51584459e-01 -9.56289568e-01  1.02559429e+00  1.13697037e+00]\n",
            "  [-7.37440910e-01 -2.09663612e-01  1.40369135e-02  5.00851250e-01]\n",
            "  [-3.90738528e-01 -5.29397196e-01  3.53724329e-02  5.07585240e-01]\n",
            "  [-6.74998175e-02 -3.40594210e-02  7.49171096e-01  4.83722733e-01]\n",
            "  [-4.31399934e-01  1.17144412e-01  1.58671989e+00  9.43699793e-01]\n",
            "  [-8.18733649e-01  6.29985640e-02  1.58261867e+00  7.26409795e-01]\n",
            "  [-9.59634944e-01  5.11163728e-01  5.10584650e-01  2.41759683e-02]\n",
            "  [-8.36528014e-01 -9.18669436e-01  3.03385758e-01 -4.84505295e-01]\n",
            "  [-2.48655818e-01 -1.25790887e-01  1.28154881e+00 -9.00574564e-01]\n",
            "  [ 4.50190155e-01  1.01829888e+00  1.52996404e+00  8.97923676e-01]\n",
            "  [ 2.48048285e-01  1.31055916e+00  8.45206985e-01  1.32423106e+00]\n",
            "  [ 5.93694373e-02  1.15856346e+00  6.80437246e-01  1.35444861e-01]\n",
            "  [ 3.73244910e-01  4.78682555e-01  8.42458379e-01  8.22474158e-01]\n",
            "  [ 7.45274556e-01  6.24450424e-01  7.61649284e-01 -6.54735406e-01]\n",
            "  [ 7.17878041e-01  1.05447419e+00  5.58511292e-01 -7.38900025e-01]\n",
            "  [ 4.07440759e-01  8.58492976e-01 -3.36460204e-01  1.00871205e+00]\n",
            "  [ 6.59144083e-01 -5.27698560e-01 -7.17986803e-01  1.08775047e+00]\n",
            "  [ 6.95000960e-01  7.33948618e-02  8.28052984e-02 -1.28924672e+00]\n",
            "  [-2.99759038e-02  8.98079327e-01  2.90706190e-01 -4.98417518e-01]\n",
            "  [-5.79682720e-01 -1.52745097e-01  6.55651055e-02  1.95119787e-02]\n",
            "  [-1.09910480e+00 -4.38745325e-01 -1.49958390e-01 -1.36680227e-01]\n",
            "  [-1.16077387e+00 -7.00080080e-01 -7.13565687e-01 -8.82892694e-01]\n",
            "  [-3.50263438e-01 -1.01329732e+00 -1.04610344e+00  1.35939702e-02]\n",
            "  [ 7.65180144e-01 -6.76220854e-01 -1.10657216e+00  9.88741716e-01]\n",
            "  [ 1.05272174e+00 -1.44507293e-01 -7.21287533e-01 -1.43600862e-01]\n",
            "  [ 9.64980550e-01 -4.94980691e-03 -5.99797089e-01  8.34508577e-01]\n",
            "  [ 8.22207048e-01 -8.89670333e-02 -4.78795250e-01 -1.25301518e-01]\n",
            "  [ 7.27283205e-01 -3.84764128e-02 -1.71183912e-01 -8.03344959e-01]\n",
            "  [ 3.87485549e-01  6.76325073e-01  6.18819318e-02  4.16457657e-01]\n",
            "  [ 1.63139547e-02  4.21334528e-01 -1.38145515e-02  5.79239446e-01]\n",
            "  [-3.36925271e-01 -1.10666894e-01  2.48101076e-01 -6.56428946e-01]\n",
            "  [-8.07360486e-01  2.89978825e-01  3.34482657e-01 -3.62463909e-01]\n",
            "  [-9.89482429e-01 -1.26315805e-01 -2.20101941e-01  4.55241018e-02]\n",
            "  [-7.22861693e-01 -8.59310146e-01 -7.10273988e-01 -6.05984564e-01]\n",
            "  [-4.37836629e-01 -5.01395258e-01 -4.96520543e-01 -1.85222253e+00]\n",
            "  [-3.91178257e-01 -2.88210743e-01 -1.71526012e-01  1.80874246e+00]\n",
            "  [-5.51863984e-01 -5.72424826e-01 -8.41449205e-02  5.34915785e-01]\n",
            "  [-7.59369004e-01 -3.12638582e-01 -1.08877416e-01  7.57623315e-01]\n",
            "  [-7.35566975e-01 -1.22511007e+00 -3.26786951e-01 -4.10976459e-01]\n",
            "  [-1.24514695e+00 -2.01981680e-01 -7.24230650e-01 -3.12236656e-01]\n",
            "  [-1.71476406e+00 -1.07996832e+00 -3.17681675e-01  2.29704295e-01]\n",
            "  [-1.21670593e+00 -1.00503740e+00 -2.15203766e-01  6.52543221e-01]\n",
            "  [-5.22806856e-01 -5.80620044e-01 -4.89536438e-01  1.50654768e-01]\n",
            "  [-7.75496870e-01 -8.75000030e-01 -7.52017011e-01  2.16038233e-01]\n",
            "  [-1.49524797e+00 -1.23291836e+00 -8.54276325e-01  8.16678799e-01]\n",
            "  [-8.96971691e-01 -1.83586735e+00 -7.62484586e-01  7.23090460e-01]\n",
            "  [ 1.01250332e-01 -1.06448918e+00 -2.93480044e-01  5.10051740e-01]\n",
            "  [ 1.12186274e-01 -3.32969755e-01  1.43363742e-01  5.59879031e-01]\n",
            "  [ 4.17253223e-01  1.56632002e-01  7.59944587e-01  2.34712677e-01]\n",
            "  [ 6.84578125e-01  5.17608261e-01  4.14317316e-01 -3.37694871e-01]\n",
            "  [-4.96762214e-01  1.06097634e+00 -5.80516469e-01 -5.49649141e-01]\n",
            "  [-1.46914158e+00 -1.04518439e+00 -4.40974649e-01 -9.40105165e-01]\n",
            "  [-1.55317268e+00 -7.77201863e-01 -4.38309647e-01  2.28717702e-01]\n",
            "  [-1.47417532e+00 -1.16015027e+00 -6.24656737e-01 -2.96425116e-01]\n",
            "  [-2.03237638e+00 -1.03103325e+00 -1.97072106e-01  3.60777812e-01]\n",
            "  [-2.30812688e+00 -1.23662307e+00 -1.56069611e-01  1.28710151e+00]\n",
            "  [-2.02423238e+00 -1.91854964e+00 -1.59619095e-01  8.05151607e-01]\n",
            "  [-1.41974006e+00 -1.45264314e+00  6.76457869e-01  7.38349311e-01]\n",
            "  [-8.13152818e-01 -5.87243258e-01  1.22675897e+00 -1.68605278e-01]\n",
            "  [-1.72027812e-01  2.44334862e-01  6.90525747e-01  1.09534227e+00]\n",
            "  [ 2.30539777e-02  1.93755776e-01 -2.16227114e-01  4.47310306e-01]\n",
            "  [-3.71466141e-01 -1.26813618e-01 -4.38598312e-01  3.08959707e-01]\n",
            "  [-6.45639171e-01 -3.33890407e-01 -4.15461965e-01 -7.46354159e-01]\n",
            "  [-1.86868507e-01 -1.14121041e+00  1.41272575e-01 -4.94212111e-01]\n",
            "  [ 3.94995113e-01 -5.90158068e-02  1.24247205e+00  4.11007525e-01]\n",
            "  [ 3.56792574e-01  6.31316061e-01  1.41122487e+00  3.09435122e-01]\n",
            "  [ 6.05100380e-01  8.87826953e-01  3.18839377e-01 -2.21264584e-03]\n",
            "  [ 1.14686692e+00  2.49355955e-01 -1.28824538e-01  1.91934967e-01]\n",
            "  [ 1.53981697e+00  1.16846051e+00  8.03616292e-02 -1.29219402e+00]\n",
            "  [ 1.45809503e+00  1.30006947e+00 -1.55345362e-01  1.06931954e+00]\n",
            "  [ 1.13160908e+00  2.75472213e-01 -2.45072972e-01 -1.40875228e+00]\n",
            "  [ 5.73899656e-01  8.82478839e-01 -4.92497763e-01  4.00078412e-01]\n",
            "  [-6.48481438e-02 -5.22423512e-01 -1.11471997e+00  5.54098591e-01]\n",
            "  [-4.46736495e-01 -2.42503669e-01 -6.89668104e-01 -1.55745205e+00]\n",
            "  [-5.92058437e-01  4.48996034e-03 -5.84451786e-01 -1.15584577e+00]\n",
            "  [-6.82911136e-01 -4.22982185e-02 -1.86674245e+00  8.52016227e-01]\n",
            "  [-3.31356968e-01 -1.59069104e+00 -3.07426065e+00  6.28896606e-01]\n",
            "  [-1.57100379e-01 -2.34985297e+00 -3.05184616e+00 -1.11999104e-03]\n",
            "  [-2.57729151e-01 -2.35525941e+00 -2.09489505e+00  2.28313610e+00]\n",
            "  [ 2.68173509e-01 -1.87487267e+00 -1.14357260e+00  2.30696832e+00]\n",
            "  [ 8.40720214e-01 -1.08172585e+00 -4.86533191e-01 -3.47453605e-01]\n",
            "  [ 1.15862908e+00  3.11514604e-01 -5.20683778e-01  7.48210464e-01]\n",
            "  [ 1.21954199e+00  5.62354654e-01 -1.02910207e+00  2.19570542e+00]]]---------------------\n",
            "\n",
            "\n",
            "(20, 250, 4)\n",
            "(9, 250, 4)\n",
            "(1, 250, 4)\n",
            "yc.shape: (1, 250, 4)\n",
            "\n",
            "\n",
            "-----------Statistical properties after scaling------------\n",
            "mean yc: [ 2.83328916e-16 -8.65973959e-17  1.17239551e-16  6.92779167e-17]\n",
            "std_dev yc: [1. 1. 1. 1.]\n",
            "\n",
            "context_points 6\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [1, 51, 105, 202, 46, 76, 149, 242, 195, 214, 99, 49, 89, 235, 72, 10, 6, 40, 110, 13, 174, 57, 196, 39, 103, 18, 190, 128, 201, 225, 241, 53, 178, 157, 165, 34, 92, 120, 75, 136, 243, 35, 84, 97, 127, 101, 164, 41, 19, 32, 203, 67, 63, 30, 217, 151, 42, 132, 58, 144, 167, 208, 71, 155, 65, 218, 78, 96, 229, 185, 45, 118, 129, 108, 88, 176, 59, 64, 94, 158, 153, 44, 162, 82, 117, 28, 170, 239, 12, 79, 54, 137, 223, 86, 7, 93, 26, 175, 131, 146, 191, 222, 2, 112, 159, 14, 219, 11, 50, 179, 142, 106, 166, 24, 123, 197, 227, 161, 87, 194, 68, 119, 116, 177, 182, 104, 5, 135, 37, 25, 230, 109, 193, 43, 95, 205, 143, 69, 4, 212, 172, 188, 124, 29, 210, 21, 91, 224, 115, 169, 148, 107, 183, 55, 60, 232, 70, 134, 100, 187, 199, 9, 38, 33, 22, 17, 121, 231, 168, 216, 8, 173, 207, 47, 192, 204, 147, 198, 98, 152, 220, 234, 73, 150, 189, 154, 237, 138, 213, 228, 31, 209, 74, 236, 77, 27, 125, 130, 81, 20, 226, 114, 139, 36, 61, 56, 145, 48, 16, 180, 83, 186, 62, 85, 238, 126, 240, 0, 160, 171, 200, 211, 181, 102, 215, 184, 23, 3, 140, 206, 15, 66, 133, 221, 113, 122, 141, 52, 233, 163, 156, 80, 111, 90]\n",
            "self.series.num_trials(split): 9\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\n",
            "-------\n",
            "checkpoint_path ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch4_sub_SNR_inf_1/epoch=55-val/norm_mse=0.26.ckpt\n",
            "\n",
            "-----------\n",
            "Forecaster\n",
            "\tL2: 0.001\n",
            "\tLinear Window: 0\n",
            "\tLinear Shared Weights: False\n",
            "\tRevIN: False\n",
            "\tDecomposition: False\n",
            "GlobalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "GlobalCrossAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalSelfAttn: AttentionLayer(\n",
            "  (inner_attention): FullAttention(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (query_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (key_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (value_projection): Linear(in_features=10, out_features=12, bias=True)\n",
            "  (out_projection): Linear(in_features=12, out_features=10, bias=True)\n",
            "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "LocalCrossAttn: None\n",
            "Using Embedding: spatio-temporal\n",
            "Time Emb Dim: 6\n",
            "Space Embedding: True\n",
            "Time Embedding: True\n",
            "Val Embedding: True\n",
            "Given Embedding: True\n",
            "Null Value: None\n",
            "Pad Value: None\n",
            "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
            " *** Spacetimeformer (v1.5) Summary: *** \n",
            "\t\tModel Dim: 10\n",
            "\t\tFF Dim: 40\n",
            "\t\tEnc Layers: 2\n",
            "\t\tDec Layers: 2\n",
            "\t\tEmbed Dropout: 0.0\n",
            "\t\tFF Dropout: 0.0\n",
            "\t\tAttn Out Dropout: 0.0\n",
            "\t\tAttn Matrix Dropout: 0.0\n",
            "\t\tQKV Dropout: 0.0\n",
            "\t\tL2 Coeff: 0.001\n",
            "\t\tWarmup Steps: 1000\n",
            "\t\tNormalization Scheme: batch\n",
            "\t\tAttention Time Windows: 1\n",
            "\t\tShifted Time Windows: False\n",
            "\t\tPosition Emb Type: abs\n",
            "\t\tRecon Loss Imp: 0.0\n",
            " ***                                  *** \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "OVERFIT:  False False\n",
            "self.series.length(split) 250\n",
            "+ time_resolution * (-target_points - context_points)+ 1: -6\n",
            "self._slice_start_points: [40, 58, 189, 242, 17, 108, 32, 114, 234, 235, 84, 136, 97, 179, 129, 220, 187, 113, 44, 205, 100, 67, 105, 163, 138, 128, 26, 202, 157, 22, 76, 208, 68, 56, 153, 82, 190, 112, 73, 131, 110, 36, 195, 98, 104, 227, 123, 184, 147, 188, 206, 146, 6, 229, 177, 59, 51, 241, 170, 134, 16, 64, 31, 223, 91, 161, 211, 168, 109, 130, 24, 55, 99, 191, 90, 47, 139, 164, 27, 228, 62, 57, 37, 171, 120, 199, 209, 1, 49, 7, 28, 194, 144, 8, 11, 81, 203, 72, 169, 222, 19, 87, 148, 125, 124, 175, 212, 63, 89, 14, 118, 15, 243, 127, 217, 107, 43, 60, 182, 116, 196, 132, 13, 158, 141, 42, 215, 103, 198, 18, 159, 78, 231, 61, 122, 178, 75, 219, 192, 92, 180, 152, 41, 151, 181, 237, 149, 23, 94, 86, 85, 69, 38, 35, 2, 74, 150, 96, 53, 45, 137, 95, 111, 117, 236, 201, 216, 145, 93, 221, 142, 66, 52, 174, 10, 4, 197, 39, 186, 88, 165, 200, 133, 167, 34, 65, 29, 115, 70, 5, 20, 160, 101, 121, 204, 46, 176, 71, 77, 50, 166, 155, 30, 225, 25, 156, 135, 172, 213, 240, 183, 218, 54, 239, 80, 238, 173, 102, 3, 33, 185, 233, 21, 162, 140, 48, 106, 230, 224, 210, 207, 12, 119, 214, 79, 83, 126, 9, 226, 0, 193, 154, 143, 232]\n",
            "self.series.num_trials(split): 9\n",
            "Restoring states from the checkpoint path at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch4_sub_SNR_inf_1/epoch=55-val/norm_mse=0.26.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at ./plots_checkpoints_logs/generated_gc_SNR_6/checkpoints/ch4_sub_SNR_inf_1/epoch=55-val/norm_mse=0.26.ckpt\n",
            "Testing DataLoader 0: 100% 18/18 [00:00<00:00, 23.66it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc                    1.0\n",
            "     test/class_loss        0.07116080075502396\n",
            "   test/forecast_loss       0.2560102045536041\n",
            "        test/loss           0.2560173273086548\n",
            "        test/mae            0.3612493574619293\n",
            "        test/mape           2.5134100914001465\n",
            "        test/mse            0.25601017475128174\n",
            "      test/norm_mae         0.3612493574619293\n",
            "      test/norm_mse         0.25601017475128174\n",
            "     test/recon_loss               -1.0\n",
            "        test/rse            0.5053277015686035\n",
            "       test/smape           0.7490116953849792\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    }
  ]
}